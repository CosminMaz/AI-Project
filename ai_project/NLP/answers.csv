id,source,original_text,type
0,IA_8.pdf_p1,Reinforcement learning (II) AI 2025/2026,narrative
1,IA_8.pdf_p2,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 2/41",narrative
2,IA_8.pdf_p2,Model-based learning is Adaptive dynamic programming,diagram_extracted
3,IA_8.pdf_p2,Model-free learning is Direct utility estimation,diagram_extracted
4,IA_8.pdf_p2,Model-free learning is Temporal-Difference learning,diagram_extracted
5,IA_8.pdf_p3,"Reinforcement learning ▶ Markov decision process (MDP) ▶ Reinforcement learning ▶ The set of states S, the set of ▶ It is based on MDPs, but: actions A ▶ ▶ The transition model is The transition model unknown P(s′|s,a) is known ▶ ▶ The reward function is The reward function R(s) is unknown known ▶ ▶ Learn an optimal policy Computes an optimal policy FII,UAIC Lecture8 AI2025/2026 3/41",narrative
6,IA_8.pdf_p4,Types of reinforcement learning ▶ Passive: the agent learns the utility of being in certain states.,narrative
7,IA_8.pdf_p4,The agent executes a fixed policy and evaluates it.,narrative
8,IA_8.pdf_p4,The agent has no control over his actions; applications: robotics.,narrative
9,IA_8.pdf_p4,▶ Active: the agent must learn what to do.,narrative
10,IA_8.pdf_p4,The agent must explore the environment and use the learned information.,narrative
11,IA_8.pdf_p4,The agent updates its policy as it learns.,narrative
12,IA_8.pdf_p4,"FII,UAIC Lecture8 AI2025/2026 4/41",narrative
13,IA_8.pdf_p4,Passive is the agent learns the utility of being in certain states.,diagram_extracted
14,IA_8.pdf_p4,The agent has no control over his actions; applications is robotics.,diagram_extracted
15,IA_8.pdf_p4,Active is the agent must learn what to do.,diagram_extracted
16,IA_8.pdf_p5,"Types of reinforcement learning ▶ Model-based: learn the model of transitions and rewards and use it to discover the optimal policy ▶ Model-free: discover the optimal policy without learning the model FII,UAIC Lecture8 AI2025/2026 5/41",narrative
17,IA_8.pdf_p5,Model-based is learn the model of transitions and rewards and use it to,diagram_extracted
18,IA_8.pdf_p5,Model-free is discover the optimal policy without learning the model,diagram_extracted
19,IA_8.pdf_p6,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 6/41",narrative
23,IA_8.pdf_p7,Passive learning ▶ The policy is fixed: always execute action π(s) in state s ▶ The goal: learn how good is the policy π ▶ learn the utility Uπ(s) of each state how?,narrative
24,IA_8.pdf_p7,"execute the policy and learn from experience ▶ similar approach to step (1) policy evaluation from the Policy iteration algorithm; the difference: doesn’t know the transition model P(s′|s,a) and R(s) Passive learning is a way of exploring the environment.",narrative
25,IA_8.pdf_p7,"FII,UAIC Lecture8 AI2025/2026 7/41",narrative
26,IA_8.pdf_p7,The policy is fixed is always execute action π(s) in state s,diagram_extracted
27,IA_8.pdf_p7,The goal is learn how good is the policy π,diagram_extracted
28,IA_8.pdf_p7,"algorithm; the difference is doesn’t know the transition model P(s′|s,a)",diagram_extracted
29,IA_8.pdf_p8,"Passive learning ▶ The agent performs a series of trials (episodes) (1,1)−.04 ⇝(1,2)−.04 ⇝(1,3)−.04 ⇝(1,2)−.04 ⇝(1,3)−.04 ⇝(2,3)−.04 ⇝ (3,3)−.04 ⇝(4,3)+1 (1,1)−.04 ⇝(1,2)−.04 ⇝(1,3)−.04 ⇝(2,3)−.04 ⇝(3,3)−.04 ⇝(3,2)−.04 ⇝ (3,3)−.04 ⇝(4,3)+1 (1,1)−.04 ⇝(2,1)−.04 ⇝(3,1)−.04 ⇝(3,2)−.04 ⇝(4,2)−1 ▶ The policy is the same, but the environment is non-deterministic ▶ The goal is to learn the expected utility Uπ(s) = E [ (cid:80)∞ γtR(S )] t=0 t FII,UAIC Lecture8 AI2025/2026 8/41",narrative
30,IA_8.pdf_p8,▶ Uπ(s) (cid is 80)∞ γtR(S,diagram_extracted
31,IA_8.pdf_p9,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 9/41",narrative
35,IA_8.pdf_p10,Model-based learning: Adaptive dynamic programming (ADP) 1.,narrative
36,IA_8.pdf_p10,"Learn the transition model Estimate P(s′|s,π(s)) and R(s) from trials.",narrative
37,IA_8.pdf_p10,Use a table of probabilities: compute how often the result of an action occurs and estimate the transition probability.,narrative
38,IA_8.pdf_p10,"Example: (1,1)−.04 ⇝(1,2)−.04 ⇝(1,3)−.04 ⇝(1,2)−.04 ⇝(1,3)−.04 ⇝ (2,3)−.04 ⇝(3,3)−.04 ⇝(4,3)+1 (1,1)−.04 ⇝(1,2)−.04 ⇝(1,3)−.04 ⇝(2,3)−.04 ⇝(3,3)−.04 ⇝(3,2)−.04 ⇝ (3,3)−.04 ⇝(4,3)+1 (1,1)−.04 ⇝(2,1)−.04 ⇝(3,1)−.04 ⇝(3,2)−.04 ⇝(4,2)−1 The action Right is executed 3 times in state (1,3) and in 2 cases the result is state (2,3) =⇒ P((2,3)|(1,3),Right) = 2/3 2.",narrative
39,IA_8.pdf_p10,"Solve MDP FII,UAIC Lecture8 AI2025/2026 10/41",narrative
41,IA_8.pdf_p10,Use a table of probabilities is compute how often the result of an,diagram_extracted
42,IA_8.pdf_p11,Adaptive dynamic programming (ADP) 1.,narrative
43,IA_8.pdf_p11,"Learn the empiric model Example: FII,UAIC Lecture8 AI2025/2026 11/41",narrative
44,IA_8.pdf_p12,Adaptive dynamic programming 2.,narrative
45,IA_8.pdf_p12,Use dynamic programming to solve the MDP.,narrative
46,IA_8.pdf_p12,Enter the learned probabilities and rewards into the Bellman equations (fixed policy).,narrative
47,IA_8.pdf_p12,"(cid:88) Uπ(s) = R(s)+γ P(s′|s,π(s))Uπ(s′) s′ Solve the system of linear equations with the unknowns Uπ(s).",narrative
48,IA_8.pdf_p12,"ADP is inefficient if the state space is large ▶ system of linear equations of order n ▶ backgammon: 1020 equations with 1020 unknowns FII,UAIC Lecture8 AI2025/2026 12/41",narrative
49,IA_8.pdf_p12,(cid is 88),diagram_extracted
50,IA_8.pdf_p12,backgammon is equations with unknowns,diagram_extracted
51,IA_8.pdf_p13,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 13/41",narrative
55,IA_8.pdf_p14,"Model-free learning: Direct utility estimation ▶ The utility of a state is the total expected reward from that state forward (reward-to-go) Example: (1,1) ⇝(1,2) ⇝(1,3) ⇝(1,2) ⇝(1,3) ⇝ −.04 −.04 −.04 −.04 −.04 (2,3) ⇝(3,3) ⇝(4,3) −.04 −.04 +1 (1,1) ⇝(1,2) ⇝(1,3) ⇝(2,3) ⇝(3,3) ⇝ −.04 −.04 −.04 −.04 −.04 (3,2) ⇝(3,3) ⇝(4,3) −.04 −.04 +1 (1,1) ⇝(2,1) ⇝(3,1) ⇝(3,2) ⇝(4,2) −.04 −.04 −.04 −.04 −1 The first attempt produces: ▶ in state (1,1), total reward 0.72 (1 - .04 x 7) ▶ in state (1,2), two total rewards 0.76 and 0.84 ▶ in state (1,3) two total rewards 0.80 and 0.88 ▶ Estimated utility: mean of sampled values ▶ U(1,1) = 0.72, U(1,2) = 0.80, U(1,3) = 0.84 etc.",narrative
56,IA_8.pdf_p14,"FII,UAIC Lecture8 AI2025/2026 14/41",narrative
58,IA_8.pdf_p14,"Example is (1,1)",diagram_extracted
59,IA_8.pdf_p14,Estimated utility is mean of sampled values,diagram_extracted
60,IA_8.pdf_p15,Direct utility estimation ▶ Assume utilities are independent (false).,narrative
61,IA_8.pdf_p15,"It doesn’t take into account that the utility of a state depends on the utilities of next states (the constraints given by the Bellman equations) (cid:88) Uπ(s) = R(s)+γ P(s′|s,π(s))Uπ(s′) s′ ▶ search in a much larger space ▶ convergence is very slow ▶ Have all episodes before FII,UAIC Lecture8 AI2025/2026 15/41",narrative
63,IA_8.pdf_p16,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 16/41",narrative
67,IA_8.pdf_p17,"Temporal-Difference learning ▶ Combines the advantages of Adaptive Dynamic Programming and Direct utility estimation ▶ approximately satisfy the Bellman equations ▶ update only the directly affected states ▶ Goal: estimate utilities Uπ(s), given the episodes generated using policy π.",narrative
68,IA_8.pdf_p17,Actions are decided by policy π.,narrative
69,IA_8.pdf_p17,▶ The utilities are adjusted after each observed transition.,narrative
70,IA_8.pdf_p17,"Example: ▶ After 1st trial: the estimates Uπ(1,3)=0.84,Uπ(2,3)=0.92.",narrative
71,IA_8.pdf_p17,"▶ 2nd trial: the transition (1,3)→(2,3).",narrative
72,IA_8.pdf_p17,"The constraint given by the Bellman equation requires the update of Uπ(1,3).",narrative
73,IA_8.pdf_p17,"FII,UAIC Lecture8 AI2025/2026 17/41",narrative
74,IA_8.pdf_p17,"Goal is estimate utilities Uπ(s), given the episodes generated using",diagram_extracted
75,IA_8.pdf_p17,"After 1st trial is the estimates Uπ(1,3)=0.84,Uπ(2,3)=0.92.",diagram_extracted
76,IA_8.pdf_p17,"2nd trial is the transition (1,3)→(2,3).",diagram_extracted
77,IA_8.pdf_p18,"Temporal-Difference learning ▶ The temporal difference equation uses the difference of utilities between successive states: Uπ(s) ← Uπ(s)+α(R(s)+γUπ(s′)−Uπ(s)) α learning rate The update involves only the successor s′, while the equilibrium conditions (Bellman eq.)",narrative
78,IA_8.pdf_p18,involve all possible next states.,narrative
79,IA_8.pdf_p18,"▶ The method applies a series of corrections to converge ▶ Obs: the method doesn’t need a transitions model to perform the updates FII,UAIC Lecture8 AI2025/2026 18/41",narrative
80,IA_8.pdf_p18,Obs is the method doesn’t need a transitions model to perform the,diagram_extracted
81,IA_8.pdf_p19,"Temporal difference learning: pseudocode Demo: https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html FII,UAIC Lecture8 AI2025/2026 19/41",narrative
82,IA_8.pdf_p19,Temporal difference learning is pseudocode,diagram_extracted
83,IA_8.pdf_p19,Demo is https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html,diagram_extracted
84,IA_8.pdf_p20,"Temporal difference learning: example FII,UAIC Lecture8 AI2025/2026 20/41",narrative
85,IA_8.pdf_p20,Temporal difference learning is example,diagram_extracted
86,IA_8.pdf_p21,"Temporal-Difference learning ▶ The learning rate α determines the speed of convergence to the true utility ▶ The mean value of Uπ(s) will converge to the correct value ▶ many trials, sparse transitions occur rarely ▶ if α is a function which decreases as the no.",narrative
87,IA_8.pdf_p21,"of visits to a state increases, then Uπ(s) converges to the correct value ▶ α(n)=1/n or α(n)=1/(1+n)∈(0,1] FII,UAIC Lecture8 AI2025/2026 21/41",narrative
88,IA_8.pdf_p22,"Temporal differences learning vs. Adaptive dynamic programming ▶ TD needs no model, ADP is model-based ▶ TD use only the observed successor for updating and not all successors ▶ TD converges more slowly but performs simpler calculations ▶ TD can be seen as an approximation of ADP FII,UAIC Lecture8 AI2025/2026 22/41",narrative
89,IA_8.pdf_p23,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 23/41",narrative
93,IA_8.pdf_p24,Passive learning vs.,narrative
94,IA_8.pdf_p24,Active learning ▶ Passive agent has a fixed policy vs. the active agent must decide actions ▶ Passive agent learns (transition probabilities and) utilities and chooses optimal actions vs.,narrative
95,IA_8.pdf_p24,"The active agent updates its policy as it learns ▶ the goal is to learn the optimal policy ▶ but, the utility function is known approximately FII,UAIC Lecture8 AI2025/2026 24/41",narrative
96,IA_8.pdf_p25,"Exploitation vs. exploration Exploitation-exploration dilemma of the agent ▶ to maximize his utility, based on the current knowledge, or ▶ to improve his knowledge A compromise between ▶ exploitation ▶ the agent stops learning and executes the actions given by the policy ▶ exploration ▶ the agent learns by trying new actions is necessary.",narrative
97,IA_8.pdf_p25,"FII,UAIC Lecture8 AI2025/2026 25/41",narrative
98,IA_8.pdf_p26,"Exploitation-exploration dilemma: solutions ϵ-greedy method ▶ Let ϵ ∈ [0,1] ▶ The next action to be selected will be: ▶ a random action, with probability ϵ ▶ the optimal action, with probability 1−ϵ ▶ Implementation ▶ initially, ϵ=1 (exploration) ▶ when a learning episode ends, ϵ decreases (for ex.",narrative
99,IA_8.pdf_p26,"with 0.05) - the exploitation rate progressively increases ▶ ϵ never falls bellow a threshold, e.g.",narrative
100,IA_8.pdf_p26,"0.1 ▶ the agent always has a chance to explore, to avoid local optima FII,UAIC Lecture8 AI2025/2026 26/41",narrative
101,IA_8.pdf_p26,Exploitation-exploration dilemma is solutions,diagram_extracted
102,IA_8.pdf_p27,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 27/41",narrative
106,IA_8.pdf_p28,"Algorithm Q-Learning (Watkins, 1989) ▶ The Q-Learning algorithm learns an action-value function Q(s,a) (Q quality).",narrative
107,IA_8.pdf_p28,"Q(s,a) the value of using action a in state s. The relationship between utilities and Q values: U(s) = max Q(s,a) a ▶ The true equations at equilibrium when the Q values are correct (cid:88) Q(s,a) = R(s)+γ P(s′|s,a)max Q(s′,a′) a′ s′ These can be used in an iterative process that computes the exact Q values.",narrative
108,IA_8.pdf_p28,"FII,UAIC Lecture8 AI2025/2026 28/41",narrative
109,IA_8.pdf_p28,"The relationship between utilities and Q values is U(s) = max Q(s,a)",diagram_extracted
111,IA_8.pdf_p29,"Q-Learning algorithm ▶ A TD agent which learns a Q function does not need a probabilistic model P(s′|s,a) (model-free learning).",narrative
112,IA_8.pdf_p29,"▶ For each sample (s,a,s′,r), update the Q value.",narrative
113,IA_8.pdf_p29,"The update equation for TD Q-Learning: Q(s,a) = Q(s,a)+α(R(s)+γmax Q(s′,a′)−Q(s,a)) a′ (by executing action a in state s we reach the state s′) The learning coefficient α determines the speed of updating the estimates; usually, α ∈ (0,1) FII,UAIC Lecture8 AI2025/2026 29/41",narrative
114,IA_8.pdf_p30,"Q-Learning algorithm: pseudocode f exploration function ▶ Q-learning converges to an optimal policy ▶ Q-Learning is slower than ADP FII,UAIC Lecture8 AI2025/2026 30/41",narrative
115,IA_8.pdf_p30,Q-Learning algorithm is pseudocode,diagram_extracted
116,IA_8.pdf_p31,"Q-learning Example 1 Q(C,Stop)=?,Q(C,Go)=?",narrative
117,IA_8.pdf_p31,"Example 2 https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example FII,UAIC Lecture8 AI2025/2026 31/41",narrative
118,IA_8.pdf_p31,https is //huggingface.co/learn/deep-rl-course/unit2/q-learning-example,diagram_extracted
119,IA_8.pdf_p32,"SARSA ▶ The update equation: Q(s,a) = Q(s,a)+α(R(s)+γQ(s′,a′)−Q(s,a)) (s′,a′) the pair (next state, next action) SARSA uses the TD approach: update the Q table after each step until the solution converges / max.",narrative
120,IA_8.pdf_p32,"▶ Example: Windy Gridworld http://www.incompleteideas.net/book/ebook/node64.html FII,UAIC Lecture8 AI2025/2026 32/41",narrative
121,IA_8.pdf_p32,SARSA uses the TD approach is update the Q table after each step,diagram_extracted
122,IA_8.pdf_p32,Example is Windy Gridworld,diagram_extracted
123,IA_8.pdf_p32,http is //www.incompleteideas.net/book/ebook/node64.html,diagram_extracted
124,IA_8.pdf_p33,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 33/41",narrative
128,IA_8.pdf_p34,"Deep Reinforcement Learning It uses a (deep) neural network to approximate the Q values ▶ input: a state output: an estimate of Q, for each possible action FII,UAIC Lecture8 AI2025/2026 34/41",narrative
129,IA_8.pdf_p34,input is a state,diagram_extracted
130,IA_8.pdf_p34,"output is an estimate of Q, for each possible action",diagram_extracted
131,IA_8.pdf_p35,"Deep Reinforcement Learning ▶ Consider the update equation of the Q values: Q(s,a) = Q(s,a)+α[r +γmax Q(s′,a′)−Q(s,a)] a′ = (1−α)Q(s,a)+α[r +γmax Q(s′,a′)] a′ ▶ The loss function: the mean squared error of the predicted Q value and the target value Q∗ (not known).",narrative
132,IA_8.pdf_p35,"Minimize loss(s,a,s′) = (Q(s,a)−target(s′))2, where the target value: target(s′) = r +γmax Q(s′,a′) a′ ▶ Use Gradient descent to optimize the loss function.",narrative
133,IA_8.pdf_p35,"Description: https://deeplearningmath.org/deep-reinforcement-learning.html Demo: https://cs.stanford.edu/people/karpathy/reinforcejs/ FII,UAIC Lecture8 AI2025/2026 35/41",narrative
134,IA_8.pdf_p35,The loss function is the mean squared error of the predicted Q value,diagram_extracted
135,IA_8.pdf_p35,"the target value is target(s′) = r +γmax Q(s′,a′)",diagram_extracted
136,IA_8.pdf_p35,Description is https://deeplearningmath.org/deep-reinforcement-learning.html,diagram_extracted
137,IA_8.pdf_p35,Demo is https://cs.stanford.edu/people/karpathy/reinforcejs/,diagram_extracted
138,IA_8.pdf_p36,Deep Q-learning Problems: ▶ the samples are correlated → the network cannot generalize ▶ target(s′) is an estimate → slow convergence/unstable alg.,narrative
139,IA_8.pdf_p36,"Solutions: ▶ ϵ-greedy policy ▶ experience replay: store the experiences (s,a,r,s′) and use them for training (mini-batch) FII,UAIC Lecture8 AI2025/2026 36/41",narrative
140,IA_8.pdf_p36,experience replay is store the experiences and use them for,diagram_extracted
141,IA_8.pdf_p37,"Double Deep Q-network ▶ the target value changes at each iteration; solution: a separate network to estimate the target value ▶ every C iterations, the parameters from the prediction network are copied to the target network FII,UAIC Lecture8 AI2025/2026 37/41",narrative
142,IA_8.pdf_p37,the target value changes at each iteration; solution is a separate,diagram_extracted
143,IA_8.pdf_p38,"Function approximation Uˆ (s) = θ f (s)+θ f (s)+...θ f (s) θ 1 1 2 2 n n f ,...f features 1 n RL learn values for the parameters θ = θ ,...θ s.t the evaluation 1 n function Uˆ approximates the true utility function.",narrative
144,IA_8.pdf_p38,"θ ▶ updates the parameters after each trial ▶ use an error function and compute the gradients E (s) = (Uˆ (s)−u (s))2/2 j θ j u (s) the observed total reward from state s in trial j j δE (s) δUˆ (s) θ ← θ −α j = θ +α(u (s)−Uˆ (s)) θ i i i j θ δθ δθ i i ▶ good generalization (visited states → unvisited states) FII,UAIC Lecture8 AI2025/2026 38/41",narrative
145,IA_8.pdf_p39,"Content Introduction Passive learning Model-based learning: Adaptive dynamic programming Model-free learning: Direct utility estimation Model-free learning: Temporal-Difference learning Active learning Q-learning Deep Q-learning Conclusions FII,UAIC Lecture8 AI2025/2026 39/41",narrative
149,IA_8.pdf_p40,"Conclusions ▶ Reinforcement learning is necessary for agents that evolve in unfamiliar environments ▶ Passive learning evaluates a given policy ▶ Active learning learns an optimal policy FII,UAIC Lecture8 AI2025/2026 40/41",narrative
150,IA_8.pdf_p41,Bibliography ▶ Artificial Intelligence: A modern Approach.,narrative
151,IA_8.pdf_p41,Reinforcement Learning ▶ Sutton&Barto.,narrative
152,IA_8.pdf_p41,"An introduction http://incompleteideas.net/book/RLbook2020.pdf FII,UAIC Lecture8 AI2025/2026 41/41",narrative
153,IA_8.pdf_p41,Artificial Intelligence is A modern Approach. Ch. 21. Reinforcement,diagram_extracted
154,IA_8.pdf_p41,http is //incompleteideas.net/book/RLbook2020.pdf,diagram_extracted
155,IA_3.pdf_p1,"Artificial Intelligence 3rd year, 1st semester State based models for decision problems Week 3: informed strategies and alternative problem spaces “In any moment of decision, the best thing you can do is the right thing, the next best thing is the wrong thing, and the worst thing you can do is nothing.” Theodore Roosevelt",narrative
156,IA_3.pdf_p1,Week 3 is informed strategies and alternative problem spaces,diagram_extracted
157,IA_3.pdf_p2,Let’s find a NP-complete problem and a computer able to solve it Problem: finding a path in a graph Describing a model - reducing a problem • describe a state • identify special states and the problem space • describe the transitions and validate them • specify a search strategy,narrative
158,IA_3.pdf_p2,Problem is finding a path in a graph,diagram_extracted
159,IA_3.pdf_p3,Search strategies ● Uninformed - no distinction between states ○ Random ○ BFS and Uniform Cost ○ DFS and Iterative Deepening ○ Backtracking ○ Bidirectional ● Informed - heuristics to help distinguish between states ○ Greedy best-first ○ Hillclimbing and Simulated Annealing ○ Beam search ○ A* and IDA*,narrative
160,IA_3.pdf_p4,Common sense is as good as a mathematical proof.,narrative
161,IA_3.pdf_p4,Statistical proofs are hard to get and largely irrelevant.,narrative
162,IA_3.pdf_p4,"Brute force (search) is great, but intelligence is faster.",narrative
163,IA_3.pdf_p5,Finding a relevant heuristic A heuristic should direct the search towards the goal.,narrative
164,IA_3.pdf_p5,"Heuristic: function applied to any state, returns extreme and opposing values for all initial state(s) and for at least one final state(s).",narrative
165,IA_3.pdf_p5,"h: S -> [min, max], h(IS) = min/max, h(FS) = max/min",narrative
166,IA_3.pdf_p5,"Heuristic is function applied to any state, returns extreme and",diagram_extracted
167,IA_3.pdf_p6,"Types of heuristics If h(IS) = max, h(FS) = 0, h is a distance.",narrative
168,IA_3.pdf_p6,"Below, h is a distance.",narrative
169,IA_3.pdf_p6,"If h(S) <= shortest_path(S,FS), h is admissible.",narrative
170,IA_3.pdf_p6,An admissible heuristic never overestimates the shortest path in the problem space between a state and the goal.,narrative
171,IA_3.pdf_p6,"If h(A) ≤ h(A,B) + h(B) if B is reachable from A, h is consistent.",narrative
172,IA_3.pdf_p6,Requiring that a node is included in a path (solution) should estimate at least as lengthy paths.,narrative
173,IA_3.pdf_p7,"Lets try finding a heuristic For Hanoi Towers: ● Number of pieces on the goal tower ● If starting tower is 1 and goal is n, sum of all values in a state ● ?",narrative
174,IA_3.pdf_p7,For Sliding Puzzle: ● Number of pieces placed correctly For chess: ● How many pieces I have - how many pieces has the opponent?,narrative
175,IA_3.pdf_p7,● A maximal value - number of moves available to the opponent?,narrative
176,IA_3.pdf_p7,"h should always be relatively easy to compute, good enough is enough",narrative
177,IA_3.pdf_p8,Three way of looking at solutions path in a graph path in a space heuristic function graph h(S) solution,narrative
178,IA_3.pdf_p9,Greedy (best-first) Evaluate all unexplored states accessible from the current state.,narrative
179,IA_3.pdf_p9,Select the unexplored state closer to the goal (best score according to a heuristic).,narrative
180,IA_3.pdf_p10,"Greedy evaluation Best case: gets you to the final state much quicker than DFS, ?",narrative
181,IA_3.pdf_p10,doesn’t guarantee optimal solution.,narrative
182,IA_3.pdf_p10,Worst case: DFS with bad choices.,narrative
183,IA_3.pdf_p10,Best case is gets you to the final,diagram_extracted
184,IA_3.pdf_p10,Worst case is DFS with bad,diagram_extracted
185,IA_3.pdf_p11,h(S) Hillclimbing Select one accessible state which is at least as close to the final state as the current one.,narrative
186,IA_3.pdf_p11,h is not a distance!,narrative
187,IA_3.pdf_p11,"h(IS) = min, h(FS) = max h(NewState) ≥ h(CurrentState) solution",narrative
188,IA_3.pdf_p12,h(S) Hillclimbing evaluation Best case: fastest general ?,narrative
189,IA_3.pdf_p12,Worst case: fails to find a solution (local optimum).,narrative
190,IA_3.pdf_p12,https://www.youtube.com/watch?v=j1H3jAAGlEA&l ist=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi&in solution dex=4,narrative
191,IA_3.pdf_p12,Best case is fastest general,diagram_extracted
192,IA_3.pdf_p12,Worst case is fails to find a,diagram_extracted
193,IA_3.pdf_p12,https is //www.youtube.com/watch?v=j1H3jAAGlEA&l,diagram_extracted
194,IA_3.pdf_p13,"Hillclimbing Selection Algorithm eligible_neighbors = {}; For each state n reachable from current_state applying one valid transition If h(n)better than h(current_state) add n to eligible_neighbors; Select next_state from eligible_neighbors; Frequently used selections: ● First one found - most common ● Best one (greedy) ● All, in the order they were found, choosing next state available if stuck (hillclimbing-backtracking)",narrative
195,IA_3.pdf_p14,"Simulated annealing Hillclimbing, but allows choosing a state further away from the goal, with an ever decreasing probability the closer you get to the goal state.",narrative
196,IA_3.pdf_p14,No more local optimum.,narrative
197,IA_3.pdf_p14,Slower than hillclimbing.,narrative
198,IA_3.pdf_p14,http is //www.cse.iitm.ac.in/~vplab/courses/optimization/SA_SEL_SLIDES.pdf,diagram_extracted
199,IA_3.pdf_p15,Beam search BFS but only keeps in a sorted list best k visited states.,narrative
200,IA_3.pdf_p15,The list should be re-sorted and extra states removed after every new explored state.,narrative
201,IA_3.pdf_p15,Explores all states in the sorted list until the final state is found - should be first in the list.,narrative
202,IA_3.pdf_p15,Does not guarantee the optimum solution!,narrative
203,IA_3.pdf_p16,"A* and IDA* A* is mostly Dijkstra combined with Greedy, IDA* is informed IDDFS IDDFS explores nodes by distance from initial state d(S) A* explores nodes by d(S) + h(S), where h(S) is the estimated distance to the goal state (h has to be an admissible heuristic and a distance) Always chooses the unexplored state which is closest to both the initial state and the goal state.",narrative
204,IA_3.pdf_p16,https is //www.redblobgames.com/pathfinding/a-star/introduction.html,diagram_extracted
205,IA_3.pdf_p16,https is //adrianstoll.com/post/a-star-pathfinding-algorithm-animation/,diagram_extracted
206,IA_3.pdf_p17,"A* Algorithm Current_state = Initial_state; Best_score = maximum of h; Sorted_queue = {All neighbours of Current_state, with computed h+1 scores, marked as unexplored}; While the score of the first unexplored state S in Sorted_queue is lower than Best_score If S is a final state, Best_score = score of S; Mark S as explored; Add to Sorted_queue all its neighbours with computed h+d score and mark them unexplored; If a duplicate state appears in Sorted_queue, keep only the occurrence with lowest score and transfer explored status; Sort Sorted_queue; The optimal path is recovered from the last state updating Best_score and looking for explored neighbours with lower score up to the initial state.",narrative
207,IA_3.pdf_p18,A* Always finds the optimum A h(A) = 1 solution.,narrative
208,IA_3.pdf_p18,3 h(B) = 4 B,narrative
210,IA_3.pdf_p19,real = 4 5 Don’t stop when you find the goal state!,narrative
211,IA_3.pdf_p19,Keep looking until all 3 states with better scores have h(B) = 4 been explored.,narrative
212,IA_3.pdf_p19,real = 4 B,narrative
213,IA_3.pdf_p21,"Optimizing A* Consistent heuristic: h(A) ≤ h(A,B) + h(B) if B is reachable from A Simplified Memory Bounded A* ● Prunes (removes) states for which the path was more costly than estimated, up to a set number of states (memory bounded) ● Remembers costs of reaching pruned states from each explored state ● If solution is not found, recovers pruned states and explores them",narrative
214,IA_3.pdf_p21,"Consistent heuristic is h(A) ≤ h(A,B) + h(B) if B is reachable from A",diagram_extracted
215,IA_3.pdf_p22,Choose a strategy ● Do you want a solution quickly?,narrative
216,IA_3.pdf_p22,● Do you want the best solution?,narrative
217,IA_3.pdf_p22,● Do you want all solutions?,narrative
218,IA_3.pdf_p22,● How much do you know about the problem?,narrative
219,IA_3.pdf_p22,● Are you sure the state representation is good enough?,narrative
220,IA_3.pdf_p23,Non-deterministic problem space Stochastic: known possibilities and probabilities Non-deterministic: known possibilities Each transition has multiple possible outcomes AND-OR trees,narrative
221,IA_3.pdf_p23,Stochastic is known possibilities,diagram_extracted
222,IA_3.pdf_p23,Non-deterministic is known,diagram_extracted
223,IA_3.pdf_p24,"Partially observable problem space: you know what could happen after a transition, you don’t know what will happen after a transition.",narrative
224,IA_3.pdf_p24,Partially observable problem space is you know what could happen,diagram_extracted
225,IA_3.pdf_p25,Unknowable problem space: you don’t know what could happen after a transition,narrative
226,IA_3.pdf_p25,Unknowable problem space is you don’t know what could happen,diagram_extracted
227,IA_3.pdf_p26,Next week: Constraint satisfaction problems (Variable based models),narrative
228,IA_3.pdf_p26,Next week is Constraint satisfaction problems (Variable based models),diagram_extracted
229,IA_3.pdf_p27,Week 5: Games ?,narrative
230,IA_3.pdf_p27,Interactive decision problems (games): ● Solution is not built exclusively by your strategy ● Not all final states are the same Hardest solvable problems in AI.,narrative
231,IA_3.pdf_p27,NLP problems hardest potentially solvable.,narrative
232,IA_3.pdf_p27,Week 5 is Games,diagram_extracted
233,IA_9.pdf_p1,"Artificial Intelligence 3rd year, 1st semester Knowledge representation and inference systems “We are drowning in information but starved for knowledge.”― John Naisbitt",narrative
234,IA_9.pdf_p2,How did you learn that a cat has whiskers?,narrative
235,IA_9.pdf_p2,Does it matter?,narrative
236,IA_9.pdf_p3,What is knowledge?,narrative
237,IA_9.pdf_p3,Intelligence applied to information produces knowledge.,narrative
238,IA_9.pdf_p3,Knowledge supplements intelligence.,narrative
239,IA_9.pdf_p3,"Intelligence is asking the right question, knowledge is having the right answer.",narrative
240,IA_9.pdf_p4,Is knowledge useful to a computer?,narrative
241,IA_9.pdf_p4,Knowledge can supplement a slow or insufficient intelligence.,narrative
242,IA_9.pdf_p4,Example: chatbot templates and the Turing test.,narrative
243,IA_9.pdf_p4,"Knowledge is a product, not a process (like intelligence), so it can be provided ready-made to the computer.",narrative
244,IA_9.pdf_p4,"Knowledge is built on facts, heuristics and intuition and can change.",narrative
245,IA_9.pdf_p4,Let’s look at unambiguous and easily parse-able representations.,narrative
246,IA_9.pdf_p4,Example is chatbot templates and the Turing test.,diagram_extracted
247,IA_9.pdf_p5,"Linnaeus, C. (1753).",narrative
248,IA_9.pdf_p5,Stockholm: Laurentii Salvii.,narrative
249,IA_9.pdf_p6,Taxonomy Greek: taxis (arrangement) nomia (method) A taxonomy is language independent (as is all knowledge).,narrative
250,IA_9.pdf_p6,Nodes are concepts.,narrative
251,IA_9.pdf_p6,No ambiguity due to careful lexicalisations and domain focus.,narrative
252,IA_9.pdf_p6,Relations are semantic.,narrative
253,IA_9.pdf_p6,Only one type of relation: “is-a”,narrative
254,IA_9.pdf_p6,Greek is taxis (arrangement) nomia (method),diagram_extracted
255,IA_9.pdf_p6,Only one type of relation is “is-a”,diagram_extracted
256,IA_9.pdf_p7,"X IS-A Y: X is a specific type of Y, with additional properties ● Symmetry: if a bear IS-A mammal, a mammal IS-A bear?",narrative
257,IA_9.pdf_p7,● Reflexivity: bear IS-A bear?,narrative
258,IA_9.pdf_p7,"● Transitivity: if a black bear IS-A bear and a bear IS-A mammal, a black bear IS-A mammal?",narrative
259,IA_9.pdf_p7,● Connectivity: cat IS-A bear or bear IS-A cat?,narrative
260,IA_9.pdf_p7,"“is-a” is asymmetric, irreflexive, transitive and not connected, thus is a strict order relation over the set of concepts (no cycles in a taxonomy).",narrative
261,IA_9.pdf_p7,"X IS-A Y is X is a specific type of Y, with additional properties",diagram_extracted
262,IA_9.pdf_p7,"● Symmetry is if a bear IS-A mammal, a mammal IS-A bear?",diagram_extracted
263,IA_9.pdf_p7,● Reflexivity is bear IS-A bear?,diagram_extracted
264,IA_9.pdf_p7,"● Transitivity is if a black bear IS-A bear and a bear IS-A mammal, a",diagram_extracted
265,IA_9.pdf_p7,● Connectivity is cat IS-A bear or bear IS-A cat?,diagram_extracted
266,IA_9.pdf_p8,Are taxonomies enough?,narrative
267,IA_9.pdf_p8,Can you represent all your computer science knowledge in a taxonomy?,narrative
268,IA_9.pdf_p8,Is IS-A enough?,narrative
269,IA_9.pdf_p8,How adaptable is a taxonomy to new knowledge?,narrative
270,IA_9.pdf_p9,Semantic networks Directed graphs with concepts as nodes and semantic relations labeling edges.,narrative
271,IA_9.pdf_p9,"Can also be represented as a list of predicates: has (Cat, Fur), lives in (Fish, Water)",narrative
272,IA_9.pdf_p10,Semantic frames “to drive” frame Agent John John drives a car.,narrative
273,IA_9.pdf_p10,Object a car Identified by an action (event) OR Starting point ?,narrative
274,IA_9.pdf_p10,a type reference: Fluffy the cat Ending point ?,narrative
275,IA_9.pdf_p10,“Cat” frame Beneficiary ?,narrative
276,IA_9.pdf_p10,Name Fluffy Modal ?,narrative
277,IA_9.pdf_p10,a type reference is Fluffy the cat Ending point ?,diagram_extracted
278,IA_9.pdf_p11,"Conceptual network Events and referential networks cat mouse Referential network: part of a semantic network in which nodes identify actual objects, not eating agent object concepts.",narrative
279,IA_9.pdf_p11,Event: defined as a set of relations with is_a is_a referential network connections.,narrative
280,IA_9.pdf_p11,Frames ca be references and marked in a semantic network.,narrative
281,IA_9.pdf_p11,"Fluffy Squeeky Eating: Referential network ⋀ agent(is_a(Fluffy, cat)) object(is_a(Squeeky, mouse))",narrative
282,IA_9.pdf_p11,Referential network is part of a semantic network,diagram_extracted
283,IA_9.pdf_p11,Event is defined as a set of relations with,diagram_extracted
284,IA_9.pdf_p12,Reasoning on semantic networks(I) Relations are inherited.,narrative
285,IA_9.pdf_p12,"IS-A(cat, mammal) AND IS-A(mammal, animal) => IS-A(cat, animal) Answering queries.",narrative
286,IA_9.pdf_p12,⋀ Eating: agent (Fluffy) object (?x),narrative
287,IA_9.pdf_p12,Eating is agent (Fluffy) object (?x),diagram_extracted
288,IA_9.pdf_p13,Conceptual network Reasoning on semantic networks(II) cat mouse Daemons: procedures used to eating agent object automatically maintain and enhance a semantic network.,narrative
289,IA_9.pdf_p13,"is_a is_a medium If (IS-A(Fluffy, Fluffy size Squeeky ⋀ animal)) ⋀ Referential network (length(Fluffy, ?x)) ?x ∊ [50,100] => size(Fluffy, medium)",narrative
290,IA_9.pdf_p13,Daemons is procedures used to,diagram_extracted
291,IA_9.pdf_p14,"Synsets = lists of synonymic senses of different words (over 120.000) S: (n) student, pupil, educatee (a learner who is enrolled in an educational institution) Relations?",narrative
292,IA_9.pdf_p14,"6 types of semantic relations: ● Antonymy: opposite meaning of good - bad ● Hyponymy: IS-A (is a specific kind of…) student - person ● Meronymy: part of (is structurally included in…) head - student ● Troponymy: identifies a manner of… whisper - speak ● Entailment: identifies requirements divorce - marriage ● Derivation: related to, produced by product - factory",narrative
293,IA_9.pdf_p14,● Antonymy is opposite meaning of good - bad,diagram_extracted
294,IA_9.pdf_p14,● Hyponymy is IS-A (is a specific kind of…) student - person,diagram_extracted
295,IA_9.pdf_p14,● Meronymy is part of (is structurally included in…) head - student,diagram_extracted
296,IA_9.pdf_p14,● Troponymy is identifies a manner of… whisper - speak,diagram_extracted
297,IA_9.pdf_p14,● Entailment is identifies requirements divorce - marriage,diagram_extracted
298,IA_9.pdf_p14,"● Derivation is related to, produced by product - factory",diagram_extracted
299,IA_9.pdf_p15,"Ontology Definitions (courtesy of Merriam-Webster): 1: a branch of metaphysics concerned with the nature and relations of being 2: a particular theory about the nature of being or the kinds of things that have existence Unambiguous representation of knowledge, more expressive than a taxonomy.",narrative
300,IA_9.pdf_p16,"Semantic triplet (Semantic) Relation Concept Concept A B Relation(Concept A, Concept B) We can represent any knowledge using semantic triplets.",narrative
301,IA_9.pdf_p16,Try with any knowledge you possess.,narrative
302,IA_9.pdf_p16,We can build complex structures (ontologies) using these triplets.,narrative
303,IA_9.pdf_p16,"The complexity is given by the size, local complexity is reduced.",narrative
304,IA_9.pdf_p16,You rarely need to explore the entire ontology.,narrative
305,IA_9.pdf_p16,We can extract complex knowledge from triplets/ontologies.,narrative
306,IA_9.pdf_p16,What is the Relation between A and B?,narrative
307,IA_9.pdf_p16,With whom is A/B in Relation?,narrative
308,IA_9.pdf_p16,Between who does Relation exists?,narrative
309,IA_9.pdf_p17,"Ontology: attempt of a formal definition A hypergraph O(C, R) and a set I where: ● C is a set of concepts c, where c is defined as a tuple (i, d, l, r, p, r), where: ○ i is an unique identifier; ○ d is a list of definitions; ○ l is a list of lexicalisations; ○ r is a list of references to other related sources; ○ p is a list of property identifiers (relations with one member); ○ r is a list of relation identifiers with c as a member.",narrative
310,IA_9.pdf_p17,Ontology is attempt of a formal definition,diagram_extracted
311,IA_9.pdf_p18,"Ontology: attempt of a formal definition ● R is a set of semantic relations r, each defined as a triple (i, m, b), where: ○ i is an unique identifier; ○ m is the number of members of that relation; ○ b indicates whether the relation is bidirectional.",narrative
312,IA_9.pdf_p18,● I is a set of inferences (rules) over the ontology performing changes to it.,narrative
313,IA_9.pdf_p18,Instance: concept to which no other concept can be linked by IS-A.,narrative
315,IA_9.pdf_p18,Instance is concept to which no other concept can be linked by IS-A.,diagram_extracted
316,IA_9.pdf_p19,Ontology viewers and editors Most common format: RDF ● PuffinSemantics ● GraphDB ● WebVOWL ● Protégé ● Fluent Editor Knowledge Graphs are instanced ontologies and are very common.,narrative
317,IA_9.pdf_p19,Most common format is RDF,diagram_extracted
318,IA_9.pdf_p20,Ontologies: unlimited expressiveness ● Anything can be included as a concept/instance ● Concepts/instances can have any properties ● Any semantic relation can be included ● Consistent relations/properties are not mandatory (but highly recommended) ● Built for continuous maintenance and development - use inferences!,narrative
319,IA_9.pdf_p20,Ontologies is unlimited expressiveness,diagram_extracted
320,IA_9.pdf_p21,The Semantic Web Classic web Semantic web,narrative
321,IA_9.pdf_p22,Semantic web: current state Describe linked data?,narrative
322,IA_9.pdf_p22,JSON-LD Open Graph Detailed domain ontologies?,narrative
323,IA_9.pdf_p22,Dbpedia.org Schema.org Semantic web browser?,narrative
324,IA_9.pdf_p22,"LodView Magpie OK, so what’s the problem?",narrative
325,IA_9.pdf_p22,THIS but also this,narrative
326,IA_9.pdf_p22,Semantic web is current state,diagram_extracted
327,IA_9.pdf_p23,Upper Merged Ontologies SUMO (appoximately 25000 terms linked) Dolce Yago,narrative
328,IA_9.pdf_p24,Potential difficulties in knowledge representation ● Semantic parasitism: externally inferred semantics Metaphors: is “semantic parasitism” something positive or negative in an ontology?,narrative
329,IA_9.pdf_p24,● Semantic subjectivity: different semantics for different contexts,narrative
330,IA_9.pdf_p24,● Semantic parasitism is externally inferred semantics,diagram_extracted
331,IA_9.pdf_p24,Metaphors is is “semantic parasitism” something positive or,diagram_extracted
332,IA_9.pdf_p24,● Semantic subjectivity is different semantics for different contexts,diagram_extracted
333,IA_9.pdf_p25,"Watson’s first experiment Considering the following series of numbers which was built observing a rule, propose a similar series observing the same rule and one disobeying the same rule.",narrative
334,IA_9.pdf_p25,2 - 4 - 6 More info,narrative
335,IA_9.pdf_p26,Watson’s selection task The four cards have a letter on one side and a number on the other side.,narrative
336,IA_9.pdf_p26,Which cards do you need to turn in order to check if the cards obey the rule: All odd numbers have vowels on the other side?,narrative
337,IA_9.pdf_p26,need to turn in order to check if the cards obey the rule is All odd numbers have vowels on the,diagram_extracted
338,IA_9.pdf_p27,Alternative to logical cognitive systems Lewis–Stalnaker possible-worlds semantics: 1.,narrative
339,IA_9.pdf_p27,Possible worlds exist (not actually in the real world).,narrative
340,IA_9.pdf_p27,Other worlds are the same as the real world (same complexity and capabilities).,narrative
341,IA_9.pdf_p27,We know of the actual world we inhabit.,narrative
342,IA_9.pdf_p27,Actual is an index for the world it is uttered.,narrative
343,IA_9.pdf_p27,Possible worlds cannot be reduced to something more basic.,narrative
344,IA_9.pdf_p27,Alternative: Probabilistic reasoning,narrative
345,IA_9.pdf_p27,Alternative is Probabilistic reasoning,diagram_extracted
346,IA_9.pdf_p28,Open book vs Closed book exam Tests memory Tests intelligence in applying knowledge The goal should never be the accumulation Test analytical ability (looking for of knowledge.,narrative
347,IA_9.pdf_p28,"The right answer means knowledge, recognizing relevant knowledge) nothing without the right question (42).",narrative
348,IA_10.pdf_p1,"Artificial Intelligence 3rd year, 1st semester Natural Language Processing “Science is what we understand well enough to explain to a computer; art is everything else.”― Donald E. Knuth",narrative
349,IA_10.pdf_p2,Language - a structured system of communication.,narrative
350,IA_10.pdf_p2,"Structured system - has items, relations between them and (production) rules over them Communication - there has to be a semantic association from that language to the outside of it Intelligence uses and builds knowledge and language.",narrative
351,IA_10.pdf_p2,Sapir-Whorf and Neurolinguistics,narrative
352,IA_10.pdf_p3,Natural (human) languages vs Formal languages (Full ?),narrative
353,IA_10.pdf_p3,Expressive power Limited expressiveness Personal interpretation Unambiguous Dynamic Little dynamism Easy to learn Harder to learn,narrative
354,IA_10.pdf_p4,Does the computer need human language?,narrative
355,IA_10.pdf_p4,● Accessibility ● Access to resources ● Expressiveness,narrative
356,IA_10.pdf_p5,Example: Virtual assistants Apple’s Siri Amazon’s Alexa Microsoft’s Cortana Google’s Assistant,narrative
357,IA_10.pdf_p5,Example is Virtual assistants,diagram_extracted
358,IA_10.pdf_p6,"OK, now what?",narrative
359,IA_10.pdf_p6,"We already know that: ● Language has items, relations between them and (production) rules over them ● There has to be a semantic association from that language to the outside of it ● Language has generous expressive power and is dynamic",narrative
360,IA_10.pdf_p7,Types of NLP Text analysis: understand the meaning of a natural language message.,narrative
361,IA_10.pdf_p7,Text generation: convey a meaning in natural language.,narrative
362,IA_10.pdf_p7,"Other multimodal applications: for example, sign language translation/transcription.",narrative
363,IA_10.pdf_p7,Text analysis is understand the meaning of a natural language message.,diagram_extracted
364,IA_10.pdf_p7,Text generation is convey a meaning in natural language.,diagram_extracted
365,IA_10.pdf_p7,"Other multimodal applications is for example, sign language",diagram_extracted
366,IA_10.pdf_p8,Text analysis: morphological level Morphology: morpho = form First step: developing a vocabulary Recognize words as belonging to your language: is a dictionary enough?,narrative
367,IA_10.pdf_p8,Text analysis is morphological level,diagram_extracted
368,IA_10.pdf_p8,Morphology is morpho = form,diagram_extracted
369,IA_10.pdf_p8,First step is developing a vocabulary,diagram_extracted
370,IA_10.pdf_p8,language is is a dictionary enough?,diagram_extracted
371,IA_10.pdf_p9,Why not 100%?,narrative
372,IA_10.pdf_p9,How many words are in “F.B.I.”?,narrative
373,IA_10.pdf_p9,Is “F.B.I.” a word?,narrative
374,IA_10.pdf_p9,How about “eccedentesiast”?,narrative
375,IA_10.pdf_p10,Text analysis: lexical level Lexicology : lexicos = of words How words are formed: ● Lemmas ● Suffixes and prefixes ● Syllables ● Morphemes ● Parts of speech Further reading How to split a word in syllables?,narrative
376,IA_10.pdf_p10,Text analysis is lexical level,diagram_extracted
377,IA_10.pdf_p10,Lexicology is lexicos = of words,diagram_extracted
378,IA_10.pdf_p11,How does a Part-Of-Speech tagger work?,narrative
379,IA_10.pdf_p11,Identify lexical tokens: The/students/are/learning/.,narrative
380,IA_10.pdf_p11,Identify lemmas: The/student-s/be-are/learn-ing/.,narrative
381,IA_10.pdf_p11,Use a lexicon and match unambiguous lemmas: D-The/student-s/V-be-are/learn-ing/P-.,narrative
382,IA_10.pdf_p11,Use a language model to disambiguate the rest: D N V V P 250 D V V V P 18 D A V AV P 5,narrative
384,IA_10.pdf_p12,"“Time flies like an arrow.” “Will, will Will will Will Will's will?”",narrative
385,IA_10.pdf_p13,"Text analysis: syntactic level Syntax: syn - together, taxis - arrange Identify sentences/ discourse units.",narrative
386,IA_10.pdf_p13,Find the roles of words in a sentence.,narrative
387,IA_10.pdf_p13,"(subject, predicate, complement,...) Find syntactic relations between words.",narrative
388,IA_10.pdf_p13,"(Noun Phrase, Anaphora, direct object,...) Further reading Example",narrative
389,IA_10.pdf_p13,Text analysis is syntactic level,diagram_extracted
390,IA_10.pdf_p13,"Syntax is syn - together, taxis - arrange",diagram_extracted
392,IA_10.pdf_p14,“Time flies like an arrow.” “We saw her duck.” “I saw the man on the hill with a telescope.”,narrative
393,IA_10.pdf_p15,Text analysis: semantic level Semantic: semantikos - significant Associate meanings to words which have them.,narrative
394,IA_10.pdf_p15,"(substantives, verbs, adjectives and adverbs) Word Sense Disambiguation: What are the possible senses of a word?",narrative
395,IA_10.pdf_p15,Is there a correct sense?,narrative
396,IA_10.pdf_p15,Text analysis is semantic level,diagram_extracted
397,IA_10.pdf_p15,Semantic is semantikos - significant,diagram_extracted
399,IA_10.pdf_p16,“He’s mad.” “You’re done.” “We are far from each other.”,narrative
400,IA_10.pdf_p17,"Text analysis: discourse level Discourse: discursus - running around, going over something What is the underlying structure of a text?",narrative
401,IA_10.pdf_p17,Several representations: - Rhetorical structure theory - Centering theory - Veins theory,narrative
402,IA_10.pdf_p17,Text analysis is discourse level,diagram_extracted
403,IA_10.pdf_p17,"Discourse is discursus - running around, going over something",diagram_extracted
404,IA_10.pdf_p18,Text analysis: pragmatic level Pragmatics: pragmatikos - fit for social activity Context matters even for written text.,narrative
405,IA_10.pdf_p18,You have to identify the target of the message.,narrative
406,IA_10.pdf_p18,"“Alexa, play a song.” Punctuation matters: “I like to eat my family and my cat” Prosody: tone and intonation matters.",narrative
407,IA_10.pdf_p18,Text analysis is pragmatic level,diagram_extracted
408,IA_10.pdf_p18,Pragmatics is pragmatikos - fit for social activity,diagram_extracted
409,IA_10.pdf_p18,Prosody is tone and intonation matters.,diagram_extracted
410,IA_10.pdf_p19,"Text generation: I have knowledge, how do I communicate it?",narrative
411,IA_10.pdf_p19,Generally done by applying templates to knowledge.,narrative
412,IA_10.pdf_p19,I have ?x ?y.,narrative
413,IA_10.pdf_p19,Quantity Type 20 apples Often uses language models to determine most likely word to be used in the context.,narrative
414,IA_10.pdf_p19,"have possess apples 180 5 frequency Overview Example systems: RosaeNLG, SimpleNLG",narrative
415,IA_10.pdf_p19,"Text generation is I have knowledge, how do I communicate it?",diagram_extracted
416,IA_10.pdf_p19,"Example systems is RosaeNLG, SimpleNLG",diagram_extracted
417,IA_10.pdf_p20,"Brief history of conversational agents: 50s to 70s - templates and regular expressions <aiml version=""1.0.1"" encoding = ""UTF-8""?> <category> <pattern> HELLO ALICE </pattern> <template> Hello User </template> </category> </aiml>",narrative
418,IA_10.pdf_p21,Brief history of conversational agents: 80s to 2000s - language models and Markov chains,narrative
419,IA_10.pdf_p22,Brief history of conversational agents: Current state-of-the-art - Transformers and Reinforcement Learning,narrative
420,IA_10.pdf_p23,Speech to text: what is that human speaking?,narrative
421,IA_10.pdf_p23,"We use sounds, not words or letters.",narrative
422,IA_10.pdf_p23,Phonemes: sounds forming words.,narrative
423,IA_10.pdf_p23,I teach Artificial Intelligence.,narrative
424,IA_10.pdf_p23,aɪ tiːʧ ˌɑːtɪˈfɪʃ(ə)l ɪnˈtɛlɪʤəns.,narrative
425,IA_10.pdf_p23,44 phonemes for English International Phonetic Alphabet Phonetic keyboard,narrative
426,IA_10.pdf_p23,Speech to text is what is that human speaking?,diagram_extracted
427,IA_10.pdf_p23,Phonemes is sounds forming words.,diagram_extracted
428,IA_10.pdf_p25,"Speech to text and reverse Phonemes to words r eh k ao g n ay z s p iy ch ""recognize speech"" r eh k ay n ay s b iy ch ""wreck a nice beach"" Language models 60k words - 216 trillion possible sets of 3 words IBM Watson STT DeepSpeech Google Assistant",narrative
429,IA_10.pdf_p26,Multimodal NLP: how else can I communicate?,narrative
430,IA_10.pdf_p26,Using sign language: Google’s MediaPipe Demo for Amazon Echo Let’s TalkSign Lip reading Deep Lip Reading Google’s DeepMind Demo Handwriting recognition Overview,narrative
431,IA_10.pdf_p26,Multimodal NLP is how else can I communicate?,diagram_extracted
432,IA_10.pdf_p27,NLP resources CLARIN VLO LRE Map Linguistic Data Consortium General Architecture for Text Engineering (GATE) Apache UIMA,narrative
433,IA_4.pdf_p1,Constraint Satisfaction Problems AI 2025/2026,narrative
434,IA_4.pdf_p2,"Content Introduction Backtracking search for CSPs Improving search The structure of the constraint graph Local search for CSPs FII,UAIC Lecture4 AI2025/2026 2/60",narrative
435,IA_4.pdf_p3,"Constraint satisfaction problems (CSPs) Constraint satisfaction problems (CSPs) ▶ Are defined by variables X with values from domain D , and a set of i i constraints specifying allowable combinations of values for subsets of variables ▶ An assignment is consistent if it does not violate any of the constraints.",narrative
436,IA_4.pdf_p3,An assignment is complete if it includes all variables.,narrative
437,IA_4.pdf_p3,Solution: a complete assignment of values to variables s.t.,narrative
438,IA_4.pdf_p3,all constraints are satisfied.,narrative
439,IA_4.pdf_p3,"▶ general-purpose algorithms, more powerful than standard search algorithms ▶ MaxCSP: maximize the number of satisfied constraints FII,UAIC Lecture4 AI2025/2026 3/60",narrative
440,IA_4.pdf_p3,Solution is a complete assignment of values to variables s.t. all,diagram_extracted
441,IA_4.pdf_p3,MaxCSP is maximize the number of satisfied constraints,diagram_extracted
442,IA_4.pdf_p4,"Example: Map-Coloring Variables: WA, NT, Q, NSW, V, SA, T Domains: D = {red,green,blue} i Constraints: adjacent regions must have different colors e.g., WA ̸= NT (if the language allows), or (WA,NT) ∈ {(red,green),(red,blue),(green,red),(green,blue),...} FII,UAIC Lecture4 AI2025/2026 4/60",narrative
443,IA_4.pdf_p4,Example is Map-Coloring,diagram_extracted
444,IA_4.pdf_p4,"Variables is WA, NT, Q, NSW, V, SA, T",diagram_extracted
445,IA_4.pdf_p4,"Domains is D = {red,green,blue}",diagram_extracted
446,IA_4.pdf_p4,Constraints is adjacent regions must have different colors,diagram_extracted
447,IA_4.pdf_p5,Example: Map-Coloring Solutions are assignments satisfying all constraints.,narrative
448,IA_4.pdf_p5,"{WA = red,NT = green,Q = red,NSW = green,V = red,SA = blue,T = green} FII,UAIC Lecture4 AI2025/2026 5/60",narrative
450,IA_4.pdf_p6,"Example: N-Queens 1st Model: Variables: X ij Domains: {0,1} (cid:80) Constraints X = N i,j ij ∀i,j,k (x ,x ) ∈ {(0,0),(0,1),(1,0)} ij ik ∀i,j,k (x ,x ) ∈ {(0,0),(0,1),(1,0)} ij kj ∀i,j,k (x ,x ) ∈ {(0,0),(0,1),(1,0)} ij i+k,j+k ∀i,j,k (x ,x ) ∈ {(0,0),(0,1),(1,0)} ij i+k,j−k FII,UAIC Lecture4 AI2025/2026 6/60",narrative
451,IA_4.pdf_p6,Example is N-Queens,diagram_extracted
452,IA_4.pdf_p6,"Domains is {0,1}",diagram_extracted
453,IA_4.pdf_p6,(cid is 80),diagram_extracted
454,IA_4.pdf_p7,"Example: N-Queens 2nd Model: Variables: Q k Domains: {1,2,3,...,N} Constraints Implicit: ∀i,j non−threatening(Q ,Q ) i j Explicit: (Q ,Q ) ∈ {(1,3),(1,4),...}... 1 2 FII,UAIC Lecture4 AI2025/2026 7/60",narrative
456,IA_4.pdf_p7,"Domains is {1,2,3,...,N}",diagram_extracted
457,IA_4.pdf_p7,"Implicit is ∀i,j non−threatening(Q ,Q )",diagram_extracted
458,IA_4.pdf_p7,"Explicit is (Q ,Q ) ∈ {(1,3),(1,4),...}...",diagram_extracted
459,IA_4.pdf_p8,"Example: Sudoku Variables: x ∈ {1,...,9} =: N (the values in the corresponding cells); ij Pairwise inequality constraints: all values in a row, column, subsquare are different x ̸= x ∀k ̸= i,j ∈ N,(rows) ij ik x ̸= x ∀k ̸= i,j ∈ N,(columns) ij kj x ̸= x ∀(i ,j ) ̸= (i ,j ) ∈ C ,∀i,j ∈ N′,(subsquares) i1j1 i2j2 1 1 2 2 ij x ∈ N ∀i,j ∈ N ij C = {(3(i −1)+i′,3(j −1)+j′)|(i′,j′) ∈ N′xN′} ij Use the global alldifferent constraint for a more powerful formulation: alldifferent(x |j ∈ N) ∀i ∈ N,(rows) ij alldifferent(x |i ∈ N) ∀j ∈ N,(columns) ij alldifferent(C ) ∀i,j ∈ N′,(subsquares) ij x ∈ N ∀i,j ∈ N i,j FII,UAIC Lecture4 AI2025/2026 8/60",narrative
460,IA_4.pdf_p8,Example is Sudoku,diagram_extracted
461,IA_4.pdf_p8,"Variables is x ∈ {1,...,9} =: N (the values in the corresponding cells);",diagram_extracted
462,IA_4.pdf_p8,"Pairwise inequality constraints is all values in a row, column, subsquare are",diagram_extracted
463,IA_4.pdf_p9,"Example: Cryptarithmetic puzzle Variables: F T U W R O X X X 1 2 3 Domains: {0,1,2,3,4,5,6,7,8,9} Constraints: alldiff(F,T,U,W,R,O) O +O = R +10·X , etc.",narrative
464,IA_4.pdf_p9,"1 FII,UAIC Lecture4 AI2025/2026 9/60",narrative
465,IA_4.pdf_p9,Example is Cryptarithmetic puzzle,diagram_extracted
466,IA_4.pdf_p9,Variables is F T U W R O X X X,diagram_extracted
467,IA_4.pdf_p9,"Domains is {0,1,2,3,4,5,6,7,8,9}",diagram_extracted
468,IA_4.pdf_p10,"Example: Job-shop scheduling Scheduling the assembly of a car: ▶ Tasks: install axles (front, back), affix wheels, tighten nuts for each wheel, affix hubcaps, and inspect the final assembly Model each task as a variable, the value is the time the task starts (minutes) X = {Axle ,Axle ,Wheel ,Wheel ,Wheel ,Wheel , F B RF LF RB LB Nuts ,Nuts ,Nuts ,Nuts ,Cap ,Cap ,Cap ,Cap ,Inspect} RF LF RB LB RF LF RB LB ▶ Constraints: ▶ a task must occur before another (a wheel must be installed before the hubcap) T +d ≤T (T must start before T ) 1 1 2 1 2 ▶ a task takes a certain amount of time to complete FII,UAIC Lecture4 AI2025/2026 10/60",narrative
469,IA_4.pdf_p10,Example is Job-shop scheduling,diagram_extracted
470,IA_4.pdf_p10,"Tasks is install axles (front, back), affix wheels, tighten nuts for each",diagram_extracted
471,IA_4.pdf_p11,Example: Job-shop scheduling Scheduling the assembly of a car.,narrative
472,IA_4.pdf_p11,"Constraints: ▶ The axles have to be in place before the wheels are put on Axle +10 ≤ Wheel F RF ▶ After affixing the wheel, then tighten the nuts, and attach the hubcap Wheel +1 ≤ Nuts ; Nuts +2 ≤ Cap RF RF RF RF ▶ To put the axle in place, share one tool (Axle , Axle must not F B overlap) (Axle +10 ≤ Axle ) or (Axle +10 ≤ Axle ) F B B F ▶ Inspection comes last and takes 3 minutes X +d ≤ Inspect X ▶ The assembly must be done in 30 minutes D = {1,2,3,...,27} FII,UAIC Lecture4 AI2025/2026 11/60",narrative
474,IA_4.pdf_p12,"Example: timetable scheduling Scheduling the timetable for teachers and students ▶ Teachers, courses, classes, classrooms, time intervals A teacher teaches certain courses A course is given to certain classes ▶ Constraints: ▶ two courses of the same class cannot be scheduled at the same time ▶ a teacher prefers some time intervals, etc.",narrative
475,IA_4.pdf_p12,"FII,UAIC Lecture4 AI2025/2026 12/60",narrative
476,IA_4.pdf_p12,Example is timetable scheduling,diagram_extracted
477,IA_4.pdf_p13,"Example: timetable scheduling Sets: t ∈ T time, r ∈ R rooms, c ∈ C courses, s ∈ S students, (c ,c ) ∈ SA: c , c cannot overlap (the same teacher), z the courses i j i j s assigned to group s ▶ Variabiles: x = t course c is scheduled at t c y = r course c takes place in r c ▶ Constraints: ▶ x =t, for c ∈C, ... c ▶ two courses cannot be held at the same time (same teacher) If x =t and (c ,c )∈SA then x ̸=t if overlap(t ,t ) c1 1 1 2 c2 2 1 2 ▶ two courses assigned to the same group of students cannot take place at the same time If z =c and z =c and x =t then x ̸=t if overlap(t ,t ) s 1 s 2 c1 1 c2 2 1 2 ▶ two courses cannot be scheduled at the same time in the same room If (x =t )(x =t )(y =r ) then y ̸=r if overlap(t ,t ) c1 1 c2 2 c1 1 c2 1 1 2 FII,UAIC Lecture4 AI2025/2026 13/60",narrative
479,IA_4.pdf_p13,"Sets is t ∈ T time, r ∈ R rooms, c ∈ C courses, s ∈ S students,",diagram_extracted
480,IA_4.pdf_p13,"(c ,c ) ∈ SA is c , c cannot overlap (the same teacher), z the courses",diagram_extracted
481,IA_4.pdf_p13,Variabiles is x = t course c is scheduled at t,diagram_extracted
482,IA_4.pdf_p14,"Real-world CSPs ▶ Staff scheduling/nurse rostering problems ▶ Transportation scheduling, Factory scheduling, Floorplanning, etc.",narrative
483,IA_4.pdf_p14,"▶ Meeting scheduling ▶ Assignment problems (who teaches, at what class) ▶ Hardware configuration, etc.",narrative
484,IA_4.pdf_p14,"Notice that many real-world problems involve real-valued variables https://www.csplib.org/Problems/ FII,UAIC Lecture4 AI2025/2026 14/60",narrative
485,IA_4.pdf_p14,https is //www.csplib.org/Problems/,diagram_extracted
486,IA_4.pdf_p15,"Varieties of CSPs ▶ Discrete variables (finite domains: size d =⇒ O(dn) complete assignments and infinite domains), continuous variables ▶ Constraints ▶ Unary: involve a single variable ex: SA̸=green ▶ Binary: involve pairs of variables ex: SA̸=WA Binary CSP: each constraint relates at most two variables ▶ Higher-order: involve 3 or more variables ex: cryptarithmetic column constraints ▶ Preferences (soft constraints), e.g., red is better than green often representable by a cost for each variable assignment → constrained optimization problems FII,UAIC Lecture4 AI2025/2026 15/60",narrative
487,IA_4.pdf_p15,Discrete variables (finite domains is size d =⇒ O(dn) complete,diagram_extracted
488,IA_4.pdf_p15,Unary is involve a single variable,diagram_extracted
489,IA_4.pdf_p15,Binary is involve pairs of variables,diagram_extracted
490,IA_4.pdf_p15,Binary CSP is each constraint relates at most two variables,diagram_extracted
491,IA_4.pdf_p15,Higher-order is involve 3 or more variables,diagram_extracted
492,IA_4.pdf_p16,"Constraint graph Constraint graph: nodes are variables, arcs show constraints General-purpose CSP algorithms use the graph structure to speed up search (e.g., Tasmania is an independent subproblem!)",narrative
493,IA_4.pdf_p16,"FII,UAIC Lecture4 AI2025/2026 16/60",narrative
494,IA_4.pdf_p16,"Constraint graph is nodes are variables, arcs show constraints",diagram_extracted
495,IA_4.pdf_p17,"Content Introduction Backtracking search for CSPs Improving search The structure of the constraint graph Local search for CSPs FII,UAIC Lecture4 AI2025/2026 17/60",narrative
496,IA_4.pdf_p18,Standard search formulation (incremental) The straightforward approach: States are defined by the values assigned so far.,narrative
497,IA_4.pdf_p18,"▶ Initial state: the empty assignment, ∅ ▶ Successor function: assign a value to an unassigned variable ▶ Goal test: the current assignment is complete FII,UAIC Lecture4 AI2025/2026 18/60",narrative
498,IA_4.pdf_p18,"Initial state is the empty assignment, ∅",diagram_extracted
499,IA_4.pdf_p18,Successor function is assign a value to an unassigned variable,diagram_extracted
500,IA_4.pdf_p18,Goal test is the current assignment is complete,diagram_extracted
501,IA_4.pdf_p19,"Standard formulation (incremental search) ▶ Solutions are at depth n (n variables assigned) =⇒ use Depth-first search ▶ The branching factor is b = (n−ℓ)d at depth ℓ, hence n!dn leaves, dn possible assignments FII,UAIC Lecture4 AI2025/2026 19/60",narrative
502,IA_4.pdf_p20,"Backtracking search Variable assignments are commutative, i.e., [WA = red then NT = green] same as [NT = green then WA = red] At each node, consider the assignment to a single variable (does not conflict with the current assignment) =⇒ b = d and there are dn leaves Depth-first search for CSPs with single-variable assignments: backtracking search.",narrative
503,IA_4.pdf_p20,Backtracking is the basic uninformed algorithm for CSPs.,narrative
504,IA_4.pdf_p20,"FII,UAIC Lecture4 AI2025/2026 20/60",narrative
505,IA_4.pdf_p20,Depth-first search for CSPs with single-variable assignments is backtracking,diagram_extracted
506,IA_4.pdf_p21,"Backtracking example When a node is expanded, each subsequent state is checked for consistency before being added.",narrative
507,IA_4.pdf_p21,"FII,UAIC Lecture4 AI2025/2026 21/60",narrative
508,IA_4.pdf_p22,Backtracking search Backtracking = DFS + variable-ordering + fail-on-violation Can solve n-queens for n ≈ 25.,narrative
509,IA_4.pdf_p22,"FII,UAIC Lecture4 AI2025/2026 22/60",narrative
510,IA_4.pdf_p23,"Content Introduction Backtracking search for CSPs Improving search The structure of the constraint graph Local search for CSPs FII,UAIC Lecture4 AI2025/2026 23/60",narrative
511,IA_4.pdf_p24,Improving backtracking efficiency 1.,narrative
512,IA_4.pdf_p24,Which variable should be assigned next?,narrative
513,IA_4.pdf_p24,In what order should the values be verified?,narrative
514,IA_4.pdf_p24,Can the inevitable failure be detected earlier?,narrative
515,IA_4.pdf_p24,Can we take advantage of the problem structure?,narrative
516,IA_4.pdf_p24,"FII,UAIC Lecture4 AI2025/2026 24/60",narrative
517,IA_4.pdf_p25,"Minimum remaining values Minimum remaining values (MRV): choose the variable with the fewest legal values (most constrained variable) ”Fail-first” heuristic FII,UAIC Lecture4 AI2025/2026 25/60",narrative
518,IA_4.pdf_p25,Minimum remaining values (MRV) is choose the variable with the fewest,diagram_extracted
519,IA_4.pdf_p26,Least constraining value Least constraining value: choose the least constraining value (the one that rules out the fewest values in the remaining variables) Example: what value should we choose for Q?,narrative
520,IA_4.pdf_p26,Combining these heuristics makes 1000-queens feasible.,narrative
521,IA_4.pdf_p26,"FII,UAIC Lecture4 AI2025/2026 26/60",narrative
522,IA_4.pdf_p26,Least constraining value is choose the least constraining value,diagram_extracted
523,IA_4.pdf_p26,Example is what value should we choose for Q?,diagram_extracted
524,IA_4.pdf_p27,"Questionnaire FII,UAIC Lecture4 AI2025/2026 27/60",narrative
525,IA_4.pdf_p28,"Questionnaire FII,UAIC Lecture4 AI2025/2026 28/60",narrative
526,IA_4.pdf_p29,"Forward checking Idea: update the domain of unassigned variables (when a node is expanded, eliminate values from unassigned variables which are connected with that node) FII,UAIC Lecture4 AI2025/2026 29/60",narrative
527,IA_4.pdf_p29,Idea is update the domain of unassigned variables (when a node is,diagram_extracted
528,IA_4.pdf_p30,"Forward checking Idea: update the domain of unassigned variables (when a node is expanded, eliminate values from unassigned variables which are connected with that node) FII,UAIC Lecture4 AI2025/2026 30/60",narrative
530,IA_4.pdf_p31,"Forward checking Idea: update the domain of unassigned variables (when a node is expanded, eliminate values from unassigned variables which are connected with that node) FII,UAIC Lecture4 AI2025/2026 31/60",narrative
532,IA_4.pdf_p32,"Forward checking Idea: update the domain of unassigned variables (when a node is expanded, eliminate values from unassigned variables which are connected with that node) FII,UAIC Lecture4 AI2025/2026 32/60",narrative
534,IA_4.pdf_p33,"Forward checking FII,UAIC Lecture4 AI2025/2026 33/60",narrative
535,IA_4.pdf_p34,"Generalized look ahead FII,UAIC Lecture4 AI2025/2026 34/60",narrative
536,IA_4.pdf_p35,"Constraint propagation Forward checking propagates information from assigned to unassigned variables, but doesn’t provide early detection for all failures: NT and SA cannot both be blue!",narrative
537,IA_4.pdf_p35,There is no propagation between unassigned variables!,narrative
538,IA_4.pdf_p35,"FII,UAIC Lecture4 AI2025/2026 35/60",narrative
539,IA_4.pdf_p36,"Arc consistency Arc consistency: simplest form of propagation, makes each arc consistent.",narrative
540,IA_4.pdf_p36,"X → Y is consistent iff for every value x of X there is some allowed y of Y FII,UAIC Lecture4 AI2025/2026 36/60",narrative
541,IA_4.pdf_p36,"Arc consistency is simplest form of propagation, makes each arc consistent.",diagram_extracted
543,IA_4.pdf_p37,"X → Y is consistent iff for every value x of X there is some allowed y of Y FII,UAIC Lecture4 AI2025/2026 37/60",narrative
546,IA_4.pdf_p38,"X → Y is consistent iff for every value x of X there is some allowed y of Y If X loses a value, neighbors of X need to be rechecked.",narrative
547,IA_4.pdf_p38,"FII,UAIC Lecture4 AI2025/2026 38/60",narrative
550,IA_4.pdf_p39,"X → Y is consistent iff for every value x of X there is some allowed y If X loses a value, neighbors of X need to be rechecked.",narrative
551,IA_4.pdf_p39,Arc consistency detects failure earlier than forward checking.,narrative
552,IA_4.pdf_p39,Can be run as a preprocessing step or after each assignment.,narrative
553,IA_4.pdf_p39,"FII,UAIC Lecture4 AI2025/2026 39/60",narrative
555,IA_4.pdf_p40,"Arc consistency algorithm O(n2d3), can be reduced to O(n2d2) FII,UAIC Lecture4 AI2025/2026 40/60",narrative
556,IA_4.pdf_p41,"Constraint propagation ▶ 3-consistent (Path consistency), k-consistency k-consistency: any consistent assignment of k −1 variables can be extended to an instantiation of k variables If a CSP with n variables is n-consistent, then there is no need for backtracking.",narrative
557,IA_4.pdf_p41,"▶ A large usage of constraint propagation techniques implies an increase in CPU time ▶ a tradeoff between pruning and searching; if pruning takes longer than searching, it is not worth it FII,UAIC Lecture4 AI2025/2026 41/60",narrative
558,IA_4.pdf_p41,k-consistency is any consistent assignment of k −1 variables can be,diagram_extracted
559,IA_4.pdf_p42,"Conflict-Directed Backjumping (CBJ) ▶ Backtracking: backtracks to the first point where a variable can be assigned a new value ▶ it backs up ONE level in the search tree at a time ▶ When we hit a dead end due to an inconsistency, we can try and deduce the reason for the problem ▶ rather than backing up one level in the search tree, we can try to go directly to one of the variables that caused the problem FII,UAIC Lecture4 AI2025/2026 42/60",narrative
560,IA_4.pdf_p42,Backtracking is backtracks to the first point where a variable can be,diagram_extracted
561,IA_4.pdf_p43,Conflict-Directed Backjumping ▶ Idea: Maintain a CONFLICT SET for every variable (updated as we assign values to variables) ▶ Assume we are setting X .,narrative
562,IA_4.pdf_p43,The CONFLICT SET for X is the set of i i PREVIOUSLY ASSIGNED VARIABLES connected to X by a i constraint.,narrative
563,IA_4.pdf_p43,"▶ When no assignment is found for the current variable X , i BACKJUMP to the deepest X in the conflict set of X k i ▶ Update the conflict set of X k CONFLICT SET(X ) = k CONFLICT SET(X )∪CONFLICT SET(X )\X k i k FII,UAIC Lecture4 AI2025/2026 43/60",narrative
564,IA_4.pdf_p43,Idea is Maintain a CONFLICT SET for every variable (updated as we,diagram_extracted
565,IA_4.pdf_p44,"Conflict-Directed Backjumping Example: map coloring X is not in the conflict set of X , so back up to the closest variable from 7 3 the conflict set (X ) 6 FII,UAIC Lecture4 AI2025/2026 44/60",narrative
566,IA_4.pdf_p44,Example is map coloring,diagram_extracted
567,IA_4.pdf_p45,"Conflict-Directed Backjumping FII,UAIC Lecture4 AI2025/2026 45/60",narrative
568,IA_4.pdf_p46,"Conflict-Directed Backjumping FII,UAIC Lecture4 AI2025/2026 46/60",narrative
569,IA_4.pdf_p47,"Content Introduction Backtracking search for CSPs Improving search The structure of the constraint graph Local search for CSPs FII,UAIC Lecture4 AI2025/2026 47/60",narrative
570,IA_4.pdf_p48,"Problem structure Tasmania and mainland are independent subproblems, identifiable as connected components of the constraint graph FII,UAIC Lecture4 AI2025/2026 48/60",narrative
571,IA_4.pdf_p49,"Problem structure Suppose each subproblem has c variables, out of n total.",narrative
572,IA_4.pdf_p49,"Worst-case solution cost is n/c ·dc, linear in n. Example: n = 80, d = 2, c = 20 280 = 4 billion years at 10 million nodes/sec vs. 4·220 = 0.4 seconds at 10 million nodes/sec Rare cases.",narrative
573,IA_4.pdf_p49,"FII,UAIC Lecture4 AI2025/2026 49/60",narrative
574,IA_4.pdf_p49,"Example is n = 80, d = 2, c = 20",diagram_extracted
575,IA_4.pdf_p50,"Tree-structured CSPs Theorem: if the constraint graph has no cycles, the CSP can be solved in O(nd2) time.",narrative
576,IA_4.pdf_p50,"(Compare to general CSPs, where worst-case time is O(dn)) FII,UAIC Lecture4 AI2025/2026 50/60",narrative
577,IA_4.pdf_p50,"Theorem is if the constraint graph has no cycles, the CSP can be solved in",diagram_extracted
578,IA_4.pdf_p51,"Algorithm for tree-structured CSPs A CSP problem is directed arc-consistent for an ordering of variables X ,X ,...,X iff each X is arc-consistent with X ,∀j > i.",narrative
579,IA_4.pdf_p51,1 2 n i j Method: 1.,narrative
580,IA_4.pdf_p51,"Choose a variable as root, order variables from root to leaves s.t.",narrative
581,IA_4.pdf_p51,every node’s parent precedes it in the ordering 2.,narrative
582,IA_4.pdf_p51,"For j from n down to 2, apply Make-Arc-Consistent(Parent(X ),X ) j j 3.",narrative
583,IA_4.pdf_p51,"For j from 1 to n, assign X (consistent with Parent(X )) j j FII,UAIC Lecture4 AI2025/2026 51/60",narrative
584,IA_4.pdf_p52,Nearly tree-structured CSPs Cutset conditioning ▶ Choose a subset of variables S s.t.,narrative
585,IA_4.pdf_p52,"the constraint graph is a tree after deleting S (S cutset) ▶ For each possible assignment of variables from S, delete from the domain of other variables values that are inconsistent with the assignment; return the two solutions.",narrative
586,IA_4.pdf_p52,"Runtime: O(dc ·(n−c)d2), c cutset size (very fast for small c) FII,UAIC Lecture4 AI2025/2026 52/60",narrative
587,IA_4.pdf_p52,Runtime is c cutset size (very fast for small c),diagram_extracted
588,IA_4.pdf_p53,"Content Introduction Backtracking search for CSPs Improving search The structure of the constraint graph Local search for CSPs FII,UAIC Lecture4 AI2025/2026 53/60",narrative
589,IA_4.pdf_p54,"Iterative algorithms for CSPs Hill-climbing, simulated annealing typically work with ”complete” states (all variables assigned) To apply to CSPs: allow states with unsatisfied constraints operators assign new values for vars Variable selection: randomly select any conflicted variable Value selection by min-conflicts heuristic: choose the value that violates the fewest constraints min h(s) = total number of violated constraints FII,UAIC Lecture4 AI2025/2026 54/60",narrative
590,IA_4.pdf_p54,Variable selection is randomly select any conflicted variable,diagram_extracted
591,IA_4.pdf_p55,"Min-conflicts FII,UAIC Lecture4 AI2025/2026 55/60",narrative
592,IA_4.pdf_p56,Example: 4-Queens as a CSP Assume one queen in each column.,narrative
593,IA_4.pdf_p56,"Variables: Q , Q , Q , Q 1 2 3 4 Domains: D = {1,2,3,4} i Constraints: Q ̸= Q (cannot be in the same row) i j |Q −Q | ≠ |i −j| (or same diagonal) i j FII,UAIC Lecture4 AI2025/2026 56/60",narrative
594,IA_4.pdf_p56,Example is 4-Queens as a CSP,diagram_extracted
595,IA_4.pdf_p56,"Variables is Q , Q , Q , Q",diagram_extracted
596,IA_4.pdf_p56,"Domains is D = {1,2,3,4}",diagram_extracted
597,IA_4.pdf_p57,"Example: 4-Queens ▶ States: 4 queens in 4 columns (44 = 256 states) ▶ Operators: move queen (in the column) ▶ Goal test: no attacks ▶ Evaluation: h(s) = the number of attacks FII,UAIC Lecture4 AI2025/2026 57/60",narrative
598,IA_4.pdf_p57,Example is 4-Queens,diagram_extracted
599,IA_4.pdf_p57,States is 4 queens in 4 columns (44 = 256 states),diagram_extracted
600,IA_4.pdf_p57,Operators is move queen (in the column),diagram_extracted
601,IA_4.pdf_p57,Goal test is no attacks,diagram_extracted
602,IA_4.pdf_p57,Evaluation is h(s) = the number of attacks,diagram_extracted
603,IA_4.pdf_p58,"Comparison of algorithms The average number of consistency checks needed to solve the problem FII,UAIC Lecture4 AI2025/2026 58/60",narrative
604,IA_4.pdf_p59,Summary ▶ Constraint Satisfaction Problems states: assignments of variables constraints on variables ▶ Backtracking = Depth-first search with one variable assigned per node ▶ Variable ordering and value selection heuristics ▶ Forward checking prevents assignments that guarantee later failure.,narrative
605,IA_4.pdf_p59,Constraint propagation (Arc consistency) does additional work to constrain values and detect inconsistencies.,narrative
606,IA_4.pdf_p59,▶ The constraint graph can be used to analyse the problem structure.,narrative
607,IA_4.pdf_p59,Tree-structured CSPs can be solved in linear time.,narrative
608,IA_4.pdf_p59,▶ Iterative Min-conflicts is usually effective in practice.,narrative
609,IA_4.pdf_p59,"FII,UAIC Lecture4 AI2025/2026 59/60",narrative
610,IA_4.pdf_p59,states is assignments of variables,diagram_extracted
611,IA_4.pdf_p60,"Bibliography ▶ S. Russell, P. Norvig.",narrative
612,IA_4.pdf_p60,Artificial Intelligence: A Modern Approach (3rd Edition).,narrative
613,IA_4.pdf_p60,"Prentice-Hall, Englewood Cliffs, NJ, 2010 (6.",narrative
614,IA_4.pdf_p60,Constraint Satisfaction Problems) ▶ R. Bartak.,narrative
615,IA_4.pdf_p60,"Constraint Programming ▶ Solvers: CP Optimizer, OR-Tools FII,UAIC Lecture4 AI2025/2026 60/60",narrative
616,IA_4.pdf_p60,"S. Russell, P. Norvig. Artificial Intelligence is A Modern Approach (3rd",diagram_extracted
617,IA_4.pdf_p60,"Solvers is CP Optimizer, OR-Tools",diagram_extracted
618,IA_1.pdf_p1,"Artificial Intelligence 3rd year, 1st semester ”Science is the belief in the ignorance of experts.” Richard Feynman Lab teachers: Lecture teachers: Laura Cornei Ionuț Cristian Pistol E-mail: laura.cornei@info.uaic.ro E-mail: ionut.pistol@info.uaic.ro Diana Trandabăț Mădălina Răschip E-mail: diana.trandabat@info.uaic.ro E-mail:madalina.raschip@info.uaic.ro",narrative
619,IA_1.pdf_p1,E-mail is laura.cornei@info.uaic.ro,diagram_extracted
620,IA_1.pdf_p1,E-mail is ionut.pistol@info.uaic.ro,diagram_extracted
621,IA_1.pdf_p1,E-mail is diana.trandabat@info.uaic.ro,diagram_extracted
622,IA_1.pdf_p1,E-mail is madalina.raschip@info.uaic.ro,diagram_extracted
623,IA_1.pdf_p2,"Course information Timetable Course webpage Discord server Evaluation: ● Points given for project tasks (4) and written tests (3) at seminars(4 X 1 + 3 X 2 = L, maximum 10).",narrative
624,IA_1.pdf_p2,"● Project (PP, maximum 10).",narrative
625,IA_1.pdf_p2,"● Written exam (E, maximum 10).",narrative
626,IA_1.pdf_p2,● Bonus points for course and lab activity (B).,narrative
627,IA_1.pdf_p2,● Grades are given using the formula ROUND(L*0.4 + PP*0.2 + E*0.4 + B).,narrative
628,IA_1.pdf_p2,To pass you need E at least 5 and final score of at least 4.5.,narrative
629,IA_1.pdf_p4,Related course materials CS221: Artificial Intelligence: Principles and Techniques Lecture Videos | Artificial Intelligence | Electrical Engineering and Computer Science | MIT OpenCourseWare,narrative
630,IA_1.pdf_p4,CS221 is Artificial Intelligence: Principles and Techniques,diagram_extracted
631,IA_1.pdf_p5,"Defining Artificial Intelligence Artificial Intelligence: Wikipedia, Britannica, Merriam-Webster Intelligence: Wikipedia, Britannica, Merriam-Webster",narrative
632,IA_1.pdf_p5,"Artificial Intelligence is Wikipedia, Britannica, Merriam-Webster",diagram_extracted
633,IA_1.pdf_p5,"Intelligence is Wikipedia, Britannica, Merriam-Webster",diagram_extracted
634,IA_1.pdf_p6,Is this AI?,narrative
635,IA_1.pdf_p7,How about this?,narrative
636,IA_1.pdf_p8,This is not AI,narrative
637,IA_1.pdf_p9,Defining AI ● Turing test: is an average human able to distinguish between a human and a computer behind two terminals?,narrative
638,IA_1.pdf_p9,● Chinese room: is using rules equivalent to understanding?,narrative
639,IA_1.pdf_p9,● Strong vs weak AI,narrative
640,IA_1.pdf_p9,● Turing test is is an average human able to distinguish between a human,diagram_extracted
641,IA_1.pdf_p9,● Chinese room is is using rules equivalent to understanding?,diagram_extracted
645,IA_1.pdf_p13,"To conclude this interlude… ● ChatGPTs are conversational agents, not AIs or LLMs ● Current conversational agents don’t think, they just re-use approximate human reasoning ● They need to hallucinate to cover inevitable lack of coverage ● They are (and should be used as) powerful search and summarisation engines, editors and virtual assistants.",narrative
646,IA_1.pdf_p13,● For this year’s project you will be required to use them.,narrative
647,IA_1.pdf_p14,Defining AI Knowledge vs intelligence Making decisions,narrative
652,IA_1.pdf_p16,Connectionism vs computationalism: is intelligent behaviour a consequence or a goal?,narrative
653,IA_1.pdf_p16,Connectionism: intelligence is a product of Computationalism: intelligence is a product structure of functions,narrative
654,IA_1.pdf_p16,Connectionism vs computationalism is is intelligent,diagram_extracted
655,IA_1.pdf_p16,Connectionism is intelligence is a product of Computationalism: intelligence is a product,diagram_extracted
656,IA_1.pdf_p17,"Four perspectives on AI: Acting humanly ● Computer manifest human capabilities: NLP, knowledge representation, reasoning, learning ● Pro: human intelligence is the highest form ● Cons: bird flight is the best kind of flight?",narrative
657,IA_1.pdf_p17,Four perspectives on AI is Acting humanly,diagram_extracted
658,IA_1.pdf_p17,"capabilities is NLP, knowledge",diagram_extracted
659,IA_1.pdf_p17,● Pro is human intelligence is the,diagram_extracted
660,IA_1.pdf_p17,● Cons is bird flight is the best,diagram_extracted
661,IA_1.pdf_p18,Four perspectives on AI: Thinking humanly ● Computer has similar thinking mechanisms as humans: artificial “brains” - replicated biological cognitive processes ● Pro: easy to explain and evaluate results ● Cons: do we know how humans think?,narrative
662,IA_1.pdf_p18,Four perspectives on AI is Thinking humanly,diagram_extracted
663,IA_1.pdf_p18,humans is artificial “brains” -,diagram_extracted
664,IA_1.pdf_p18,● Pro is easy to explain and,diagram_extracted
665,IA_1.pdf_p18,● Cons is do we know how,diagram_extracted
666,IA_1.pdf_p19,"Four perspectives on AI: Thinking rationally ● Computer reasons using an accepted deductive system (a set of logical rules) ● Pro: Easy to replicate, easy to prove ● Cons: Informal knowledge is not conductive to formal rules, logical solutions are not conductive to informal realities",narrative
667,IA_1.pdf_p19,Four perspectives on AI is Thinking rationally,diagram_extracted
668,IA_1.pdf_p19,"● Pro is Easy to replicate, easy to",diagram_extracted
669,IA_1.pdf_p19,● Cons is Informal knowledge is,diagram_extracted
670,IA_1.pdf_p20,"Four perspectives on AI: Acting rationally ● Computer produces rational results: rational agents with well defined scopes ● Pro: most useful results, most common type of AI ● Cons: who defines the goals?",narrative
671,IA_1.pdf_p20,can it really do everything?,narrative
672,IA_1.pdf_p20,Four perspectives on AI is Acting rationally,diagram_extracted
673,IA_1.pdf_p20,results is rational agents with,diagram_extracted
674,IA_1.pdf_p20,"● Pro is most useful results, most",diagram_extracted
675,IA_1.pdf_p20,● Cons is who defines the goals?,diagram_extracted
676,IA_1.pdf_p21,Four functional types of AI ● Reactive Machines: AI produces an output based on the provided current goal.,narrative
677,IA_1.pdf_p21,"Significant techniques: rules (deterministic or stochastic), neural networks, state and variable based models.",narrative
678,IA_1.pdf_p21,● Limited Memory: AI adapts using current experiences in order to improve future behaviour.,narrative
679,IA_1.pdf_p21,"Significant techniques: reinforcement learning, genetic algorithms - evolutive networks, LSTM models.",narrative
680,IA_1.pdf_p21,"● Theory of Mind: AI capable of analyzing, processing and predicting the human mind (behaviour and goals).",narrative
681,IA_1.pdf_p21,● Self Aware: AI capable of setting it’s own goals and changing its own behaviour.,narrative
682,IA_1.pdf_p21,● Reactive Machines is AI produces an output based on the provided current,diagram_extracted
683,IA_1.pdf_p21,"goal. Significant techniques is rules (deterministic or stochastic), neural",diagram_extracted
684,IA_1.pdf_p21,● Limited Memory is AI adapts using current experiences in order to improve,diagram_extracted
685,IA_1.pdf_p21,"future behaviour. Significant techniques is reinforcement learning, genetic",diagram_extracted
686,IA_1.pdf_p21,"● Theory of Mind is AI capable of analyzing, processing and predicting the",diagram_extracted
687,IA_1.pdf_p21,● Self Aware is AI capable of setting it’s own goals and changing its own,diagram_extracted
688,IA_1.pdf_p22,Defining a reasonable and practical decision system An decision (intelligent) system has to be able to: ● Understand a model and a goal ● Recognize and be able to employ means to reach the goal ● Decide if and when to use those means ● Provide a satisfactory answer for the established goal,narrative
689,IA_1.pdf_p23,"The pillars of modern AI Data Structures Modeling Databases Graph Algorithms Formal Languages, Automata and Compilers Knowledge Based Systems Logics for Computer Science Algorithms Design Graph Algorithms Reasoning Learning Probabilities and Statistics Machine Learning Neural Networks",narrative
690,IA_1.pdf_p24,"Modeling An AI engine should be able to describe, work with and output real-world data All models are lossy",narrative
691,IA_1.pdf_p25,"Reasoning An AI engine should be able to discover new data (concepts, relations) from the available data Inferred data can be contradicted by real data - exceptions are messy",narrative
692,IA_1.pdf_p26,An AI engine should be able to adapt it’s model for particular contexts What is learned is at best as good as the available data,narrative
693,IA_1.pdf_p27,First approach: state-based models ● in what way should I change the current state of the problem in order to get closer or reach the goal?,narrative
694,IA_1.pdf_p27,"● compute a solution (algorithm - sequence of transitions) starting from an initial state and ending in the goal state ● mostly covered by search strategies, reasoning systems, AI for games",narrative
695,IA_1.pdf_p27,First approach is state-based models,diagram_extracted
696,IA_1.pdf_p28,Second approach: variable-based models ● what formula (function) can be applied to the problem data in order to output the goal?,narrative
697,IA_1.pdf_p28,"● start from arbitrary formulas, test them over the expected results and adjust accordingly ● mostly covered by machine learning and constraint satisfaction problems",narrative
698,IA_1.pdf_p28,Second approach is variable-based models,diagram_extracted
699,IA_1.pdf_p29,Course synopsis Weeks 2 and 3: State-based models ● (Decision) problems ● Representing states ● Search strategies ● Deterministic and non-deterministic problem spaces ?,narrative
700,IA_1.pdf_p29,Can the computer solve NP-complete problems?,narrative
701,IA_1.pdf_p29,Weeks 2 and 3 is State-based models,diagram_extracted
702,IA_1.pdf_p30,Course synopsis Week 4: Constraint satisfaction problems ● Variable-based models ● Soft constraints ● Optimisations Can the computer satisfy constraints over variables in a model?,narrative
703,IA_1.pdf_p30,Week 4 is Constraint satisfaction problems,diagram_extracted
704,IA_1.pdf_p31,Course synopsis Week 5: Games ● Types of games ● Games theory ● Strategies Can the computer play games competitively?,narrative
706,IA_1.pdf_p32,Course synopsis Weeks 6: Neural networks ● Perceptrons ● Machine learning ● Applications in games and NLP Can the computer learn anything?,narrative
707,IA_1.pdf_p32,Weeks 6 is Neural networks,diagram_extracted
708,IA_1.pdf_p33,Course synopsis Weeks 6-7-10: Reinforcement learning and applications ● Markov decision process ● Q Learning,narrative
709,IA_1.pdf_p33,Weeks 6-7-10 is Reinforcement learning,diagram_extracted
710,IA_1.pdf_p34,Course synopsis Weeks 9-11: Knowledge representation and NLP ● Ontologies ● Understanding natural language ● Language ambiguity Can the computer talk to us using our language?,narrative
711,IA_1.pdf_p34,Weeks 9-11 is Knowledge representation,diagram_extracted
712,IA_1.pdf_p35,Course synopsis Week 12: Bayesian networks ● Reasoning with probabilities ● Independence and conditional independence Can the computer decide and learn on probabilistic data?,narrative
713,IA_1.pdf_p35,Week 12 is Bayesian networks,diagram_extracted
714,IA_1.pdf_p36,Course synopsis Week 13: Planning ● STRIPS and PDDL ● Forward and backward search Can the computer find a plan which is guaranteed to succeed?,narrative
715,IA_1.pdf_p36,Week 13 is Planning,diagram_extracted
716,IA_1.pdf_p37,More about AI Why Intelligence Is Not a Computational Process von Neumann bottleneck Reasoning agents,narrative
717,IA_11.pdf_p1,Bayesian networks AI 2025/2026,narrative
718,IA_11.pdf_p2,"Content Introduction Inference Independence Bayes’ rule Bayesian networks Inference in Bayesian Networks FII,UAIC Lecture12 AI2025/2026 2/53",narrative
719,IA_11.pdf_p3,Probabilities The probability of a proposition is equal to the sum of the probabilities of worlds in which it holds.,narrative
720,IA_11.pdf_p3,"(cid:88) P(ϕ) = P(w), ϕ proposition w∈ϕ Example: P(Total = 11) = P((5,6))+P((6,5)) = 1/36+1/36 = 1/18.",narrative
721,IA_11.pdf_p3,Let’s suppose P(doubles) = 1/4.,narrative
722,IA_11.pdf_p3,"Unconditional probabilities (prior) FII,UAIC Lecture12 AI2025/2026 3/53",narrative
724,IA_11.pdf_p3,"Example is P(Total = 11) = P((5,6))+P((6,5)) = 1/36+1/36 = 1/18.",diagram_extracted
725,IA_11.pdf_p4,"Conditional probabilities Conditional probabilities (posterior) P(A|B) is the fraction of possible worlds where B is true and then A is also true ▶ probability of A, given B P(a∧b) P(a|b) = P(b) Example: P(doubles|Die = 5) = P(doubles∧Die1=5) .",narrative
726,IA_11.pdf_p4,"1 P(Die1=5) The product rule: P(a∧b) = P(a|b)P(b) FII,UAIC Lecture12 AI2025/2026 4/53",narrative
727,IA_11.pdf_p4,Example is P(doubles|Die = 5) = .,diagram_extracted
728,IA_11.pdf_p4,The product rule is P(a∧b) = P(a|b)P(b),diagram_extracted
729,IA_11.pdf_p5,"Probabilities ▶ The probability distribution of a discrete random variable P(Weather = sunny) = 0.6 P(Weather = rain) = 0.1 P(Weather = cloudy) = 0.29 P(Weather = snow) = 0.01 P(Weather) = ⟨0.6,0.1,0.29,0.01⟩ ▶ Probability density function to describe the probability distribution of a continuous random variable P(NoonTemp = x) = Uniform (x) the noon temperature is [18C,26C] uniformly distributed btw 18C and 26C P(x) = lim P(x ≤ X ≤ x +dx)/dx dx→0 FII,UAIC Lecture12 AI2025/2026 5/53",narrative
730,IA_11.pdf_p6,Probabilities ▶ Joint probability distribution: the probabilities of all combinations of values of vars.,narrative
731,IA_11.pdf_p6,"P(Weather,Cavity): a 4 x 2 table Obs: instead of 8 ecuations, we could use P(Weather,Cavity) = P(Weather|Cavity)P(Cavity).",narrative
732,IA_11.pdf_p6,"Full joint probability distribution: the joint distribution for all of the random variables Cavity,Toothache,Weather → P(Cavity,Toothache,Weather) FII,UAIC Lecture12 AI2025/2026 6/53",narrative
733,IA_11.pdf_p6,Joint probability distribution is the probabilities of all combinations of,diagram_extracted
734,IA_11.pdf_p6,"P(Weather,Cavity) is a 4 x 2 table",diagram_extracted
735,IA_11.pdf_p6,"Obs is instead of 8 ecuations, we could use",diagram_extracted
736,IA_11.pdf_p6,Full joint probability distribution is the joint distribution for all of,diagram_extracted
737,IA_11.pdf_p7,Example: joint probability distribution toothache ¬toothache catch ¬catch catch ¬catch cavity 0.108 0.012 0.072 0.008 ¬cavity 0.016 0.064 0.144 0.576 Boolean vars.,narrative
738,IA_11.pdf_p7,"Toothache,Cavity,Catch; 3 binary vars: 23–1 = 7 independent parameters; For n boolean vars, the table has the size O(2n).",narrative
739,IA_11.pdf_p7,"P(cavity∨toothache) = 0.108+0.012+0.072+0.008+0.016+0.064 = 0.28 FII,UAIC Lecture12 AI2025/2026 7/53",narrative
740,IA_11.pdf_p7,Example is joint probability distribution,diagram_extracted
741,IA_11.pdf_p7,"Boolean vars. Toothache,Cavity,Catch; 3 binary vars is = 7",diagram_extracted
742,IA_11.pdf_p8,"Content Introduction Inference Independence Bayes’ rule Bayesian networks Inference in Bayesian Networks FII,UAIC Lecture12 AI2025/2026 8/53",narrative
743,IA_11.pdf_p9,Marginal distribution Marginal distribution for a var / a subset of vars.,narrative
744,IA_11.pdf_p9,▶ Marginalization: sum the probabilities for each possible value of the other variables.,narrative
745,IA_11.pdf_p9,"(cid:88) P(Y) = P(Y,z) z∈Z (cid:80) Example: P(Cavity)= P(Cavity,z) z∈{Catch,Toothache} Marginal probability P(cavity)=0.108+0.012+0.072+0.008=0.2 toothache ¬toothache catch ¬catch catch ¬catch cavity 0.108 0.012 0.072 0.008 ¬cavity 0.016 0.064 0.144 0.576 ▶ (cid:80) Conditioning P(Y) = P(Y|z)P(z) z FII,UAIC Lecture12 AI2025/2026 9/53",narrative
746,IA_11.pdf_p9,Marginalization is sum the probabilities for each possible value of the,diagram_extracted
749,IA_11.pdf_p9,"Example is P(Cavity)= P(Cavity,z)",diagram_extracted
750,IA_11.pdf_p9,▶ (cid is 80),diagram_extracted
751,IA_11.pdf_p10,"Probabilistic inference Probabilistic inference: the computation of conditional probabilities, given certain observations.",narrative
752,IA_11.pdf_p10,"Inference procedure: let’s consider the var X, E the list of evidence variables, e the list of observed values, Y the remaining unobserved vars.",narrative
753,IA_11.pdf_p10,"(cid:88) P(X|e) = αP(X,e) = α P(X,e,y) (1) y FII,UAIC Lecture12 AI2025/2026 10/53",narrative
754,IA_11.pdf_p10,"Probabilistic inference is the computation of conditional probabilities, given",diagram_extracted
755,IA_11.pdf_p10,"Inference procedure is let’s consider the var X, E the list of evidence",diagram_extracted
757,IA_11.pdf_p11,"Conditional probabilities: example toothache ¬toothache catch ¬catch catch ¬catch cavity 0.108 0.012 0.072 0.008 ¬cavity 0.016 0.064 0.144 0.576 P(cavity ∧toothache) 0.108+0.012 P(cavity|toothache)= = =0.6 P(toothache) 0.108+0.012+0.016+0.064 P(¬cavity|toothache)=0.4 P(Cavity|toothache)=αP(Cavity,toothache) =α[P(Cavity,toothache,catch)+P(Cavity,toothache,¬catch)] =α[⟨0.108,0.016⟩+⟨0.012,0.064⟩]=α⟨0.12,0.08⟩=⟨0.6,0.4⟩ Obs: the term 1/P(toothache) const - normalization const for the prob.",narrative
758,IA_11.pdf_p11,"distribution P(Cavity|toothche) FII,UAIC Lecture12 AI2025/2026 11/53",narrative
759,IA_11.pdf_p11,Conditional probabilities is example,diagram_extracted
760,IA_11.pdf_p11,Obs is the term 1/P(toothache) const - normalization const for the prob. distribution,diagram_extracted
761,IA_11.pdf_p12,"Content Introduction Inference Independence Bayes’ rule Bayesian networks Inference in Bayesian Networks FII,UAIC Lecture12 AI2025/2026 12/53",narrative
762,IA_11.pdf_p13,"Independence The variables X and Y are independent: P(X|Y) = P(X) or P(Y|X) = P(Y) or P(X,Y) = P(X)P(Y).",narrative
763,IA_11.pdf_p13,Example: add the var Weather.,narrative
764,IA_11.pdf_p13,Less information needed to specify the joint probability distribution.,narrative
765,IA_11.pdf_p13,The joint distribution can be factored into two distributions.,narrative
766,IA_11.pdf_p13,"FII,UAIC Lecture12 AI2025/2026 13/53",narrative
767,IA_11.pdf_p13,The variables X and Y are independent is P(X|Y) = P(X) or,diagram_extracted
768,IA_11.pdf_p13,Example is add the var Weather.,diagram_extracted
769,IA_11.pdf_p14,"Independence: example ▶ Use the product rule P(toothache,catch,cavity,cloudy)= P(cloudy|toothache,catch,cavity)P(toothache,catch,cavity) ▶ Obs: dental problems do not influence the weather and vice versa.",narrative
770,IA_11.pdf_p14,"P(cloudy|toothache,catch,cavity)=P(cloudy) independence (marginal, absolute) ▶ Deduce P(toothache,catch,cavity,cloudy)=P(cloudy)P(toothache,catch,cavity) and P(Toothache,Catch,Cavity,Weather)=P(Toothache,Catch,Cavity)P(Weather) FII,UAIC Lecture12 AI2025/2026 14/53",narrative
771,IA_11.pdf_p14,Independence is example,diagram_extracted
772,IA_11.pdf_p14,Obs is dental problems do not influence the weather and vice versa.,diagram_extracted
773,IA_11.pdf_p15,"Content Introduction Inference Independence Bayes’ rule Bayesian networks Inference in Bayesian Networks FII,UAIC Lecture12 AI2025/2026 15/53",narrative
774,IA_11.pdf_p16,"Bayes’ rule, ’73 Using the product rule: P(a∧b) = P(a|b)P(b) P(a∧b) = P(b|a)P(a) → P(a|b)P(b) P(b|a) = P(a) P(X|Y)P(Y) P(Y|X) = P(X) FII,UAIC Lecture12 AI2025/2026 16/53",narrative
775,IA_11.pdf_p17,"Bayes’s rule P(E|H)P(H) P(H|E) = P(E) ▶ H hypothesis, E evidence (comes from observed data) ▶ P(H|E) a posteriori probability of the hypothesis, given the evidence (posterior) ▶ P(E|H) likelihood: the probability of observing the evidence, given the hypothesis ▶ P(H) prior probability (the degree of confidence in the hypothesis) FII,UAIC Lecture12 AI2025/2026 17/53",narrative
776,IA_11.pdf_p17,"P(E|H) likelihood is the probability of observing the evidence, given",diagram_extracted
777,IA_11.pdf_p18,"Bayes’s rule We know the evidence (the effect of an unknown cause), and we want to find the cause: P(effect|cause)P(cause) P(cause|effect) = P(effect) Medical diagnosis: the doctor knows P(symptoms|disease) and identifies the diagnostic P(disease|symptoms).",narrative
778,IA_11.pdf_p18,"Example: P(s|m) = 0.7 the meningitis causes a stiff neck in 70% of cases, P(m) = 1/50000, P(s) = 0.01.",narrative
779,IA_11.pdf_p18,"P(s|m)P(m) 0.7·1/50000 P(m|s) = = = 0.0014 P(s) 0.01 FII,UAIC Lecture12 AI2025/2026 18/53",narrative
780,IA_11.pdf_p18,Medical diagnosis is the doctor knows P(symptoms|disease) and identifies,diagram_extracted
781,IA_11.pdf_p18,"Example is P(s|m) = 0.7 the meningitis causes a stiff neck in 70% of cases,",diagram_extracted
782,IA_11.pdf_p19,"Bayes’s rule General form P(Y|X) = αP(X|Y)P(Y) α the normalization const If we have more than one evidence var ▶ If we know the joint probability distribution P(Cavity|toothache ∧catch) = α⟨0.108,0.016⟩ ≈ ⟨0.871,0.129⟩.",narrative
783,IA_11.pdf_p19,"▶ Using Bayes’ theorem: P(Cavity|toothache ∧catch) = αP(toothache ∧catch|Cavity)P(Cavity) = αP(toothache|Cavity)P(catch|Cavity)P(Cavity) Conditional independence of toothache and catch, given Cavity FII,UAIC Lecture12 AI2025/2026 19/53",narrative
784,IA_11.pdf_p20,"Bayes’ theorem Conditional independence of two variables X and Y given Z: P(X,Y|Z) = P(X|Z)P(Y|Z).",narrative
785,IA_11.pdf_p20,"Example: P(Toothache,Catch|Cavity)=P(Toothache|Cavity)P(Catch|Cavity) P(Toothache,Catch,Cavity)=P(Toothache,Catch|Cavity)P(Cavity) =P(Toothache|Cavity)P(Catch|Cavity)P(Cavity) For n symptoms cond indep, given Cavity, the size of the representation grows linearly.",narrative
786,IA_11.pdf_p20,"(cid:81) P(Cause,Effect ,...,Effect )=P(Cause) P(Effect|Cause) 1 n i i (Naive Bayes) FII,UAIC Lecture12 AI2025/2026 20/53",narrative
787,IA_11.pdf_p20,"Example is P(Toothache,Catch|Cavity)=P(Toothache|Cavity)P(Catch|Cavity)",diagram_extracted
788,IA_11.pdf_p20,(cid is 81),diagram_extracted
789,IA_11.pdf_p21,Independence and conditional independence Example 1.,narrative
790,IA_11.pdf_p21,Ion (A) and Maria (B) flip a coin 100 times.,narrative
791,IA_11.pdf_p21,Everyone has a different coin.,narrative
792,IA_11.pdf_p21,"▶ independent events P(A|B) = P(A),P(B|A) = P(B) The result of one experiment does not influence the result of the other experiment.",narrative
793,IA_11.pdf_p21,"Ion and Maria use the same coin ▶ if the coin is not correct, event A (John) may bring informations on event B (Mary) ▶ the events are not independent FII,UAIC Lecture12 AI2025/2026 21/53",narrative
794,IA_11.pdf_p22,Independence and conditional independece Example 3.,narrative
795,IA_11.pdf_p22,"Ion and Maria live in different areas of the city and come to work by tram and car, respectively ▶ ”John was late” and ”Mary was late” may be considered independent ▶ if the tram drivers are on strike, then road traffic also increases; the events are conditionally independent FII,UAIC Lecture12 AI2025/2026 22/53",narrative
796,IA_11.pdf_p23,"Content Introduction Inference Independence Bayes’ rule Bayesian networks Inference in Bayesian Networks FII,UAIC Lecture12 AI2025/2026 23/53",narrative
797,IA_11.pdf_p24,"Bayesian networks ▶ Probabilistic graphical models that represent dependencies between variables ▶ Representation of information related to probabilistic events help us to perform reasoning efficiently ▶ Applications: systems for medical diagnosis, estimation of psychological characteristics from tests, environment modeling (polar bear populations), etc.",narrative
798,IA_11.pdf_p24,"FII,UAIC Lecture12 AI2025/2026 24/53",narrative
799,IA_11.pdf_p24,"Applications is systems for medical diagnosis, estimation of",diagram_extracted
800,IA_11.pdf_p25,"Bayesian network Is an acyclic digraph ▶ each node corresponds to a random variable (event) ▶ an arc from X to Y (X is the parent of Y): a relationship X has a direct influence on Y ▶ each node X has a conditional probability distribution i P(X |Parents(X )) (the effect of parents on the node) i i The network topology specifies conditional independence relationships: Toothache and Catch are conditionally independent, given Cavity.",narrative
801,IA_11.pdf_p25,"FII,UAIC Lecture12 AI2025/2026 25/53",narrative
802,IA_11.pdf_p25,an arc from X to Y (X is the parent of Y) is a relationship X has a,diagram_extracted
803,IA_11.pdf_p26,Bayesian networks: example Metastatic cancer is a possible cause of brain tumors and is also an explanation for increased total serum calcium.,narrative
804,IA_11.pdf_p26,Any of these could explain a patient going into a coma.,narrative
805,IA_11.pdf_p26,Severe headache is also associated with brain tumors.,narrative
806,IA_11.pdf_p26,Obs: the conditional probability tables are given next.,narrative
807,IA_11.pdf_p26,"FII,UAIC Lecture12 AI2025/2026 26/53",narrative
808,IA_11.pdf_p26,Bayesian networks is example,diagram_extracted
809,IA_11.pdf_p26,Obs is the conditional probability tables are given next.,diagram_extracted
810,IA_11.pdf_p27,"Bayesian networks: example An alarm system activates in the event of a burglary, but also in the event of an earthquake.",narrative
811,IA_11.pdf_p27,Neighbors John and Mary call the owner at work if they hear the alarm.,narrative
812,IA_11.pdf_p27,Burglary and earthquakes influence the probability of triggering the alarm.,narrative
813,IA_11.pdf_p27,The alarm influences the probability that John and Mary call.,narrative
814,IA_11.pdf_p27,We want to estimate the probability of a burglary based on who called.,narrative
815,IA_11.pdf_p27,"FII,UAIC Lecture12 AI2025/2026 27/53",narrative
817,IA_11.pdf_p28,"Simple queries A conjunction P(X = x ∧...∧X = x ) 1 1 n n The assumption of the Bayesian network model is that a variable depends only on its parents: n (cid:89) P(x ,...x ) = P(x |parents(X )) (2) 1 n i i i=1 Example: the probability of the alarm to start when there was no burglary or earthquake and John and Mary called P(j,m,a,¬b,¬e) = P(j|a)P(m|a)P(a|¬b∧¬e)P(¬b)P(¬e) = 0.90·0.70·0.001·0.999·0.998 = 0.000628 FII,UAIC Lecture12 AI2025/2026 28/53",narrative
818,IA_11.pdf_p28,(cid is 89),diagram_extracted
819,IA_11.pdf_p28,Example is the probability of the alarm to start when there was no burglary,diagram_extracted
820,IA_11.pdf_p29,"Construction of a Bayesian network We use the product rule to rewrite the joint probability distribution: P(x ,...,x ) = P(x |x ,...,x )P(x ,...,x ) 1 n n n−1 1 n−1 1 = P(x |x ,...,x )P(x |x ,...,x )...P(x |x )P(x ) n n−1 1 n−1 n−2 1 2 1 1 n (cid:89) = P(x |x ,...,x ) i i−1 1 i=1 (The chain rule) P(X |X ,...,X ) = P(X |Parents(X )) (3) i i−1 1 i i ∀X network variable, provided that Parents(X ) ⊆ {X ,...,X }.",narrative
821,IA_11.pdf_p29,"i i i−1 1 The Bayesian network is a correct representation if each node is conditionally independent on its predecessors in the ordering, given its parents.",narrative
822,IA_11.pdf_p29,"FII,UAIC Lecture12 AI2025/2026 29/53",narrative
824,IA_11.pdf_p30,"Construction of a Bayesian network ▶ Identify the set of vars {X ,...,X }.",narrative
825,IA_11.pdf_p30,Sort the variables s.t.,narrative
826,IA_11.pdf_p30,causes 1 n precede effects.,narrative
827,IA_11.pdf_p30,"▶ For i = 1,...n ▶ choose from X ,...,X a minimal set of parents s.t.",narrative
828,IA_11.pdf_p30,"equation (3) is 1 i−1 satisfied ▶ for each parent, insert an arc from it to X i ▶ add the conditional probability table P(X|Parents(X)) i i Example: P(MaryCalls|JohnCalls,Alarm,Earthquake,Burglary)=P(MaryCalls|Alarm), so Alarm is the only parent of MaryCalls FII,UAIC Lecture12 AI2025/2026 30/53",narrative
829,IA_11.pdf_p31,Topological sort ▶ Topological sort of a digraph is a linear ordering of the nodes s.t.,narrative
830,IA_11.pdf_p31,"∀A → B, A appears before B.",narrative
831,IA_11.pdf_p31,"▶ For a Bayesian network, topological sort ensures that parents will appear before children.",narrative
832,IA_11.pdf_p31,"▶ If the graph is directeda and acyclic, there is at least one solution; if there are cycles, topological sorting is not possible.",narrative
833,IA_11.pdf_p31,"FII,UAIC Lecture12 AI2025/2026 31/53",narrative
834,IA_11.pdf_p32,Topological sort: Kahn’s algorithm 1.,narrative
835,IA_11.pdf_p32,"L=∅,S ={G,A} 2.",narrative
836,IA_11.pdf_p32,"L={G},S ={A} 3. remove (GF) F cannot be added to S: ∃(AF) 4.",narrative
837,IA_11.pdf_p32,"L={G,A},S =∅ 5. remove (AF), S ={F} Time complexity: O(n+m), n 6.",narrative
838,IA_11.pdf_p32,"L={G,A,F},S =∅ vertices, m arcs 7. remove (FO), S ={O}, ... →L={G,A,F,O,X} FII,UAIC Lecture12 AI2025/2026 32/53",narrative
839,IA_11.pdf_p32,Topological sort is Kahn’s algorithm,diagram_extracted
840,IA_11.pdf_p32,F cannot be added to S is ∃(AF),diagram_extracted
841,IA_11.pdf_p32,"Time complexity is O(n+m), n",diagram_extracted
842,IA_11.pdf_p33,"Bayesian networks: advantages n variables, each influenced by at most k variables → 2k (to specify a conditional probability table) → n2k vs. Joint distribution: 2n Example: n=30 vertices, k = 5 parents → 960 vs. 109.",narrative
843,IA_11.pdf_p33,"FII,UAIC Lecture12 AI2025/2026 33/53",narrative
844,IA_11.pdf_p33,Bayesian networks is advantages,diagram_extracted
845,IA_11.pdf_p33,"Example is n=30 vertices, k = 5 parents → 960 vs. 109.",diagram_extracted
846,IA_11.pdf_p34,"Conditional independence ▶ Each variable is conditionally independent of all its non-descendants, given the parents.",narrative
847,IA_11.pdf_p34,"JohnCalls is independent of Burglary, Earthquake, MarryCalls, given Alarm.",narrative
848,IA_11.pdf_p34,"▶ Markov blanket: a node is conditionally independent of other nodes, given its parents, children, and parents of children Burglary is independent of JohnCalls and MaryCalls, given Alarm and Earthquake FII,UAIC Lecture12 AI2025/2026 34/53",narrative
849,IA_11.pdf_p34,"Markov blanket is a node is conditionally independent of other nodes,",diagram_extracted
850,IA_11.pdf_p35,"Content Introduction Inference Independence Bayes’ rule Bayesian networks Inference in Bayesian Networks FII,UAIC Lecture12 AI2025/2026 35/53",narrative
851,IA_11.pdf_p36,"Inference of marginal probabilities ▶ Compute the node probabilities, in the absence of evidence nodes ▶ For a node: compute the sum of the conditional probabilities of the possible combinations of parent values, multiplied by the parents’ probabilities of having those values.",narrative
852,IA_11.pdf_p36,"FII,UAIC Lecture12 AI2025/2026 36/53",narrative
853,IA_11.pdf_p36,For a node is compute the sum,diagram_extracted
854,IA_11.pdf_p37,"Inference of marginal probabilities: example P(f)=P(f|g,a)P(g)P(a)+P(f|g,¬a)P(g)P(¬a) +P(f|¬g,a)P(¬g)P(a)+P(f|¬g,¬a)P(¬g)P(¬a) =0.8·0.1·0.05+0.7·0.1·0.95+0.25·0.9·0.05+0.05·0.9·0.95 =0.1245 P(¬f)=1−P(f)=0.8755 FII,UAIC Lecture12 AI2025/2026 37/53",narrative
855,IA_11.pdf_p37,Inference of marginal probabilities is example,diagram_extracted
856,IA_11.pdf_p38,"Example: Fatigue and Anorexia P(o) = P(o|f)P(f)+P(o|¬f)P(¬f) = 0.6·0.1245+0.2·0.8755 = 0.25 P(¬o) = 1−P(o) = 0.75 P(x) = P(x|f)P(f)+P(x|¬f)P(¬f) = 0.5·0.1245+0.1·0.8755 = 0.15 P(¬x) = 1−P(x) = 0.85 FII,UAIC Lecture12 AI2025/2026 38/53",narrative
857,IA_11.pdf_p38,Example is Fatigue and Anorexia,diagram_extracted
858,IA_11.pdf_p39,"Inference in bayesian networks ▶ Probabilistic inference system: compute the posterior probability distribution for a set of variables, given an event ▶ X variable, E the set of evidence variables E ,...,E , e event, Y 1 m non-evidence, Y ,...Y hidden variables 1 l Posterior probability distribution P(X|e) =?",narrative
859,IA_11.pdf_p39,"Example: observe the event in which JohnCalls = true and MaryCalls = true; then, the probability of a burglary: P(Burglary|JohnCalls = true,MaryCalls = true) = ⟨0.284,0.716⟩.",narrative
860,IA_11.pdf_p39,"FII,UAIC Lecture12 AI2025/2026 39/53",narrative
861,IA_11.pdf_p39,Probabilistic inference system is compute the posterior probability,diagram_extracted
862,IA_11.pdf_p39,Example is observe the event in which JohnCalls = true and,diagram_extracted
863,IA_11.pdf_p40,"Inference by enumeration (cid:88) P(X|e) = αP(X,e) = α P(X,e,y) (1) y A Bayesian network provides a representation of the joint distribution.",narrative
864,IA_11.pdf_p40,"According to equation (2), the term P(X,e,y) can be written as a product of conditional probabilities.",narrative
865,IA_11.pdf_p40,"FII,UAIC Lecture12 AI2025/2026 40/53",narrative
867,IA_11.pdf_p41,Inference by enumeration: example Query: What is the probability of a burglary when John and Mary call?,narrative
868,IA_11.pdf_p41,"P(Burglary|JohnCalls = true,MaryCalls = true) FII,UAIC Lecture12 AI2025/2026 41/53",narrative
869,IA_11.pdf_p41,Inference by enumeration is example,diagram_extracted
870,IA_11.pdf_p41,Query is What is the probability of a burglary when John and Mary call?,diagram_extracted
871,IA_11.pdf_p42,"Inference by enumeration: example P(Burglary|JohnCalls = true,MaryCalls = true) ▶ The hidden variables are Earthquake and Alarm.",narrative
872,IA_11.pdf_p42,"(cid:80) (cid:80) P(B|j,m) = αP(B,j,m) = α P(B,j,m,e,a) e a ▶ For Burglary = true: According to (2): (cid:80) (cid:80) P(b|j,m) = α P(b)P(e)P(a|b,e)P(j|a)P(m|a) e a The term P(b) const, P(e) it doesn’t depend on a: (cid:88) (cid:88) P(b|j,m)=αP(b) P(e) P(a|b,e)P(j|a)P(m|a) e a (cid:88) =αP(b) P(e)[P(a|b,e)P(j|a)P(m|a)+ e P(¬a|b,e)P(j|¬a)P(m|¬a)] =... =α×0.00059224 FII,UAIC Lecture12 AI2025/2026 42/53",narrative
874,IA_11.pdf_p42,(cid is 80) (cid:80),diagram_extracted
876,IA_11.pdf_p42,(cid is 88) (cid:88),diagram_extracted
878,IA_11.pdf_p43,"Inference by enumeration: example For Burglary = false, according to (2): (cid:88)(cid:88) P(¬b|j,m) = α P(¬b)P(e)P(a|¬b,e)P(j|a)P(m|a) e a = ... = α×0.0014919 P(b|j,m)+P(¬b|j,m) = 1 → alpha = 479.8142 P(B|j,m) = α⟨0.00059224,0.0014919⟩ ≈ ⟨0.284,0.716⟩.",narrative
879,IA_11.pdf_p43,The probability of a burglary: 28%.,narrative
880,IA_11.pdf_p43,"Obs: to make the computation more efficient, it is recommended that the remaining nodes are first topologically sorted, i.e.",narrative
881,IA_11.pdf_p43,parents to appear before children.,narrative
882,IA_11.pdf_p43,"In this case, it will be possible to decompose the sums more easily.",narrative
883,IA_11.pdf_p43,"FII,UAIC Lecture12 AI2025/2026 43/53",narrative
885,IA_11.pdf_p43,(cid is 88)(cid:88),diagram_extracted
886,IA_11.pdf_p43,The probability of a burglary is 28%.,diagram_extracted
887,IA_11.pdf_p43,"Obs is to make the computation more efficient, it is recommended that the",diagram_extracted
888,IA_11.pdf_p44,"Example The evaluation process: FII,UAIC Lecture12 AI2025/2026 44/53",narrative
889,IA_11.pdf_p45,Inference by enumeration The algorithm evaluates expression trees in DFS order.,narrative
890,IA_11.pdf_p45,"Space complexity: O(n), n variables.",narrative
891,IA_11.pdf_p45,Time complexity: O(2n) for a network with n variables bool.,narrative
892,IA_11.pdf_p45,"FII,UAIC Lecture12 AI2025/2026 45/53",narrative
893,IA_11.pdf_p45,"The algorithm evaluates expression trees in DFS order. Space complexity is O(n),",diagram_extracted
894,IA_11.pdf_p45,n variables. Time complexity is O(2n) for a network with n variables bool.,diagram_extracted
895,IA_11.pdf_p46,"Variable elimination algorithm ▶ Obs: P(j|a)P(m|a) and P(j|¬a)P(m|¬a) are computed twice, for each value of e. ▶ Idea: perform computations from right to left (from bottom to top) and save the results.",narrative
896,IA_11.pdf_p46,"FII,UAIC Lecture12 AI2025/2026 46/53",narrative
897,IA_11.pdf_p46,"Obs is P(j|a)P(m|a) and P(j|¬a)P(m|¬a) are computed twice, for",diagram_extracted
898,IA_11.pdf_p46,Idea is perform computations from right to left (from bottom to top),diagram_extracted
899,IA_11.pdf_p47,"Variable elimination: example (cid:88) (cid:88) P(B|j,m) = αP(b) P(e) P(a|B,e)P(j|a)P(m|a) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) e a f1(B) f2(E) f3(A,B,E) f4(A) f5(A) ▶ Each factor f is a matrix indexed by the argument variables: i (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) P(j|a) 0.90 P(m|a) 0.70 f (A) = = ,f (A) = = .",narrative
900,IA_11.pdf_p47,"4 P(j|¬a) 0.05 5 P(m|¬a) 0.01 f (A,B,E) is a 2x2x2 matrix.",narrative
901,IA_11.pdf_p47,"3 (cid:88) (cid:88) P(B|j,m) = αf (B)× f (E)× f (A,B,E)×f (A)×f (A) 1 2 3 4 5 e a FII,UAIC Lecture12 AI2025/2026 47/53",narrative
902,IA_11.pdf_p47,Variable elimination is example,diagram_extracted
904,IA_11.pdf_p47,(cid is 124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125),diagram_extracted
905,IA_11.pdf_p47,(cid is 18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19),diagram_extracted
907,IA_11.pdf_p48,"Variable elimination: example (cid:88) f (B,E) = f (A,B,E)×f (A)×f (A) 6 3 4 5 a = f (a,B,E)×f (a)×f (a)+f (¬a,B,E)×f (¬a)×f (¬a) 3 4 5 3 4 5 (cid:88) f (B) = f (E)×f (B,E) 7 2 6 e = f (e)×f (B,e)+f (¬e)×f (B,¬e) 2 6 2 6 P(B|j,m) = αf (B)×f (B) 1 7 FII,UAIC Lecture12 AI2025/2026 48/53",narrative
911,IA_11.pdf_p49,Approximate inference ▶ Exact algorithms cannot be applied for complex networks with hundreds of nodes.,narrative
912,IA_11.pdf_p49,Approximate inference increase computation speed.,narrative
913,IA_11.pdf_p49,"▶ Random sampling algorithms for computing conditional probabilities: Rejection sampling, Likelihood weighting.",narrative
914,IA_11.pdf_p49,▶ generate N samples from a distribution ▶ compute an approximation Pˆ for the posterior probability (converges to the true probability P) ▶ Prior sampling The probability of generating a particular event: (cid:81) S (x ...x )= P(x|parents(X))=P(x ...x ).,narrative
915,IA_11.pdf_p49,"PS 1 n i=1 i i 1 n FII,UAIC Lecture12 AI2025/2026 49/53",narrative
917,IA_11.pdf_p50,"Prior sampling: example Consider the ordering Cloudy,Sprinkler,Rain,WetGrass.",narrative
918,IA_11.pdf_p50,"SamplefromP(WetGrass|Sprinkler =false,Rain=true)=<0.9,0.1> thevaluetrue.",narrative
919,IA_11.pdf_p50,"S (t,f,t,t)=0.5×0.9×0.8×0.9=0.324=P(t,f,t,t) PS N (x ...x ) no.",narrative
920,IA_11.pdf_p50,"of samples generated for the event x ,...,x .",narrative
921,IA_11.pdf_p50,"PS 1 n 1 n lim Pˆ(x ,...,x )=lim N (x ,...,x )/N N→∞ 1 n N→∞ PS 1 n =S (x ,...,x ) PS 1 n =P(x ...x ) 1 n If in 511 samples out of 1000 ’Rain=true’, then Pˆ(Rain=true)=0.511.",narrative
922,IA_11.pdf_p50,"FII,UAIC Lecture12 AI2025/2026 50/53",narrative
923,IA_11.pdf_p50,Prior sampling is example,diagram_extracted
924,IA_11.pdf_p51,"Rejection sampling ▶ generate samples from the prior distribution ▶ reject samples that do not fit the evidence ▶ estimate Pˆ(X = x|e) N (X,e) Pˆ(X|e) = αN (X,e) = PS PS N (e) PS Example: estimate P(Rain|Sprinkler = true) Of the 100 samples generated, 73 have ’Sprinkler = false’ (rejected) and 27 have ’Sprinkler = true’; of those 27, 8 have ’Rain = true’ and 19 ’Rain = false’.",narrative
925,IA_11.pdf_p51,"P(Rain|Sprinkler = true) ≈ NORMALIZE(< 8,19 >) =< 0.296,0.704 > FII,UAIC Lecture12 AI2025/2026 51/53",narrative
926,IA_11.pdf_p51,Example is estimate P(Rain|Sprinkler = true),diagram_extracted
927,IA_11.pdf_p52,Rejection sampling Disadvantage: many samples are rejected (the no.,narrative
928,IA_11.pdf_p52,of consistent samples decreases exponentially with the no.,narrative
929,IA_11.pdf_p52,"of evidence vars) FII,UAIC Lecture12 AI2025/2026 52/53",narrative
930,IA_11.pdf_p52,Disadvantage is many samples are rejected (the no. of consistent samples decreases,diagram_extracted
932,IA_11.pdf_p53,Artificial Intelligence: A Modern Approach.,narrative
933,IA_11.pdf_p53,Quantifying Uncertainty; Ch.,narrative
934,IA_11.pdf_p53,"Probabilistic Reasoning ▶ Belief and Decision Networks https://aispace.org/bayes/ FII,UAIC Lecture12 AI2025/2026 53/53",narrative
935,IA_11.pdf_p53,"S. Russell, P. Norvig. Artificial Intelligence is A Modern Approach. Ch.",diagram_extracted
936,IA_11.pdf_p53,Belief and Decision Networks https is //aispace.org/bayes/,diagram_extracted
937,IA_5.pdf_p1,"Artificial Intelligence 3rd year, 1st semester Week 5: Game theory “The true source of uncertainty lies in the intentions of others.” Peter L. Bernstein",narrative
938,IA_5.pdf_p1,Week 5 is Game theory,diagram_extracted
939,IA_5.pdf_p2,What is a Game?,narrative
940,IA_5.pdf_p2,An adversarial search problem A interactive decision problem where ● The solution is not built exclusively by a single strategy ● Not all final states are desirable,narrative
941,IA_5.pdf_p3,These are puzzles (decision problems) and not games (interactive decision problems)!,narrative
942,IA_5.pdf_p4,A Game always has: A starting state (the game instance) ● At least one state which determines the end of the game if ● reached At least one goal for each player (not necessarily an end state) ● Rules for each player (not always the same) ● At least two sources determining changes to the current state ●,narrative
943,IA_5.pdf_p5,Additional references Stanford Game Theory Online Yale Game Theory Giacomo Bonanno Game Theory,narrative
944,IA_5.pdf_p6,Zero sum games ● Something won by a player is lost by the other(s).,narrative
945,IA_5.pdf_p6,● Also called strictly competitive games.,narrative
946,IA_5.pdf_p6,● Many games are not zero-sum!,narrative
947,IA_5.pdf_p6,Non zero-sum game: I have n money and I want to give it to a group of m players.,narrative
948,IA_5.pdf_p6,Each of the m players has to write a sum on a piece of paper visible only to them.,narrative
949,IA_5.pdf_p6,I collect all pieces of paper and total the sums.,narrative
950,IA_5.pdf_p6,"If the total is at most as much as n, each player gets the written amount, otherwise nobody gets anything.",narrative
951,IA_5.pdf_p7,Single player games ● One active player - one controlling strategy ● At least one additional source of information ● Cannot be zero-sum games Multiple player games ● Can have different rules and goals for all players.,narrative
952,IA_5.pdf_p7,● Symmetrical games: all rules and goals are the same for all players.,narrative
953,IA_5.pdf_p7,● Symmetrical games is all rules and goals,diagram_extracted
954,IA_5.pdf_p8,"Other types of games ● If the state change is determined by multiple player decisions at the same time, the game is simultaneous.",narrative
955,IA_5.pdf_p8,"● If the state change is determined in alternative turns by each player, the game has alternate moves.",narrative
956,IA_5.pdf_p8,"● If each player knows everything about the current state of the game, all previous decisions made by all players and all rules and goals of all players, the game has perfect information.",narrative
957,IA_5.pdf_p8,"● Imperfect information games: card games, kriegspiel, others.",narrative
958,IA_5.pdf_p8,"● Imperfect information games is card games, kriegspiel, others.",diagram_extracted
959,IA_5.pdf_p9,"OK, now let’s implement a game as an interactive decision problem The game: Tic-Tac-Toe On a 3x3 board two players place alternatively pieces (first one X pieces, second one O pieces) until either one of them has three pieces in the same row, column or diagonal (in which case that player has won the game), or no more pieces can be placed (in which case the game ended in a draw).",narrative
960,IA_5.pdf_p9,"Two player game, symmetrical, zero-sum, alternate moves.",narrative
961,IA_5.pdf_p9,The game is Tic-Tac-Toe,diagram_extracted
962,IA_5.pdf_p10,"Describing a state (p, b , b , b , b , b , b , b , b , b ) 11 12 13 21 22 23 31 32 33 ● p𝞊{1,2}, 1 if we are next to move, 2 if the opponent; ● b 𝞊{0,1,2}, 0 if the b position is empty, 1 if it’s occupied by one ij ij of our pieces and 2 if it’s one of the opponents.",narrative
963,IA_5.pdf_p10,"is (1, 2, 1, 2, 2, 2, 1, 1, 2, 1)",narrative
964,IA_5.pdf_p11,"Game initialisation State initialisation (int first_player) { return (first_player, 0, 0, 0, 0, 0, 0, 0, 0, 0) ; } Select which player is moving first and send selection as parameter for initialisation.",narrative
965,IA_5.pdf_p12,Game ending int IsFinal (State S) { if same_value_column_line_diagonal(S) return value; elseif no_zero_values(S) return 0; else return -1; },narrative
966,IA_5.pdf_p13,"Transition and validation State move (State S, int position) { S[position]=S[0]; S[0]=3-S[0]; return S; } Boolean Validate (State S, int position) { return (S[position]==0); }",narrative
967,IA_5.pdf_p14,How should a strategy look like?,narrative
968,IA_5.pdf_p15,"Void strategy(State s) { While (!isFinal(s)) { If it’s my turn Choose position; If (Validate (s, position)) s = Transition(s, position); Else get other player(s) decisions and change s } }",narrative
969,IA_5.pdf_p16,"The MINIMAX strategy ● Select a depth for exploration, at the minimum it has to include a full step for all other players.",narrative
970,IA_5.pdf_p16,● Select a score function (heuristic) which evaluates a state as to its value for the other player(s).,narrative
971,IA_5.pdf_p16,"The better the score, the less desirable that state is from your perspective.",narrative
972,IA_5.pdf_p16,The opponent should either be the maximising player (higher score is better) or minimising player (lower score is better).,narrative
973,IA_5.pdf_p16,"score minimax(depth, state, next_player) { if(final(state)||(depth==0)) return score(state); else if (next_player is the minimising one) return min(minimax(depth-1,each neighbour state,next_player); else return max(minimax(depth-1,each neighbour state,next_player); }",narrative
974,IA_5.pdf_p17,"The MINIMAX strategy example S A B C 1 5 9 8 3 9 2 minimax(2, S, AI) = min (score (minimax (1, A, OP)), score (minimax (1, B, OP)), score (minimax (1, C, OP))) = min (max (1, 5, 9), max (8), max (3, 9, 2)) = 8 (choice B)",narrative
975,IA_5.pdf_p18,"MINIMAX considerations ● The objective of MINIMAX is to allow the opponent to make the worst possible moves, assuming it will always choose the best move available.",narrative
976,IA_5.pdf_p18,● The depth of exploration is crucial to the MINIMAX accuracy.,narrative
977,IA_5.pdf_p18,● The depth of exploration is limited by the necessity to store every explored state.,narrative
978,IA_5.pdf_p18,"For chess (average branching factor of 35), a depth of 4 would require 1,500,625 states.",narrative
979,IA_5.pdf_p18,An exhaustive exploration would require 1030 states.,narrative
980,IA_5.pdf_p18,● MINIMAX is predictable and assumes predictable opponents.,narrative
981,IA_5.pdf_p19,Optimising MINIMAX S MIN<=9 A MAX>=9 9,narrative
982,IA_5.pdf_p20,Optimising MINIMAX S MIN=9 A MAX=9 9 8,narrative
983,IA_5.pdf_p21,Optimising MINIMAX ?,narrative
984,IA_5.pdf_p21,S MIN=9 A MAX=9 MAX>=10 B 9 8 10,narrative
985,IA_5.pdf_p22,Optimising MINIMAX S MIN=9 A MAX=9 MAX>=10 B 9 8 10,narrative
986,IA_5.pdf_p23,"The ALPHA-BETA optimisation ● For each state up to (but not including) the selected depth, keep two values (alpha and beta), alpha being the minimum maximum up to that depth and beta being the maximum score up to that depth.",narrative
987,IA_5.pdf_p23,"● If for a state alpha is greater or equal than beta, stop exploring further accessible states.",narrative
988,IA_5.pdf_p23,● Update alpha and beta scores each time a score is computed.,narrative
989,IA_5.pdf_p23,"score alphabeta(depth, state, next_player, alpha, beta) { if(final(state)||(depth==0)) return score(state); else for all neighbour states if (next_player is minimising) score = min(alphabeta (depth-1, each_child, next_player, alpha, beta); if (score <= alpha) return alpha; if (score < beta) beta = score; else score = max(alphabeta (depth-1, each_child, next_player, alpha, beta); if (score >= beta) return beta; if (score > alpha) alpha = score; return score; } implemented alternative",narrative
990,IA_5.pdf_p24,ALPHA-BETA example alpha=3 S beta=-1000 alpha=8 alpha=9 beta=3 beta=9 A alpha=9 B C beta=8 1 5 9 8 3 9 2 ALPHA-BETA is sensitive to the order of evaluation of neighbours and can be further optimised by an order heuristic.,narrative
991,IA_5.pdf_p24,Minimax game search algorithm with alpha-beta pruning,narrative
992,IA_5.pdf_p25,Normal (strategic) form representation Uses a matrix of payoffs to represent all possible strategies for each player.,narrative
993,IA_5.pdf_p25,Works for complete games (all information is known to all players).,narrative
994,IA_5.pdf_p25,One matrix for each pair of players.,narrative
995,IA_5.pdf_p25,One column for each combination of moves.,narrative
996,IA_5.pdf_p25,Player’s possible moves Opponent’s possible moves Player’s payoffs considering the opponents’ moves.,narrative
997,IA_5.pdf_p26,Prisoner’s dilemma A B Deny Confess Deny -1 -1 -3 0 Confess 0 -3 -2 -2 What is the best strategy for A?,narrative
998,IA_5.pdf_p27,"Prisoner’s dilemma A B Deny Confess Deny -1 -1 -3 0 Confess 0 -3 -2 -2 Average payoff for A, with non-cooperation.",narrative
999,IA_5.pdf_p27,If A chooses Deny: (-1+-3)/2 = -2 If A chooses Confess: (0+-2)/2=-1,narrative
1000,IA_5.pdf_p27,If A chooses Deny is (-1+-3)/2 = -2 If A chooses Confess: (0+-2)/2=-1,diagram_extracted
1001,IA_5.pdf_p28,Prisoner’s dilemma A B Deny Confess Deny -1 -1 -3 0 Confess 0 -3 -2 -2 Strategy: The decisions made by a player for all possible states in which he has to make a decision.,narrative
1002,IA_5.pdf_p28,A strategy making the same choice everytime is called a pure strategy.,narrative
1003,IA_5.pdf_p28,"For example, choosing “Deny” all the time is a pure strategy for player A.",narrative
1004,IA_5.pdf_p28,Strategy is The decisions made by a player for all possible states in which he has to make a,diagram_extracted
1005,IA_5.pdf_p29,Dominant strategy A strategy is dominant if it always provides payoffs at least as good as any other strategy.,narrative
1006,IA_5.pdf_p29,A strategy is strictly dominant is the payoffs are always better than any other strategy.,narrative
1007,IA_5.pdf_p29,Is there a dominant strategy for Prisoner’s Dilemma?,narrative
1011,IA_5.pdf_p30,"Yes, “Confess” is a strictly dominant strategy.",narrative
1012,IA_5.pdf_p31,The Monty Hall problem,narrative
1013,IA_5.pdf_p32,Pareto optimality A strategy is Pareto optimal if no change can be made to it in order to improve the players’ payoff without diminishing the other players payoffs.,narrative
1014,IA_5.pdf_p32,"(Deny/Deny) is Pareto optimal, as well as (Deny/Confess) and (Confess/Deny).",narrative
1015,IA_5.pdf_p32,(Confess/Confess) is not optimal as if either player changes decision to improve outcome the outcome for the other player will decrease.,narrative
1016,IA_5.pdf_p32,What about the game below?,narrative
1017,IA_5.pdf_p32,"(Y/Y), (X/Y), (Y/X) are Paretto optimal, (X/X) is not Paretto optimal.",narrative
1018,IA_5.pdf_p32,X Y X 1 1 2 2 Y 2 2 1 3,narrative
1019,IA_5.pdf_p33,"Nash Equilibrium If in a game all players obey the game rules and aim to maximize their payoffs, there is at least one strategy for each player which guarantees maximum payoffs for each player.",narrative
1020,IA_5.pdf_p33,That strategy is called Nash optimum or equilibrium.,narrative
1021,IA_5.pdf_p33,No player has a reason to change an equilibrium as long as anybody follows one.,narrative
1022,IA_5.pdf_p33,A dominant and Paretto optimal strategy is always an equilibrium.,narrative
1023,IA_5.pdf_p33,"However, for most games no dominant strategies exist.",narrative
1024,IA_5.pdf_p33,More about equilibria.,narrative
1025,IA_5.pdf_p34,Hunting game Stag Rabbit Stag 5 5 0 3 Rabbit 3 0 4 4 Are there any dominant strategies?,narrative
1026,IA_5.pdf_p34,Are there any equilibria?,narrative
1028,IA_5.pdf_p35,NO Are there any equilibria?,narrative
1029,IA_5.pdf_p35,"YES: (Stag, Stag) and (Rabbit, Rabbit) Equilibria are dependent on the other players behaving as expected!",narrative
1030,IA_5.pdf_p35,"Are there any equilibria? YES is (Stag, Stag) and (Rabbit, Rabbit)",diagram_extracted
1031,IA_5.pdf_p36,"Chicken game Straight Turn Straight -3 -3 3 -2 Turn -2 3 -3 -3 The two equilibria (Straight, Turn) and (Turn, Straight) are very dependent on the opponent’s strategy, otherwise both players get worse results.",narrative
1032,IA_5.pdf_p37,The money game I have n money and I want to give it to a group of m players.,narrative
1036,IA_5.pdf_p37,Is there any dominant strategy?,narrative
1037,IA_5.pdf_p37,What is the equilibrium?,narrative
1043,IA_5.pdf_p38,NO What is the equilibrium?,narrative
1044,IA_5.pdf_p38,Each player writes n/m.,narrative
1045,IA_5.pdf_p39,The Goalkeeper game Shooter Keeper L eft Right Left -1 1 1 -1 Right 1 -1 -1 1 There is no pure strategy equilibria.,narrative
1046,IA_5.pdf_p39,What is the actual equilibria?,narrative
1047,IA_5.pdf_p40,Computing equilibria as a mixed strategy Shooter Keeper L eft Right Left -1 1 1 -1 Right 1 -1 -1 1 The equilibria (EQS and EQK) must provide equal payoffs for all possible opponent decisions.,narrative
1048,IA_5.pdf_p40,EQS(Left) payoff is -1*EQK(Left)+1*EQK(Right) EQS(Right) payoff is 1*EQK(Left)-1*EQK(Right) Equal payoff means that -1*EQK(Left)+1*EQK(Right)=1*EQK(Left)-1*EQK(Right) So EQK(Left)=EQK(Right)=0.5 since EQK(Right)+EQK(Left)=1.,narrative
1049,IA_5.pdf_p40,Equilibria is reached when both choices are made with the same frequency.,narrative
1050,IA_5.pdf_p41,Mixed strategies Strategies which are not pure are called mixed.,narrative
1051,IA_5.pdf_p41,"For most games, pure strategies are not equilibria.",narrative
1052,IA_5.pdf_p41,"Mixed strategies are usually non-deterministic, making some random decisions (stochastic or probabilistic).",narrative
1053,IA_5.pdf_p41,How can we compute the equilibria for the goalkeeper game?,narrative
1054,IA_5.pdf_p42,Equilibria work only if all players obey the game rules and aim to maximize their payoffs.,narrative
1055,IA_5.pdf_p42,Can you trust the other players?,narrative
1056,IA_5.pdf_p42,How much can you cooperate with them?,narrative
1057,IA_5.pdf_p42,Minimax is an equilibria for all two player zero-sum game.,narrative
1058,IA_5.pdf_p42,A chess program implementing Minimax is easy to defeat.,narrative
1059,IA_5.pdf_p42,Trust in Game Theory,narrative
1060,IA_5.pdf_p43,Parrondo's paradox There is at least one winning strategy which can be built using two losing strategies alternatively.,narrative
1061,IA_5.pdf_p43,Game: add or substract money.,narrative
1062,IA_5.pdf_p43,Game ends if sum at or below zero.,narrative
1063,IA_5.pdf_p43,"S1: If you have an even sum, add 3.",narrative
1064,IA_5.pdf_p43,Otherwise substract 5.,narrative
1065,IA_5.pdf_p43,"S2: If you have an odd sum, substract 1.",narrative
1066,IA_5.pdf_p43,"S1,S2,S1,S2,... is dominant for both S1 and S2.",narrative
1067,IA_5.pdf_p43,Game is add or substract money. Game ends if sum at or below zero.,diagram_extracted
1068,IA_5.pdf_p44,Overall considerations about strategies ● Strategies should have to have a way of estimating the missing information (eg.,narrative
1069,IA_5.pdf_p44,"other player’s decisions) ● Strategies have to be unpredictable: many use either random steps or select randomly between two or more strategies ● You can prove that some strategies are better than others ● If a there is a strategy better than all others then it’s not a game, it’s a puzzle ● An useful benchmark strategy is the equilibria which exists and can be determined for all games Computer “outsmarts” Kasparov and Fisher outthinks computers",narrative
1070,IA_5.pdf_p44,● Strategies have to be unpredictable is many use either random steps or select randomly,diagram_extracted
1071,IA_5.pdf_p45,Monte-Carlo strategies ● Generate all neighbours of the current state and add them to the graph until a set depth has been reached OR until enough final states have been reached ● Run a simulation for all unexplored states in the graph ○ Until n final states are reached OR ○ Up to depth m ● Compute a score for each state after each simulation (for example (positive-negative)/n)) ● Update the scores of all nodes above the current one using the newly computed score ● Return to the first step ● Select the neighbour of the starting state with the best score More,narrative
1072,IA_7.pdf_p1,Reinforcement learning (I) AI 2025/2026,narrative
1073,IA_7.pdf_p2,"Content Introduction Markov decision process Value iteration Policy iteration FII,UAIC Lecture7 AI2025/2026 2/41",narrative
1074,IA_7.pdf_p3,"Reinforcement learning The agent must learn a behavior, without having an instructor ▶ The agent has a task to perform ▶ Performs a series of actions ▶ Receives feedback from the environment: how well he acted to accomplish the task.",narrative
1075,IA_7.pdf_p3,"The agent receives a positive reward if the task is performed well, otherwise a negative reward.",narrative
1076,IA_7.pdf_p3,This learning method is called reinforcement learning.,narrative
1077,IA_7.pdf_p3,"FII,UAIC Lecture7 AI2025/2026 3/41",narrative
1078,IA_7.pdf_p3,Receives feedback from the environment is how well he acted to,diagram_extracted
1079,IA_7.pdf_p4,"The interaction model ▶ The agent performs actions ▶ The environment presents the agent situations called states and gives rewards Trajectory / episode (s ,a ,r ,s ,a ,r ,...) 0 0 1 1 1 2 FII,UAIC Lecture7 AI2025/2026 4/41",narrative
1080,IA_7.pdf_p5,"Reinforcement learning ▶ The goal: to make the agent to act in order to maximize his rewards ▶ The agent must identify the sequence of actions leading to the completion of the task ▶ Data acquisition and supervised learning on data Training data: (S,A,R) State, Action, Reward FII,UAIC Lecture7 AI2025/2026 5/41",narrative
1081,IA_7.pdf_p5,The goal is to make the agent to act in order to maximize his rewards,diagram_extracted
1082,IA_7.pdf_p5,"Training data is (S,A,R) State, Action, Reward",diagram_extracted
1083,IA_7.pdf_p6,"Applications: TD-Gammon A neural network trained with TD-learning ▶ input: configuration ▶ output: an estimation of the value for that configuration Learn from simulations (play games against himself) FII,UAIC Lecture7 AI2025/2026 6/41",narrative
1084,IA_7.pdf_p6,Applications is TD-Gammon,diagram_extracted
1085,IA_7.pdf_p6,input is configuration,diagram_extracted
1086,IA_7.pdf_p6,output is an estimation of the value for that configuration,diagram_extracted
1087,IA_7.pdf_p7,"Examples: robotics AIBO: learn to walk ’04 (using Minitaur quadrupedal robot: learn Policy gradient) to walk (using actor-critic deep RL) https://sites.google.com/view/ https://www.cs.utexas.edu/users/ minitaur-locomotion/ AustinVilla/?p=research/learned_walk FII,UAIC Lecture7 AI2025/2026 7/41",narrative
1088,IA_7.pdf_p7,Examples is robotics,diagram_extracted
1089,IA_7.pdf_p7,AIBO is learn to walk ’04 (using Minitaur quadrupedal robot: learn,diagram_extracted
1090,IA_7.pdf_p7,https is //sites.google.com/view/,diagram_extracted
1091,IA_7.pdf_p7,https is //www.cs.utexas.edu/users/,diagram_extracted
1092,IA_7.pdf_p8,"Deep Reinforcement Learning AlphaGo (Google DeepMind, ’15): the program learned to play Atari 2600 games by directly watching only the display and the score.",narrative
1093,IA_7.pdf_p8,"▶ 2016: won against a 9 dan professional GO player (prize of 1 000 000$) ▶ 2017: won against Ke Jie, the best GO player in the world AlphaGo Zero learned to play without input from human games (just based on the rules of the game) and beat AlphaGo with 100-0 (Oct. ’17) AlphaZero beat AlphaGo Zero with 60-40, and after just 8 hours of training outperformed all existing GO and CHESS programs (Dec. ’17) FII,UAIC Lecture7 AI2025/2026 8/41",narrative
1094,IA_7.pdf_p8,"AlphaGo (Google DeepMind, ’15) is the program learned to play Atari 2600",diagram_extracted
1095,IA_7.pdf_p8,2016 is won against a 9 dan professional GO player (prize of 1 000,diagram_extracted
1096,IA_7.pdf_p8,"2017 is won against Ke Jie, the best GO player in the world",diagram_extracted
1097,IA_7.pdf_p9,"Content Introduction Markov decision process Value iteration Policy iteration FII,UAIC Lecture7 AI2025/2026 9/41",narrative
1098,IA_7.pdf_p10,"Sequential decisions ▶ Deterministic environment ▶ (up, up, right, right, right) ▶ Stochastic environment ▶ Transitions model P(s′|s,a): probability of reaching state s′ if action a is done in state s ▶ The action obtains the intended effect with probability 0.8 ▶ The agent receives a reward: -0.04 for nonterminal states; +/-1 for terminal states FII,UAIC Lecture7 AI2025/2026 10/41",narrative
1099,IA_7.pdf_p10,The agent receives a reward is -0.04 for nonterminal states; +/-1 for,diagram_extracted
1100,IA_7.pdf_p11,"Deterministic vs. stochastic Stochastic: for the sequence (up, up, right, right, right), the probability of obtaining the final state is 0.85 = 0.33 FII,UAIC Lecture7 AI2025/2026 11/41",narrative
1101,IA_7.pdf_p11,"Stochastic is for the sequence (up, up, right, right, right), the probability of",diagram_extracted
1102,IA_7.pdf_p12,"Markov assumption ▶ The current state s depends on a finite history of previous states t ▶ First order Markov process: the current state s depends only on the t previous state s t−1 P(s |s ,...,s ) = P(s |s ) t t−1 0 t t−1 FII,UAIC Lecture7 AI2025/2026 12/41",narrative
1103,IA_7.pdf_p12,First order Markov process is the current state s depends only on the,diagram_extracted
1104,IA_7.pdf_p13,"Markov Decision Process (MDP) Markov Decision Process: a sequential decision problem for a stochastic environment with a Markov transition model and additive rewards ▶ States s ∈ S (initial state s ), actions a ∈ A 0 ▶ Transitions model P(s′|s,a) ▶ Reward function R(s) What does a solution look like?",narrative
1105,IA_7.pdf_p13,Must specify what the agent should do in each state (policy π).,narrative
1106,IA_7.pdf_p13,MDPs are non-deterministic search problems.,narrative
1107,IA_7.pdf_p13,▶ Search algorithms.,narrative
1108,IA_7.pdf_p13,Expectimax alg: when the opponent does not play optimally.,narrative
1109,IA_7.pdf_p13,"FII,UAIC Lecture7 AI2025/2026 13/41",narrative
1110,IA_7.pdf_p13,Markov Decision Process is a sequential decision problem for a stochastic,diagram_extracted
1111,IA_7.pdf_p13,Search algorithms. Expectimax alg is when the opponent does not play,diagram_extracted
1112,IA_7.pdf_p14,Example ▶ A robot car wants to travel far and quickly.,narrative
1113,IA_7.pdf_p14,"▶ States: Cool, Warm, Overheated; Actions: Slow, Fast; Going faster gets double reward.",narrative
1114,IA_7.pdf_p14,"FII,UAIC Lecture7 AI2025/2026 14/41",narrative
1115,IA_7.pdf_p14,"States is Cool, Warm, Overheated; Actions: Slow, Fast; Going faster",diagram_extracted
1116,IA_7.pdf_p15,"Example: search tree FII,UAIC Lecture7 AI2025/2026 15/41",narrative
1117,IA_7.pdf_p15,Example is search tree,diagram_extracted
1118,IA_7.pdf_p16,"MDP search trees ▶ Each state projects a search tree s state, (s,a,s′) transition, P(s′|s,a), R(s,a,s′) ▶ In search problems, the goal is to identify an optimal sequence.",narrative
1119,IA_7.pdf_p16,"In MDP, the goal is to identify an optimal policy π∗ (strategy) ▶ π :S →A π(s) is the action recommended in state s FII,UAIC Lecture7 AI2025/2026 16/41",narrative
1120,IA_7.pdf_p17,MDP ▶ Utility: the sum of rewards for a sequence of states.,narrative
1121,IA_7.pdf_p17,"The reward is the immediate, short-term gain; utility is the total, long-term gain.",narrative
1122,IA_7.pdf_p17,▶ Quality of a policy: expected utility of possible sequences of states.,narrative
1123,IA_7.pdf_p17,Stochastic environment: we can have a different sequence of states when executing the same policy from the initial state.,narrative
1124,IA_7.pdf_p17,Optimal policy π∗ maximizes the expected utility.,narrative
1125,IA_7.pdf_p17,"FII,UAIC Lecture7 AI2025/2026 17/41",narrative
1126,IA_7.pdf_p17,Utility is the sum of rewards for a sequence of states.,diagram_extracted
1127,IA_7.pdf_p17,Quality of a policy is expected utility of possible sequences of states.,diagram_extracted
1128,IA_7.pdf_p17,Stochastic environment is we can have a different sequence of states,diagram_extracted
1129,IA_7.pdf_p18,"MDP Example: the optimal policy and the state values FII,UAIC Lecture7 AI2025/2026 18/41",narrative
1130,IA_7.pdf_p18,Example is the optimal policy and the state values,diagram_extracted
1131,IA_7.pdf_p19,"Utilities Finite horizon ▶ U ([s ,s ,...,s ]) = U ([s ,s ,...,s ]),∀k > 0 h 0 1 N+k h 0 1 N After a fixed time N, nothing matters.",narrative
1132,IA_7.pdf_p19,"▶ The optimal policy is not stationary: the optimal action for a given state may change over time Example: ▶ N = 3 → have to risk (up) ▶ N = 100 → can choose the safer solution (left) FII,UAIC Lecture7 AI2025/2026 19/41",narrative
1133,IA_7.pdf_p19,The optimal policy is not stationary is the optimal action for a given,diagram_extracted
1134,IA_7.pdf_p20,Utilities Infinite horizon ▶ There is no fixed deadline ▶ The optimal policy is stationary ▶ a.,narrative
1135,IA_7.pdf_p20,"Additive rewards U ([s ,s ,s ,...]) = R(s )+R(s )+R(s )+... h 0 1 2 0 1 2 ▶ b.",narrative
1136,IA_7.pdf_p20,"Discounted rewards U ([s ,s ,s ,...]) = R(s )+γR(s )+γ2R(s )+... h 0 1 2 0 1 2 γ ∈ [0,1] discount factor indicates that future rewards matter less than immediate ones FII,UAIC Lecture7 AI2025/2026 20/41",narrative
1137,IA_7.pdf_p21,"Discounted rewards U ([s ,s ,s ,...]) = R(s )+γR(s )+γ2R(s )+... h 0 1 2 0 1 2 ▶ Prefer current rewards, not later ones ▶ Values of rewards decay exponentially Example 1: γ = 0.5 R(s ) = 1,R(s ) = 2,... 0 1 U([1,2,3]) = 1∗1+0.5∗2+0.25∗3 U([1,2,3]) < U([3,2,1]) Example 2: Quiz FII,UAIC Lecture7 AI2025/2026 21/41",narrative
1138,IA_7.pdf_p21,Example 1 is γ = 0.5,diagram_extracted
1139,IA_7.pdf_p21,Example 2 is Quiz,diagram_extracted
1140,IA_7.pdf_p22,Infinite horizon - evaluation We must ensure that the utility of a possibly infinite sequence is finite.,narrative
1141,IA_7.pdf_p22,▶ 1st Approach.,narrative
1142,IA_7.pdf_p22,"If the rewards are bounded and γ < 1 then: ∞ ∞ (cid:88) (cid:88) U ([s ,s ,s ,...]) = γtR(s ) ≤ γtR = R /(1−γ) h 0 1 2 t max max t=0 t=0 ▶ 2nd Approach.",narrative
1143,IA_7.pdf_p22,"If the environment contains terminal states and it’s guaranteed that the agent will reach one of them (we have a proper policy), we can use γ = 1 FII,UAIC Lecture7 AI2025/2026 22/41",narrative
1145,IA_7.pdf_p23,"Expected utility ▶ Each policy generates multiple sequences of states, due to the uncertainty of transitions P(s′|s,a) ▶ Let S be a random variable: the state the agent reaches at time t t executing policy π; S = s. 0 Expected utility obtained by executing policy π starting in state s: (cid:34) ∞ (cid:35) (cid:88) Uπ(s) = E γtR(S ) t t=0 ( = the expected value of the sum of all discounted rewards obtained for all possible sequences of states) FII,UAIC Lecture7 AI2025/2026 23/41",narrative
1146,IA_7.pdf_p23,Let S be a random variable is the state the agent reaches at time t,diagram_extracted
1147,IA_7.pdf_p23,(cid is 34) (cid:35),diagram_extracted
1149,IA_7.pdf_p24,Evaluation of a policy ▶ Optimal policy π∗ = argmax Uπ(s) s π ▶ Uπ∗(s) the true utility of a state: the expected value of the sum of the discounted rewards if the agent executes an optimal policy Example: Consider γ = 1 and R(s) = −0.04.,narrative
1150,IA_7.pdf_p24,"Near the final state, the utilities are higher because fewer steps with negative reward are required to reach that state.",narrative
1151,IA_7.pdf_p24,"FII,UAIC Lecture7 AI2025/2026 24/41",narrative
1152,IA_7.pdf_p24,the true utility of a state is the expected value of the sum of,diagram_extracted
1153,IA_7.pdf_p24,Example is Consider γ = 1 and R(s) = −0.04.,diagram_extracted
1154,IA_7.pdf_p25,"Maximum Expected Utility Maximum Expected Utility: choose the action that maximizes the expected utility of the subsequent state (cid:88) π∗(s) = argmax P(s′|s,a)U(s′) a∈A(s) s′ FII,UAIC Lecture7 AI2025/2026 25/41",narrative
1155,IA_7.pdf_p25,Maximum Expected Utility is choose the action that maximizes the,diagram_extracted
1157,IA_7.pdf_p26,"Bellman equation (cid:88) U(s) = R(s)+γmax P(s′|s,a)U(s′) a∈A(s) s′ Bellman equation (1957): the utility of a state is the immediate reward for that state, R(s), plus the maximum expected utility of the next state.",narrative
1158,IA_7.pdf_p26,"FII,UAIC Lecture7 AI2025/2026 26/41",narrative
1160,IA_7.pdf_p26,Bellman equation (1957) is the utility of a state is the immediate reward for,diagram_extracted
1161,IA_7.pdf_p27,"Example The utility of state (1,1): U(1,1) = −0.04+γmax[0.8U(1,2)+0.1U(2,1)+0.1U(1,1), (Up) 0.9U(1,1)+0.1U(1,2), (Left) 0.9U(1,1)+0.1U(2,1), (Down) 0.8U(2,1)+0.1U(1,2)+0.1U(1,1)] (Right) The best action: Up.",narrative
1162,IA_7.pdf_p27,"FII,UAIC Lecture7 AI2025/2026 27/41",narrative
1163,IA_7.pdf_p27,The best action is Up.,diagram_extracted
1164,IA_7.pdf_p28,"Solving a Markov decision process ▶ n possible states ▶ n Bellman equations, one for each state ▶ n equations with n unknowns: U(s) ▶ Cannot be solved as a system of linear equations because of the max function FII,UAIC Lecture7 AI2025/2026 28/41",narrative
1165,IA_7.pdf_p28,n equations with n unknowns is U(s),diagram_extracted
1166,IA_7.pdf_p29,"Content Introduction Markov decision process Value iteration Policy iteration FII,UAIC Lecture7 AI2025/2026 29/41",narrative
1167,IA_7.pdf_p30,Value iteration Computes the utility of each state and identifies the optimal action in each state.,narrative
1168,IA_7.pdf_p30,"The algorithm for computing the optimal policy: ▶ Initializes utilities with arbitrary values ▶ Updates the utility of each state from the utilities of its neighbors (cid:88) U (s) = R(s)+γmax P(s′|s,a)U (s′) i+1 a i s′ ▶ Repeat for each s simultaneously, until an equilibrium is reached FII,UAIC Lecture7 AI2025/2026 30/41",narrative
1170,IA_7.pdf_p31,"Value iteration: pseudocode FII,UAIC Lecture7 AI2025/2026 31/41",narrative
1171,IA_7.pdf_p31,Value iteration is pseudocode,diagram_extracted
1172,IA_7.pdf_p32,Value iteration From Sutton&Barto.,narrative
1173,IA_7.pdf_p32,"Reinforcement Learning: an introduction Utility function U ↔ V value function FII,UAIC Lecture7 AI2025/2026 32/41",narrative
1174,IA_7.pdf_p32,From Sutton&Barto. Reinforcement Learning is an introduction,diagram_extracted
1175,IA_7.pdf_p33,"Value iteration ▶ Example: Racing ▶ Demo: https://courses.grainger.illinois.edu/cs440/fa2018/ lectures/mdp-value-demo.pdf FII,UAIC Lecture7 AI2025/2026 33/41",narrative
1176,IA_7.pdf_p33,Example is Racing,diagram_extracted
1177,IA_7.pdf_p33,Demo is https://courses.grainger.illinois.edu/cs440/fa2018/,diagram_extracted
1178,IA_7.pdf_p34,"Problems with Value iteration ▶ Slow: O(S2A) per iteration ▶ The “max” at each state rarely changes ▶ The policy often converges before the values FII,UAIC Lecture7 AI2025/2026 34/41",narrative
1179,IA_7.pdf_p34,Slow is per iteration,diagram_extracted
1180,IA_7.pdf_p35,"Content Introduction Markov decision process Value iteration Policy iteration FII,UAIC Lecture7 AI2025/2026 35/41",narrative
1181,IA_7.pdf_p36,"Policy iteration ▶ If we fix the policy, we have only one action per state ▶ The algorithm alternates the following steps: ▶ 1.",narrative
1182,IA_7.pdf_p36,"Policy evaluation: given a policy π, compute the utilities of the i states based on the policy π i : U i =Uπi ▶ 2.",narrative
1183,IA_7.pdf_p36,"Policy improvement: compute a new policy π , based on the i+1 utilities U i Repeat the steps until the policy converges FII,UAIC Lecture7 AI2025/2026 36/41",narrative
1184,IA_7.pdf_p36,"1. Policy evaluation is given a policy π, compute the utilities of the",diagram_extracted
1185,IA_7.pdf_p36,states based on the policy π is U =Uπi,diagram_extracted
1186,IA_7.pdf_p36,"2. Policy improvement is compute a new policy π , based on the",diagram_extracted
1187,IA_7.pdf_p37,"Policy evaluation The action for each state is fixed by the policy; at iteration i, the policy π i specifies the action π (s) in state s. i The simplified Bellman equations (cid:88) U (s) = R(s)+γ P(s′|s,π (s))U (s′) i i i s′ ▶ A system of n linear equations with n unknowns ▶ It can be solved exactly in O(n3) or approximately ▶ Apply Value iteration (cid:88) U (s) = R(s)+γ P(s′|s,π (s))U (s′) i+1 i i s′ FII,UAIC Lecture7 AI2025/2026 37/41",narrative
1190,IA_7.pdf_p38,"Policy evaluation Example: π (1,1) = Up, π (1,2) = Up i i Simplified Bellman equations: U (1,1) = −0.04+0.8U (1,2)+0.1U (1,1)+0.1U (2,1) i i i i U (1,2) = −0.04+0.8U (1,3)+0.2U (1,2) i i i FII,UAIC Lecture7 AI2025/2026 38/41",narrative
1191,IA_7.pdf_p38,"Example is π (1,1) = Up, π (1,2) = Up",diagram_extracted
1192,IA_7.pdf_p39,"Policy improvement ▶ The values U(s) are known ▶ Computes for each s, the optimal action (cid:88) a∗(s) = max P(s′|s,a)U(s′) i a s′ ▶ If a∗(s) ̸= π (s), update the policy: π (s) ← a∗(s) i i i+1 i Only the ”promising” parts of the search space can be updated.",narrative
1193,IA_7.pdf_p39,"FII,UAIC Lecture7 AI2025/2026 39/41",narrative
1195,IA_7.pdf_p39,"If ̸= π (s), update the policy is π (s) ←",diagram_extracted
1196,IA_7.pdf_p40,"Policy iteration: pseudocode Demo: https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html FII,UAIC Lecture7 AI2025/2026 40/41",narrative
1197,IA_7.pdf_p40,Policy iteration is pseudocode,diagram_extracted
1198,IA_7.pdf_p40,Demo is https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html,diagram_extracted
1199,IA_7.pdf_p41,Bibliography ▶ Russel&Norvig.,narrative
1200,IA_7.pdf_p41,Artificial Intelligence: A modern Approach.,narrative
1201,IA_7.pdf_p41,Making Complex Decisions ▶ Sutton&Barto.,narrative
1202,IA_7.pdf_p41,An introduction Ch 3.,narrative
1203,IA_7.pdf_p41,"Finite Markov Decision Processes, Ch 4.",narrative
1204,IA_7.pdf_p41,"Dynamic Programming http://incompleteideas.net/book/RLbook2020.pdf FII,UAIC Lecture7 AI2025/2026 41/41",narrative
1205,IA_7.pdf_p41,Russel&Norvig. Artificial Intelligence is A modern Approach. Ch. 17.,diagram_extracted
1207,IA_6.pdf_p1,Neural networks AI 2025/2026,narrative
1208,IA_6.pdf_p2,"Content Introduction The perceptron Perceptron training rule Gradient descent and Delta rule Multi-layer neural networks Backpropagation FII,UAIC Lecture6 AI2025/2026 2/57",narrative
1209,IA_6.pdf_p3,History ▶ McCulloch&Pitts ’43 propose the first mathematical model of an artificial neuron.,narrative
1210,IA_6.pdf_p3,"It cannot learn, the parameters are set analytically.",narrative
1211,IA_6.pdf_p3,"▶ Minsky ’51 - the first electronic circuit built as an artificial neural network (subcircuits that work like interconnected neurons) ▶ Rosenblatt ’58 develops the perceptron, the first functional neural network ▶ Hinton ’06 designs Deep Neural Network FII,UAIC Lecture6 AI2025/2026 3/57",narrative
1212,IA_6.pdf_p4,Artificial neural networks ▶ Inspired by the way the brain is structured and the way it works Try to reproduce the intelligence (the behavior of a biological neuron).,narrative
1213,IA_6.pdf_p4,"FII,UAIC Lecture6 AI2025/2026 4/57",narrative
1214,IA_6.pdf_p5,Artificial neural networks A neuron connects to other neurons via dendrites.,narrative
1215,IA_6.pdf_p5,Neurons communicate with each other through synapses (excitatory or inhibitory).,narrative
1216,IA_6.pdf_p5,The neuron can activate and produce an electrical signal that is transmitted further along the axon.,narrative
1217,IA_6.pdf_p5,The interconnection of neurons provides computing power.,narrative
1218,IA_6.pdf_p5,"FII,UAIC Lecture6 AI2025/2026 5/57",narrative
1219,IA_6.pdf_p6,"Artificial neural networks Unit (artificial neuron): a simplified computational model of the neuron ▶ input signals ▶ weights attached to connections ▶ activation threshold ▶ output Analogy Biological NN Artificial NN the cell body neuron dendrites inputs axon output synapse weight FII,UAIC Lecture6 AI2025/2026 6/57",narrative
1220,IA_6.pdf_p6,Unit (artificial neuron) is a simplified computational model of the neuron,diagram_extracted
1221,IA_6.pdf_p7,Artificial neural networks ▶ An ensemble of interconnected functional units (neurons).,narrative
1222,IA_6.pdf_p7,"▶ Training involves determining the parameters of the network, given training data ▶ Are adaptive ”black box” systems that extract a pattern through a learning process FII,UAIC Lecture6 AI2025/2026 7/57",narrative
1223,IA_6.pdf_p8,"Learning methods ▶ Supervised (classification, regression) ▶ Labeled training examples ▶ Goal: estimate parameters that minimize the error (the difference btw the correct answers and those produced by the network) ▶ Unsupervised (clustering, association, dimensionality reduction) ▶ Unlabeled training data ▶ Goal: obtaining information FII,UAIC Lecture6 AI2025/2026 8/57",narrative
1224,IA_6.pdf_p8,Goal is estimate parameters that minimize the error (the difference btw,diagram_extracted
1225,IA_6.pdf_p8,Goal is obtaining information,diagram_extracted
1226,IA_6.pdf_p9,"Applications: Classification Given a set of instances (attributes, labels), identify the class of a new instance.",narrative
1227,IA_6.pdf_p9,"(supervised learning) Example: identify the iris flower species ▶ attributes: sepal/petal length and width ▶ classes: Iris versicolor, Iris setosa, Iris virginica FII,UAIC Lecture6 AI2025/2026 9/57",narrative
1228,IA_6.pdf_p9,Applications is Classification,diagram_extracted
1229,IA_6.pdf_p9,Example is identify the iris flower species,diagram_extracted
1230,IA_6.pdf_p9,attributes is sepal/petal length and width,diagram_extracted
1231,IA_6.pdf_p9,"classes is Iris versicolor, Iris setosa, Iris virginica",diagram_extracted
1232,IA_6.pdf_p10,"Applications: Regression Find the relationship between two or more variables, given a sequence of values (approximation of a function) The difference btw classification and regression: the type of output (discrete vs. continuous) FII,UAIC Lecture6 AI2025/2026 10/57",narrative
1233,IA_6.pdf_p10,Applications is Regression,diagram_extracted
1234,IA_6.pdf_p10,The difference btw classification and regression is the type of output,diagram_extracted
1235,IA_6.pdf_p11,"Content Introduction The perceptron Perceptron training rule Gradient descent and Delta rule Multi-layer neural networks Backpropagation FII,UAIC Lecture6 AI2025/2026 11/57",narrative
1236,IA_6.pdf_p12,"Perceptron (Rosenblatl, 1958) Input: an array of real values x i Computes a linear combination of those values.",narrative
1237,IA_6.pdf_p12,x w 1 1 x w (cid:80) 2 2 .,narrative
1238,IA_6.pdf_p12,"function x w n n inputs weights w ,...w weights (real consts.)",narrative
1239,IA_6.pdf_p12,attached to connections; w the 1 n i contribution of input x to the result.,narrative
1240,IA_6.pdf_p12,"i FII,UAIC Lecture6 AI2025/2026 12/57",narrative
1241,IA_6.pdf_p12,Input is an array of real values x,diagram_extracted
1243,IA_6.pdf_p13,Perceptron Input: an array of real values x i Computes a linear combination of those values.,narrative
1244,IA_6.pdf_p13,"Returns 1, if the result is larger than a threshold (−w ), -1 otherwise.",narrative
1245,IA_6.pdf_p13,"0 (cid:40) 1 if w +w x +...w x > 0 0 1 1 n n o(x ,...,x ) = (1) 1 n −1 otherwise w bias 0 Learning a perceptron: identify weights w ,...,w .",narrative
1246,IA_6.pdf_p13,"0 n FII,UAIC Lecture6 AI2025/2026 13/57",narrative
1248,IA_6.pdf_p13,(cid is 40),diagram_extracted
1249,IA_6.pdf_p13,"Learning a perceptron is identify weights w ,...,w .",diagram_extracted
1250,IA_6.pdf_p14,Perceptron Simplified notation: a constant input x = 1.,narrative
1251,IA_6.pdf_p14,"0 (cid:80)n w x > 0, or →− w · →− x > 0. i=0 i i →− →− →− o(x ) = f(w · x ) Step activation function (cid:40) 1 if y ≥ 0 f(y) = 0 otherwise Sign activation function sign (cid:40) 1 if y ≥ 0 f(y) = −1 otherwise Perceptron: an artificial neuron using the unit step/sign function as the activation function.",narrative
1252,IA_6.pdf_p14,"FII,UAIC Lecture6 AI2025/2026 14/57",narrative
1253,IA_6.pdf_p14,Simplified notation is a constant input x = 1.,diagram_extracted
1254,IA_6.pdf_p14,(cid is 80)n →− →−,diagram_extracted
1257,IA_6.pdf_p14,Perceptron is an artificial neuron using the unit step/sign function as the,diagram_extracted
1258,IA_6.pdf_p15,The representational power of perceptrons The goal of the perceptron: classify the inputs in 2 classes.,narrative
1259,IA_6.pdf_p15,"Perceptron: a hyperplane that divides the space of input vectors (n-dimensional) in two →− →− regions: the region for which w · x > 0, and the other region.",narrative
1260,IA_6.pdf_p15,"→− →− The equation of the hyperplane: w · x = 0 a) Linearly separable FII,UAIC Lecture6 AI2025/2026 15/57",narrative
1261,IA_6.pdf_p15,The goal of the perceptron is classify the inputs in 2 classes. Perceptron: a,diagram_extracted
1262,IA_6.pdf_p15,"regions is the region for which w · x > 0, and the other region.",diagram_extracted
1263,IA_6.pdf_p15,The equation of the hyperplane is w · x = 0,diagram_extracted
1264,IA_6.pdf_p16,The representational power of perceptrons A perceptron can be used to represent boolean functions.,narrative
1265,IA_6.pdf_p16,"To represent the AND function, we set the weights, for ex.",narrative
1266,IA_6.pdf_p16,"w = −1.5,w = w = 1.",narrative
1267,IA_6.pdf_p16,"0 1 2 FII,UAIC Lecture6 AI2025/2026 16/57",narrative
1268,IA_6.pdf_p17,The representational power of perceptrons The XOR function (1 ⇔ x ̸= x ) cannot be represented by a single 1 2 perceptron.,narrative
1269,IA_6.pdf_p17,Any boolean function can be represented by a network of interconnected units.,narrative
1270,IA_6.pdf_p17,"FII,UAIC Lecture6 AI2025/2026 17/57",narrative
1271,IA_6.pdf_p18,"Content Introduction The perceptron Perceptron training rule Gradient descent and Delta rule Multi-layer neural networks Backpropagation FII,UAIC Lecture6 AI2025/2026 18/57",narrative
1272,IA_6.pdf_p19,Perceptron training rule ▶ Learning weights: identify the vector of weights s.t.,narrative
1273,IA_6.pdf_p19,the perceptron will return the correct output for each training example.,narrative
1274,IA_6.pdf_p19,▶ Generate random weights.,narrative
1275,IA_6.pdf_p19,"Compute the output for training inputs, modify the weights when it misclassifies an example.",narrative
1276,IA_6.pdf_p19,Repeat this process until the perceptron correctly classifies the training examples.,narrative
1277,IA_6.pdf_p19,"FII,UAIC Lecture6 AI2025/2026 19/57",narrative
1278,IA_6.pdf_p19,Learning weights is identify the vector of weights s.t. the perceptron,diagram_extracted
1279,IA_6.pdf_p20,"Perceptron training rule The weights are modified according to the perceptron training rule: w ← w +∆w i i i where ∆w = η(t −o)x i i t the target output for the training example, o the output generated by the perceptron, η the learning rate (pos.",narrative
1280,IA_6.pdf_p20,"FII,UAIC Lecture6 AI2025/2026 20/57",narrative
1281,IA_6.pdf_p21,"Intuition for Perceptron training rule ▶ If the training example is correctly classified t−o = 0;∆w = 0 → no i weights are updated ▶ If the perceptron outputs -1 when the target output is +1 and η = 0.1,x = 0.8, then ∆w = 0.1(1−(−1))0.8 = 0.16 i i ▶ If the perceptron outputs +1 when the target output is -1, then the weight will decrease FII,UAIC Lecture6 AI2025/2026 21/57",narrative
1282,IA_6.pdf_p22,"Convergence When the training examples are linearly separable and η small enough, the perceptron training rule converges (considering a finite no.",narrative
1283,IA_6.pdf_p22,of applications of the perceptron training rule) to a vector of weights that classifies all training examples.,narrative
1284,IA_6.pdf_p22,"FII,UAIC Lecture6 AI2025/2026 22/57",narrative
1285,IA_6.pdf_p23,"Content Introduction The perceptron Perceptron training rule Gradient descent and Delta rule Multi-layer neural networks Backpropagation FII,UAIC Lecture6 AI2025/2026 23/57",narrative
1286,IA_6.pdf_p24,Delta rule ▶ The perceptron training rule may fail if the examples are not linearly separable.,narrative
1287,IA_6.pdf_p24,▶ Delta rule: use Gradient descent to search in the space of weight vectors.,narrative
1288,IA_6.pdf_p24,→− →− →− Consider a linear unit for which the output is o(x ) = w · x .,narrative
1289,IA_6.pdf_p24,"The training error for a vector of weights w: E( →− w) = 1 (cid:88) (t −o )2 d d 2 d∈D where D the training data set, t the target output for example d, o d d the output of the linear unit for d. FII,UAIC Lecture6 AI2025/2026 24/57",narrative
1290,IA_6.pdf_p24,Delta rule is use Gradient descent to search in the space of weight,diagram_extracted
1291,IA_6.pdf_p24,→− 1 (cid is 88),diagram_extracted
1292,IA_6.pdf_p25,"The hypothesis space visualization The error surface has a parabolic shape, with a global minimum.",narrative
1293,IA_6.pdf_p25,Gradient descent: iteratively modifies the vector of weights.,narrative
1294,IA_6.pdf_p25,"At each step, the vector is modified in the direction that produces the steepest descent.",narrative
1295,IA_6.pdf_p25,The process continues until the global minimum error is reached.,narrative
1296,IA_6.pdf_p25,"FII,UAIC Lecture6 AI2025/2026 25/57",narrative
1297,IA_6.pdf_p25,"Gradient descent is iteratively modifies the vector of weights. At each step,",diagram_extracted
1298,IA_6.pdf_p26,"Gradient descent The gradient specifies the direction that produces the steepest ascent in E. →− δE δE δE ∇E(w) = [ , ,..., ] δw δw δw 0 1 n →− →− →− The training rule for Gradient descent: w ← w +∆w, where →− →− ∆w = −η∇E(w), η the learning rate (pos.",narrative
1299,IA_6.pdf_p26,"δE w = w +∆w , ∆w = −η i i i i δw i FII,UAIC Lecture6 AI2025/2026 26/57",narrative
1300,IA_6.pdf_p26,"The training rule for Gradient descent is w ← w +∆w, where",diagram_extracted
1301,IA_6.pdf_p27,Gradient descent δE δ 1 (cid:88) = (t −o )2 d d δw δw 2 i i d∈D 1 (cid:88) δ = (t −o )2 d d 2 δw i d∈D 1 (cid:88) δ = 2(t −o ) (t −o ) d d d d 2 δw i d∈D (cid:88) δ →− →− = (t −o ) (t − w ·x ) d d d d δw i d∈D δE (cid:88) = (t −o )(−x ) d d id δw i d∈D x the component x of the training example d. id i (cid:80) Updating the weight with ∆w = η (t −o )x .,narrative
1302,IA_6.pdf_p27,"i d∈D d d id FII,UAIC Lecture6 AI2025/2026 27/57",narrative
1303,IA_6.pdf_p27,δE δ 1 (cid is 88),diagram_extracted
1306,IA_6.pdf_p27,(cid is 88) δ,diagram_extracted
1309,IA_6.pdf_p28,"Gradient descent FII,UAIC Lecture6 AI2025/2026 28/57",narrative
1310,IA_6.pdf_p29,"Stochastic gradient descent Problems with the Gradient descent algorithm: ▶ slow convergence ▶ the existence of several local minima Stochastic gradient descent: update the weights incrementally, by computing the error for each individual example ∆w = η(t −o)x i i where t the target value, o the real output, x the ith input for the i training example.",narrative
1311,IA_6.pdf_p29,Equation T4.1 is replaced with w ← w +η(t −o)x .,narrative
1312,IA_6.pdf_p29,i i i The training rule ∆w = η(t −o)x is also called the delta rule/LMS i i (least-mean-square) rule/Adaline rule.,narrative
1313,IA_6.pdf_p29,"FII,UAIC Lecture6 AI2025/2026 29/57",narrative
1314,IA_6.pdf_p29,"Stochastic gradient descent is update the weights incrementally, by",diagram_extracted
1315,IA_6.pdf_p30,"Content Introduction The perceptron Perceptron training rule Gradient descent and Delta rule Multi-layer neural networks Backpropagation FII,UAIC Lecture6 AI2025/2026 30/57",narrative
1316,IA_6.pdf_p31,"Multi-layer neural networks A feed-forward neural network has ▶ an input layer ▶ one or more hidden layers ▶ an output layer ▶ Input signals are propagated forward through the network layers ▶ The calculations are performed in the neurons from hidden and output layer FII,UAIC Lecture6 AI2025/2026 31/57",narrative
1317,IA_6.pdf_p32,Multi-layer neural networks Can express nonlinear decision surfaces.,narrative
1318,IA_6.pdf_p32,Example: Neural network trained to recognize btw 10 vowels (”h d”).,narrative
1319,IA_6.pdf_p32,"The voice signal is represented by two numerical parameters, obtained by the spectral analysis of the sound.",narrative
1320,IA_6.pdf_p32,The points in the figure are testing data.,narrative
1321,IA_6.pdf_p32,"FII,UAIC Lecture6 AI2025/2026 32/57",narrative
1322,IA_6.pdf_p32,Example is Neural network trained to recognize btw 10 vowels (”h d”).,diagram_extracted
1323,IA_6.pdf_p33,"Sigmoid unit →− →− 1 o = σ(w · x ), where σ(y) = 1+e−y dσ(y) σ the sigmoid fct; derivative = σ(y)·(1−σ(y)) dy FII,UAIC Lecture6 AI2025/2026 33/57",narrative
1324,IA_6.pdf_p34,"Nonlinear activation functions ▶ Sigmoid function (logistic) f(x) = 1 , f′(x) = f(x)(1−f(x)) 1+e−x ▶ Bipolar sigmoid function (hyperbolic tangent) f(x) = 1−e−2x , f′(x) = 1−f(x)2 1+e−2x ▶ ReLU (Rectified Linear Unit) function (cid:40) (cid:40) 0 if x < 0 0 if x < 0 f(x) = , f′(x) = x if x ≥ 0 1 if x ≥ 0 FII,UAIC Lecture6 AI2025/2026 34/57",narrative
1325,IA_6.pdf_p34,(cid is 40) (cid:40),diagram_extracted
1326,IA_6.pdf_p35,"Neural network with sigmoid activation functions FII,UAIC Lecture6 AI2025/2026 35/57",narrative
1327,IA_6.pdf_p36,"Universal approximation property ▶ A neural network with a hidden layer, with a possible infinite number of neurons, can approximate any continuous real function ▶ An additional layer can reduce the number of neurons needed in the hidden layers ▶ A multi-layer perceptron with linear activation functions is equivalent with a single layer perceptron ▶ a linear combination of linear functions is also a linear function ex: f(x)=2x+1, g(y)=y-3, g(f(x))=(2x+1)-3=2x-2 FII,UAIC Lecture6 AI2025/2026 36/57",narrative
1328,IA_6.pdf_p37,"Content Introduction The perceptron Perceptron training rule Gradient descent and Delta rule Multi-layer neural networks Backpropagation FII,UAIC Lecture6 AI2025/2026 37/57",narrative
1329,IA_6.pdf_p38,"Backpropagation algorithm ▶ Rumelhart, Hinton& Williams, ’86 ▶ Learn the weights in a multi-layer network.",narrative
1330,IA_6.pdf_p38,It uses Gradient descent to minimize the squared error between the network output and the target values.,narrative
1331,IA_6.pdf_p38,"▶ Since we have networks with multiple output units, we redefine E E( →− w) = 1 (cid:88) (cid:88) (t −o )2 kd kd 2 d∈Dk∈outputs where outputs the set of output units, t and o the target values kd kd respectively the output values associated with the output unit k and the training example d. FII,UAIC Lecture6 AI2025/2026 38/57",narrative
1332,IA_6.pdf_p38,→− (cid is 88) (cid:88),diagram_extracted
1333,IA_6.pdf_p39,"Backpropagation Has two phases: ▶ The network receives the input vector and propagates the signal forward, layer by layer, until the output is generated ▶ The error signal is propagated backwards, from the output layer to the input layer, by adjusting the network weights FII,UAIC Lecture6 AI2025/2026 39/57",narrative
1334,IA_6.pdf_p40,"Backpropagation ▶ Initialization: choose the number of inputs, hidden and output units; initialize the weights and the thresholds with small random values ▶ in general, values in the range [-0.1, 0.1] ▶ Activation ▶ →− the network is activated by applying the training data x ▶ compute the output of the neurons from the hidden layer ▶ →− →− computetheoutputoftheneuronsfromtheoutputlayero =σ(w · x ) FII,UAIC Lecture6 AI2025/2026 40/57",narrative
1335,IA_6.pdf_p40,"Initialization is choose the number of inputs, hidden and output units;",diagram_extracted
1336,IA_6.pdf_p41,"Backpropagation ▶ The output of the neurons from the hidden layer n (cid:88) o = σ( w x ) h hi i i=0 ▶ The output of neurons from the output layer m (cid:88) o = σ( w o ) k ki i i=0 FII,UAIC Lecture6 AI2025/2026 41/57",narrative
1339,IA_6.pdf_p42,"Backpropagation Update the weights proportional to the learning rate η, the input value x ji and the error δ .",narrative
1340,IA_6.pdf_p42,"j ▶ For the output neurons ▶ compute the gradients of the neurons from the output layer For the output unit k, δ =(t −o )o (1−o ) k k k k k ▶ For the neurons from the hidden layer ▶ compute the gradients of the neurons from the hidden layer For the hidden unit h, sum the gradients δ for each output unit k influenced by h, weighted by w (the weight from the hidden layer h kh to the output layer k): (cid:88) δ =o (1−o ) w δ h h h kh k k∈outputs FII,UAIC Lecture6 AI2025/2026 42/57",narrative
1342,IA_6.pdf_p43,"Updating weights For each training example d, add w = w +∆w , ∆w = −ηδE d, where ji ji ji ji δwji E is the error for the training example d. d E ( →− w) = 1 (cid:88) (t −o )2 d k k 2 k∈outputs ▶ The weights of an output unit ∆w = ηδ x , δ = (t −o )o (1−o ) ji j ji j j j j j ▶ The weights of a hidden neuron (cid:88) ∆w = ηδ x , δ = o (1−o ) δ w ji j ji j j j k kj k∈Downstream(j) FII,UAIC Lecture6 AI2025/2026 43/57",narrative
1343,IA_6.pdf_p43,→− (cid is 88),diagram_extracted
1345,IA_6.pdf_p44,"Backpropagation For the feed-forward networks with an arbitrary number of layers, (cid:88) δ = o (1−o ) w δ r r r sr s s∈layer m+1 δ for the unit r from layer m is computed from the δ values from the next r layer m+1 FII,UAIC Lecture6 AI2025/2026 44/57",narrative
1347,IA_6.pdf_p45,"Chain Rule FII,UAIC Lecture6 AI2025/2026 45/57",narrative
1348,IA_6.pdf_p46,"Derivation of the Backpropagation rule Use chain rule: δE δE δnet d d j = δw δnet δw ji j ji (2) δE d = x ji δnet j FII,UAIC Lecture6 AI2025/2026 46/57",narrative
1349,IA_6.pdf_p47,"Derivation of the Backpropagation rule FII,UAIC Lecture6 AI2025/2026 47/57",narrative
1350,IA_6.pdf_p48,"Derivation of the Backpropagation rule FII,UAIC Lecture6 AI2025/2026 48/57",narrative
1351,IA_6.pdf_p49,"Stochastic Gradient Descent FII,UAIC Lecture6 AI2025/2026 49/57",narrative
1352,IA_6.pdf_p50,Backpropagation ▶ Iterate over all training examples (vectors) (an epoch) ▶ The training of the network continues until the error falls below an acceptable threshold or until a predetermined maximum no.,narrative
1353,IA_6.pdf_p50,of training epochs is reached Example: from Artificial Intelligence.,narrative
1354,IA_6.pdf_p50,A Guide to Intelligent Systems.,narrative
1355,IA_6.pdf_p50,"FII,UAIC Lecture6 AI2025/2026 50/57",narrative
1356,IA_6.pdf_p50,Example is from Artificial Intelligence. A Guide to Intelligent Systems.,diagram_extracted
1357,IA_6.pdf_p51,"Backpropagation ▶ Convergence: the algorithm Backpropagation converges to a local minimum ▶ Incremental learning vs. batch learning ▶ batch learning: the weights are updated once, after considering all training vectors from batch Advantage: the training results no longer depend on the order the training vectors are given FII,UAIC Lecture6 AI2025/2026 51/57",narrative
1358,IA_6.pdf_p51,Convergence is the algorithm Backpropagation converges to a local,diagram_extracted
1359,IA_6.pdf_p51,"batch learning is the weights are updated once, after considering all",diagram_extracted
1360,IA_6.pdf_p51,Advantage is the training results no longer depend on the order the,diagram_extracted
1361,IA_6.pdf_p52,"The ”momentum” variant of Backpropagation algorithm ▶ The adjustment term for the weight from the current epoch is computed based on the error signal and the adjustments from the previous epoch ∆w (n) = ηδ x +α∆w (n−1) ji j ji ji where ∆w (n) the updating weight in epoch n, 0 ≤ α < 1 const.",narrative
1362,IA_6.pdf_p52,"ji momentum (inertia) ▶ Stabilize the search Why Momentum Really Works, https://distill.pub/2017/momentum/ FII,UAIC Lecture6 AI2025/2026 52/57",narrative
1363,IA_6.pdf_p52,"Why Momentum Really Works, https is //distill.pub/2017/momentum/",diagram_extracted
1364,IA_6.pdf_p53,"Overfitting Solutions ▶ weight decay: include a penalty term in the error function (cid:88) E = E +λ w2 ji i,j ▶ using a validation set k-fold cross-validation FII,UAIC Lecture6 AI2025/2026 53/57",narrative
1365,IA_6.pdf_p53,weight decay is include a penalty term in the error function,diagram_extracted
1367,IA_6.pdf_p54,"Regularization ▶ Dropout: during training, we randomly set activation functions to 0 ▶ Early stopping: stop training FII,UAIC Lecture6 AI2025/2026 54/57",narrative
1368,IA_6.pdf_p54,"Dropout is during training, we randomly set activation functions to 0",diagram_extracted
1369,IA_6.pdf_p54,Early stopping is stop training,diagram_extracted
1370,IA_6.pdf_p55,"Loss ▶ Cross entropy: for models that return a probability ▶ Mean squared error: for regression FII,UAIC Lecture6 AI2025/2026 55/57",narrative
1371,IA_6.pdf_p55,Cross entropy is for models that return a probability,diagram_extracted
1372,IA_6.pdf_p55,Mean squared error is for regression,diagram_extracted
1373,IA_6.pdf_p56,Designing neural networks: steps ▶ Architecture: the no.,narrative
1374,IA_6.pdf_p56,"of levels and units per level, topology (interconnection method), activation functions Architectures: unidirectional vs. recurrent ▶ Training: finding the values of weights ▶ Testing: testing the model on test data https://playground.tensorflow.org/ FII,UAIC Lecture6 AI2025/2026 56/57",narrative
1375,IA_6.pdf_p56,Designing neural networks is steps,diagram_extracted
1376,IA_6.pdf_p56,"Architecture is the no. of levels and units per level, topology",diagram_extracted
1377,IA_6.pdf_p56,Architectures is unidirectional vs. recurrent,diagram_extracted
1378,IA_6.pdf_p56,Training is finding the values of weights,diagram_extracted
1379,IA_6.pdf_p56,Testing is testing the model on test data,diagram_extracted
1380,IA_6.pdf_p56,https is //playground.tensorflow.org/,diagram_extracted
1381,IA_6.pdf_p57,"Bibliography ▶ T. M. Mitchell, Machine Learning, Ch.",narrative
1382,IA_6.pdf_p57,"4 Artificial Neural Networks, McGraw-Hill Science, 1997 ▶ S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, Ch.",narrative
1383,IA_6.pdf_p57,"18.7 Artificial Neural Networks, Prentice Hall, 1995 ▶ M. Neqnevitsky.",narrative
1384,IA_6.pdf_p57,"A Guide to Intelligent Systems, Ch.",narrative
1385,IA_6.pdf_p57,"Multilayer neural networks, 2005 FII,UAIC Lecture6 AI2025/2026 57/57",narrative
1386,IA_6.pdf_p57,"S. Russell, P. Norvig, Artificial Intelligence is A Modern Approach, Ch.",diagram_extracted
1387,IA_2.pdf_p1,"Artificial Intelligence 3rd year, 1st semester State based models for decision problems Week 2: models and uninformed search strategies “Intelligence is what you use when you don't know what to do” Jean Piaget",narrative
1388,IA_2.pdf_p1,Week 2 is models and uninformed search strategies,diagram_extracted
1389,IA_2.pdf_p2,We aim to develop rational AI systems: ● we define a problem and goals (instances) ● the AI reaches the goals (solves the instances) ● the AI outputs the result (unexplainable AI) or the solution (explainable AI),narrative
1391,IA_2.pdf_p3,"● compute a solution (algorithm - sequence of steps) starting from an initial state and ending in the goal state ● mostly covered by search strategies, reasoning systems, AI for games",narrative
1393,IA_2.pdf_p4,"Solving problems Problem: We have two numbers, A and B.",narrative
1394,IA_2.pdf_p4,Which is larger?,narrative
1395,IA_2.pdf_p4,"Instance: A=?, B=?",narrative
1396,IA_2.pdf_p4,A solution should produce the correct answer for all possible instances,narrative
1397,IA_2.pdf_p4,"Problem is We have two numbers, A and B. Which is",diagram_extracted
1398,IA_2.pdf_p4,"Instance is A=?, B=?",diagram_extracted
1399,IA_2.pdf_p5,What’s the Problem with problems?,narrative
1400,IA_2.pdf_p5,P = set of problems that can be solved/checked in Polynomial time by a deterministic TM.,narrative
1401,IA_2.pdf_p5,NP = set of problems that can be solved in Polynomial time by a non-deterministic TM.,narrative
1402,IA_2.pdf_p5,Validating a NP solution is a P problem.,narrative
1403,IA_2.pdf_p5,NP-complete = subset of NP with the property that any problem in NP can be reduced to a NP-complete problem in polynomial time.,narrative
1404,IA_2.pdf_p5,NP-hard = set of problems with the property that any problem in NP can be reduced to a NP-hard problem in polynomial time.,narrative
1405,IA_2.pdf_p6,Let the computer solve our problems!,narrative
1406,IA_2.pdf_p6,Or not... Solvable NP-hard problems: QSAT: is a logical proposition using quantified (existential ∃ or universal ∀) variables satisfiable?,narrative
1407,IA_2.pdf_p6,"Undecideable NP-hard problems: Turing Halting problem: given an algorithm and an input, determine whether the algorithm execution for that input will eventually halt.",narrative
1408,IA_2.pdf_p6,The rest can be solved by a computer.,narrative
1409,IA_2.pdf_p6,Solvable NP-hard problems is QSAT: is a logical proposition using,diagram_extracted
1410,IA_2.pdf_p6,Undecideable NP-hard problems is Turing Halting problem: given an,diagram_extracted
1411,IA_2.pdf_p7,Stop looking for solutions!,narrative
1412,IA_2.pdf_p7,"Don’t think about solutions, think about problems.",narrative
1413,IA_2.pdf_p7,"Solving problems requires effort, describing problems requires intelligence.",narrative
1414,IA_2.pdf_p7,"“If I had only one hour to save the world, I would spend fifty-five minutes defining the problem, and only five minutes finding the solution.” A.Einstein",narrative
1415,IA_2.pdf_p8,Let’s find a NP-complete problem and help a computer to solve it Problem: finding a path in a graph Describing a model - reducing a problem • describe a state • identify special states and the problem space • describe the transitions and validate them • specify a search strategy,narrative
1417,IA_2.pdf_p9,"Problem: finding a path in a graph Finding a path in a graph is NP-complete, however it can easily be scaled up to NP-hard by many slight reformulations, such as: ● Finding a path if the graph has cycles ● Finding the longest path ● Graph coloring ● Finding cliques This means that you can also solve NP-hard problems using the same method by adapting the requirements.",narrative
1419,IA_2.pdf_p10,Generalized Hanoi Towers We have n towers with m distinct sized pieces placed on one of the towers.,narrative
1420,IA_2.pdf_p10,"Considering that you can only move one piece at a time and that no piece can be placed on top of a smaller piece, find a sequence of movements that place all pieces on a different tower.",narrative
1421,IA_2.pdf_p11,Choosing a representation for a state State: all data required to continue looking for a solution No ambiguity Expressive enough to include all required data,narrative
1422,IA_2.pdf_p11,State is all data required to,diagram_extracted
1423,IA_2.pdf_p12,"Choosing a representation for a state Compact enough to be easy to store and perform changes on it A list of towers pieces are placed, in the order of piece size size (3, 3, 3, 3, 3, 1, 1, 1, 2, 2) Is this good enough?",narrative
1424,IA_2.pdf_p13,"Choosing a representation for a state Compact enough to be easy to store and perform changes on it A list of towers where pieces are placed, in the order of piece size, with the number of towers added at the beginning (3, 3, 3, 3, 3, 3, 1, 1, 1, 2, 2) (n, t , t ,...,t ), 1≤t ≤n 1 2 m i",narrative
1425,IA_2.pdf_p14,Choosing a representation for a state is the most important and difficult step No ambiguity Compact Expressive Includes all required data There is no generally best representation,narrative
1426,IA_2.pdf_p15,"Special states Initial state (3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) State Initialize (int n, int m) { Return (n, 1, 1, 1, …, 1); } m There can be more than one initial state.",narrative
1427,IA_2.pdf_p15,There has to be at least one initial state.,narrative
1428,IA_2.pdf_p16,"Special states Final state(s) (n, n, n, n, n, n, n, n, n, n, n) Boolean IsFinal (State s) { If s = (k, k, k, …, k) then return true; m+1 else return false; } There can be more than one final state.",narrative
1429,IA_2.pdf_p16,There has to be at least one final state.,narrative
1430,IA_2.pdf_p17,Problem space: all possible states Final state Initial state Finite space (limited variability in chosen state representation and only valid states are included),narrative
1431,IA_2.pdf_p17,Problem space is all possible states,diagram_extracted
1432,IA_2.pdf_p18,What is the problem now?,narrative
1433,IA_2.pdf_p18,How difficult is it to solve?,narrative
1434,IA_2.pdf_p18,Problem space has m dimensions Final state How many states are within the space?,narrative
1435,IA_2.pdf_p18,How can you move within the problem space?,narrative
1436,IA_2.pdf_p18,"Initial state Simon’s Ant Complexity of the solution is determined by the problem, NOT by the way in which you solve it",narrative
1437,IA_2.pdf_p19,Transitions Only one possible way to change current state: move one piece to another tower.,narrative
1438,IA_2.pdf_p19,"(n, t , t ,...,t ) → (n, t , t ,...,t ), where t = t for all 1≤i≤m, 11 12 1m 21 22 2m 1i 2i except exactly one i = k State Transition (State s, piece, tower)",narrative
1439,IA_2.pdf_p19,Only one possible way to change current state is move one piece to,diagram_extracted
1440,IA_2.pdf_p20,Valid transitions 1.,narrative
1441,IA_2.pdf_p20,No smaller piece is placed atop piece k 2.,narrative
1442,IA_2.pdf_p20,"No smaller piece ends up below piece k 1. t ⧧ t , for all 1≤i<k 1i 1k 2. t ⧧ t , for all 1≤i<k 2i 2k Boolean Validate (State s, piece, tower) Implement transitions and validation separately, it’s good practice!",narrative
1443,IA_2.pdf_p21,"Search strategy Void strategy(State s) { While (!isFinal(s)) { Choose piece, tower; If (Validate (s, piece, tower)) s = Transition(s, piece, tower); } }",narrative
1444,IA_2.pdf_p22,Search strategies ● Uninformed - no distinction between states ○ Random ○ BFS and Uniform Cost ○ DFS and Iterative Deepening ○ Backtracking ○ Bidirectional ● Informed - heuristics to help distinguish between states ○ Greedy best-first ○ Hillclimbing and Simulated Annealing ○ Beam ○ A* and IDA*,narrative
1445,IA_2.pdf_p23,Breadth First Search and Uniform Cost ● Visit states in the order of distance (number of transitions) from the initial state.,narrative
1446,IA_2.pdf_p23,● Explores all immediate neighbors (accessible states) until no more neighbors or final state found.,narrative
1447,IA_2.pdf_p23,● Has to memorise each generated state: very costly.,narrative
1448,IA_2.pdf_p23,● Might visit the same state multiple times.,narrative
1449,IA_2.pdf_p23,"● Finds shortest path (optimum solution) ● Uniform cost: if options have different costs, explore cheaper paths first",narrative
1450,IA_2.pdf_p23,● Uniform cost is if options have different,diagram_extracted
1451,IA_2.pdf_p24,"Depth First Search and Iterative Deepening Search ● Visit one immediate neighbor of the current state until final state is found, return to previous unexplored neighbor if no more neighbours available for current state.",narrative
1454,IA_2.pdf_p24,● Might not finish (if loops are present in the problem space) ● IDS: Explore only up to an increasing depth (distance from initial state) - no more infinite paths,narrative
1455,IA_2.pdf_p24,● IDS is Explore only up to an increasing,diagram_extracted
1456,IA_2.pdf_p25,"Random vs DFS ● The same, if random remembers visited neighbours Backtracking vs DFS ● NOT the same ● Backtracking sets an order for the neighbor states ● How do you order states?",narrative
1457,IA_2.pdf_p25,● Backtracking doesn't need to memorize visited states!,narrative
1458,IA_2.pdf_p25,"● Backtracking can avoid loops WITHOUT storing visited states ● Backtracking can prune invalid branches earlier, if the order and selection of neighbours is optimal",narrative
1459,IA_2.pdf_p26,"Bidirectional Search ● Starts exploring from both the initial and final state simultaneously ● Has to memorise each generated state, but they should not be that many ● Finds shortest path (optimum solution) ● What are the reverse transitions?",narrative
1460,IA_2.pdf_p27,"Comparison Complexity of the solution is given by the problem, NOT by the way in which you solve it Criterion BFS DFS IDS Bidirectional BKT Random ETC NO NA NO NO NA NA Optimum Yes No Yes Yes No No All Yes No Yes Yes Yes No N = average number of accessible states O = length of the optimum solution A = length of the average solution",narrative
1461,IA_2.pdf_p28,Additional references Uninformed Search Algorithms Stanford Programming Abstractions,narrative
